{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys,os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy import linalg\n",
    "from numpy import dot\n",
    "import geomloss as gs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import grad\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.modules import Linear\n",
    "from torch.autograd.functional import jacobian,hessian,vjp,vhp,hvp\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions preparation\n",
    "\n",
    "def setup_seed(seed):\n",
    "     torch.cuda.manual_seed(seed)\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.benchmark=False\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, dim_hidden=64, num_hidden=0, activation=nn.Tanh()):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        if num_hidden == 0:\n",
    "            self.linears = nn.ModuleList([nn.Linear(dim_in, dim_out)])\n",
    "        elif num_hidden >= 1:\n",
    "            self.linears = nn.ModuleList() \n",
    "            self.linears.append(nn.Linear(dim_in, dim_hidden))\n",
    "            self.linears.extend([nn.Linear(dim_hidden, dim_hidden) for _ in range(num_hidden-1)])\n",
    "            self.linears.append(nn.Linear(dim_hidden, dim_out))\n",
    "        else:\n",
    "            raise Exception('number of hidden layers must be positive')\n",
    "\n",
    "        for m in self.linears:\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.uniform_(m.bias,a=-0.1,b=0.1)\n",
    " \n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.linears[:-1]:\n",
    "            x = self.activation(m(x))\n",
    "\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "## WGAN-div term\n",
    "def compute_gradient_penalty(D, real_sample, fake_sample,k=2,p=6):\n",
    "    real_samples = real_sample.requires_grad_(True)\n",
    "    fake_samples = fake_sample.requires_grad_(True)\n",
    "\n",
    "    real_validity = D(real_samples)\n",
    "    fake_validity = D(fake_samples)\n",
    "\n",
    "    real_grad_out = torch.ones((real_samples.shape[0],1),dtype=torch.float32,requires_grad=False,device=\"cuda\")\n",
    "    real_grad = grad(\n",
    "        real_validity, real_samples, real_grad_out, create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    real_grad_norm = real_grad.view(real_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "\n",
    "    fake_grad_out = torch.ones((fake_samples.shape[0],1),dtype=torch.float32,requires_grad=False,device=\"cuda\")\n",
    "    fake_grad = grad(\n",
    "        fake_validity, fake_samples, fake_grad_out, create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    fake_grad_norm = fake_grad.view(fake_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "\n",
    "    return (torch.sum(real_grad_norm) + torch.sum(fake_grad_norm)) * k / (real_sample.shape[0]+fake_sample.shape[0])\n",
    "\n",
    "# generator, EM scheme\n",
    "class JumpEulerForwardCuda(nn.Module):\n",
    "    def __init__(self,in_features,num_hidden,dim_hidden,step_size,intensity,bd):\n",
    "        super(JumpEulerForwardCuda,self).__init__()\n",
    "\n",
    "        self.drift = MLP(in_features,in_features,dim_hidden,num_hidden)\n",
    "        self.intensity = torch.tensor(intensity,device=\"cuda\")\n",
    "        self.mean = nn.Parameter(0.01*torch.ones(in_features))\n",
    "        self.covHalf = nn.Parameter(0.08*torch.eye(in_features))\n",
    "        self.diffusion = nn.Parameter(torch.ones(bd,in_features))\n",
    "        self.in_features = in_features\n",
    "        self.jump = MLP(in_features,in_features,dim_hidden,num_hidden)\n",
    "        self.step_size = step_size\n",
    "        self.bd = bd\n",
    "\n",
    "    def forward(self,z0,Nsim,steps):\n",
    "\n",
    "        PopulationPath = torch.empty(size = (Nsim,steps+1,self.in_features),device=\"cuda\")\n",
    "        PopulationPath[:,0,:] = z0\n",
    "        state = z0\n",
    "\n",
    "        for i in range(1,steps+1):\n",
    "            DP = D.poisson.Poisson(self.intensity*self.step_size) \n",
    "            pois = DP.sample((Nsim,1)).cuda()\n",
    "            state = state + self.drift(state)*self.step_size + math.sqrt(self.step_size)*torch.normal(0,1,size=(Nsim,self.bd),device=\"cuda\")@self.diffusion+\\\n",
    "                (pois*self.mean + pois**(0.5)*torch.normal(0,1,size=(Nsim,self.in_features),device=\"cuda\")@self.covHalf)*self.jump(state)\n",
    "            PopulationPath[:,i,:] = state\n",
    "        return PopulationPath\n",
    "\n",
    "## Sinkhorn distance\n",
    "a = gs.SamplesLoss(loss='sinkhorn',p=2,blur=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Train(AggregateData,n_steps,bd,intensity,num_hidden,dim_hidden,step_size=0.05,n_epochs=30000,n_critic=4,lr=0.0001,Seed=80):\n",
    "    ##########################################################################################################################\n",
    "    ####### input\n",
    "    # AggregateData: a list length l, each element is a numpy recording the observation at a time point. row:variables, col:sample size\n",
    "    # nsteps: a list with length l-1\n",
    "    # bd: the dimension of Brownian motion m\n",
    "    # intensity: lambda, the jump intensity\n",
    "    # num_hidden, dim_hidden: parameters for the neural networks\n",
    "    # step_size: a numerical value,delta\n",
    "    # n_epochs: training iterations\n",
    "    # n_critic: the number of times that the critic is updated until the generator is updated\n",
    "    # lr: learning rate\n",
    "    # Seed: random seed\n",
    "    \n",
    "    ###### output\n",
    "    # netG: the jump diffusion model\n",
    "    # AggregateData: the training data in cuda\n",
    "    ##########################################################################################################################\n",
    "    \n",
    "    # transform to torch.tensor\n",
    "    AggregateData = [torch.tensor(ele,dtype=torch.float32,requires_grad = True,device=\"cuda\").t() for ele in AggregateData]\n",
    "    \n",
    "    D = len(AggregateData) ## D is the number of observed time points\n",
    "    print(\"{0} time points have been observed\".format(D))\n",
    "    \n",
    "    n_sims = AggregateData[0].shape[0]\n",
    "    in_features = AggregateData[0].shape[1]\n",
    "    \n",
    "    #print(n_sims,in_features,AggregateData[0][0])\n",
    "    \n",
    "    # random seed\n",
    "    setup_seed(Seed)\n",
    "\n",
    "    # create generator and critic\n",
    "    netG = JumpEulerForwardCuda(in_features,num_hidden,dim_hidden,step_size,intensity,bd).cuda()\n",
    "    netD = [MLP(in_features,1,dim_hidden,num_hidden).cuda() for _ in range(D-1)]\n",
    "\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizerSD = [optim.Adam(netD_i.parameters(), lr=lr, betas=(0.5, 0.999)) for netD_i in netD]\n",
    "\n",
    "\n",
    "    ### training process\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "\n",
    "        # -------------------\n",
    "        # train the critic\n",
    "        # -------------------\n",
    "\n",
    "\n",
    "        for _ in range(n_critic): \n",
    "            fake_data = netG(AggregateData[0],n_sims,n_steps[-1])\n",
    "            fake = [fake_data[:,ele,:] for ele in n_steps]\n",
    "            \n",
    "            for SDi in range(D-1):\n",
    "                optimizerSD[SDi].zero_grad()\n",
    "                div_gp = compute_gradient_penalty(netD[SDi],AggregateData[SDi+1],fake[SDi])\n",
    "                d_loss = -torch.mean(netD[SDi](AggregateData[SDi+1]))+torch.mean(netD[SDi](fake[SDi]))+div_gp\n",
    "                d_loss.backward(retain_graph=True) # retain_graph=True\n",
    "\n",
    "                optimizerSD[SDi].step()\n",
    "\n",
    "\n",
    "        # ----------------\n",
    "        # train the generator\n",
    "        # ----------------\n",
    "\n",
    "        for _ in range(1):\n",
    "            optimizerG.zero_grad()\n",
    "\n",
    "            fake_data = netG(AggregateData[0],n_sims,n_steps[-1])\n",
    "            fake = [fake_data[:,ele,:] for ele in n_steps]\n",
    "            g_loss = [-torch.mean(netD[el](fake[el])) for el in range(D-1)]\n",
    "            g_loss = sum(g_loss)\n",
    "            g_loss.backward() \n",
    "\n",
    "            optimizerG.step()\n",
    "\n",
    "        if epoch %10==0:\n",
    "            error = [a(fake[ii],AggregateData[ii+1]).item() for ii in range(D-1)]\n",
    "            #print(\"epoch:\",epoch,\";\", \"d1_loss:\",(-d1_loss+div_gp1).item(),\";\",\"d2_loss:\",(-d2_loss+div_gp2).item(),\";\",\"g_loss:\",g_loss.item())\n",
    "            print(\"epoch: \",epoch,\"training: \",error)\n",
    "            \n",
    "    return netG,AggregateData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example: the stem cell differentiation dataset with two genes Tagln and Gsn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 4000.1233\n",
      "total residual 138.7959\n",
      "Iteration 200:\n",
      "fit residual 123.6643\n",
      "total residual 31.8464\n",
      "Iteration 400:\n",
      "fit residual 21.7894\n",
      "total residual 12.6115\n",
      "Iteration 500:\n",
      "fit residual 4.3442\n",
      "total residual 8.6686\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 2257.8358\n",
      "total residual 58.2265\n",
      "Iteration 200:\n",
      "fit residual 56.8198\n",
      "total residual 4.0447\n",
      "Iteration 400:\n",
      "fit residual 3.4777\n",
      "total residual 0.6841\n",
      "Iteration 500:\n",
      "fit residual 0.3744\n",
      "total residual 0.3185\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 2925.3664\n",
      "total residual 80.2657\n",
      "Iteration 200:\n",
      "fit residual 78.1148\n",
      "total residual 6.1559\n",
      "Iteration 400:\n",
      "fit residual 5.1661\n",
      "total residual 1.1917\n",
      "Iteration 500:\n",
      "fit residual 0.6183\n",
      "total residual 0.5917\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 3719.0401\n",
      "total residual 121.8387\n",
      "Iteration 200:\n",
      "fit residual 113.1429\n",
      "total residual 20.967\n",
      "Iteration 400:\n",
      "fit residual 15.2785\n",
      "total residual 7.12\n",
      "Iteration 500:\n",
      "fit residual 2.7305\n",
      "total residual 4.5793\n"
     ]
    }
   ],
   "source": [
    "## The data processing steps for the steam cell dataset is the same as that in https://github.com/thashim/population-diffusions. \n",
    "FilePath = './'\n",
    "\n",
    "file_list = ['GSM1599494_ES_d0_main.csv', 'GSM1599497_ES_d2_LIFminus.csv', 'GSM1599498_ES_d4_LIFminus.csv', 'GSM1599499_ES_d7_LIFminus.csv']\n",
    "\n",
    "table_list = []\n",
    "for filein in file_list:\n",
    "    table_list.append(pd.read_csv(FilePath+filein, header=None))\n",
    "\n",
    "matrix_list = []\n",
    "gene_names = table_list[0].values[:,0]\n",
    "for table in table_list:\n",
    "    matrix_list.append(table.values[:,1:].astype('float32'))\n",
    "\n",
    "cell_counts = [matrix.shape[1] for matrix in matrix_list]\n",
    "\n",
    "# Normalization\n",
    "def normalize_run(mat):\n",
    "    rpm = np.sum(mat,0)/1e6\n",
    "    detect_pr = np.sum(mat==0,0)/float(mat.shape[0])\n",
    "    return np.log(mat*(np.median(detect_pr)/detect_pr)*1.0/rpm + 1.0)\n",
    "\n",
    "norm_mat = [normalize_run(matrix) for matrix in matrix_list]\n",
    "\n",
    "# sort genes by Wasserstein distance between D0 and D7\n",
    "qt_mat = [np.percentile(norm_in,q=np.linspace(0,100,50),axis=1) for norm_in in norm_mat] \n",
    "wdiv=np.sum((qt_mat[0]-qt_mat[3])**2,0)\n",
    "w_order = np.argsort(-wdiv)\n",
    "\n",
    "wsub = w_order[0:100]\n",
    "\n",
    "def nmf(X, latent_features, max_iter=100, error_limit=1e-6, fit_error_limit=1e-6, print_iter=200):\n",
    "    \"\"\"\n",
    "    Decompose X to A*Y\n",
    "    \"\"\"\n",
    "    eps = 1e-5\n",
    "    print('Starting NMF decomposition with {} latent features and {} iterations.'.format(latent_features, max_iter))\n",
    "    #X = X.toarray()   I am passing in a scipy sparse matrix\n",
    "\n",
    "    # mask\n",
    "    mask = np.sign(X)\n",
    "\n",
    "    # initial matrices. A is random [0,1] and Y is A\\X.\n",
    "    rows, columns = X.shape\n",
    "    A = np.random.rand(rows, latent_features) \n",
    "    A = np.maximum(A, eps)\n",
    "\n",
    "    Y = linalg.lstsq(A, X)[0]\n",
    "    Y = np.maximum(Y, eps)\n",
    "\n",
    "    masked_X = mask * X\n",
    "    X_est_prev = dot(A, Y)\n",
    "    for i in range(1, max_iter + 1):\n",
    "        # ===== updates =====\n",
    "        # Matlab: A=A.*(((W.*X)*Y')./((W.*(A*Y))*Y'));\n",
    "        top = dot(masked_X, Y.T)\n",
    "        bottom = (dot((mask * dot(A, Y)), Y.T)) + eps\n",
    "        A *= top / bottom\n",
    "\n",
    "        A = np.maximum(A, eps)\n",
    "        # print 'A',  np.round(A, 2)\n",
    "\n",
    "        # Matlab: Y=Y.*((A'*(W.*X))./(A'*(W.*(A*Y))));\n",
    "        top = dot(A.T, masked_X)\n",
    "        bottom = dot(A.T, mask * dot(A, Y)) + eps\n",
    "        Y *= top / bottom\n",
    "        Y = np.maximum(Y, eps)\n",
    "        # print 'Y', np.round(Y, 2)\n",
    "\n",
    "\n",
    "        # ==== evaluation ====\n",
    "        if i % print_iter == 0 or i == 1 or i == max_iter:\n",
    "            print('Iteration {}:'.format(i),)\n",
    "            X_est = dot(A, Y)\n",
    "            err = mask * (X_est_prev - X_est)\n",
    "            fit_residual = np.sqrt(np.sum(err ** 2))\n",
    "            X_est_prev = X_est\n",
    "\n",
    "            curRes = linalg.norm(mask * (X - X_est), ord='fro')\n",
    "            print('fit residual', np.round(fit_residual, 4),)\n",
    "            print('total residual', np.round(curRes, 4))\n",
    "            if curRes < error_limit or fit_residual < fit_error_limit:\n",
    "                break\n",
    "    return A, Y, dot(A,Y)\n",
    "\n",
    "np.random.seed(0)\n",
    "norm_imputed = [nmf(normin[wsub,:], latent_features = len(wsub)*4, max_iter=500)[2] for normin in norm_mat]\n",
    "\n",
    "## choose the 3rd and 7th genes as the training varibales\n",
    "norm_adj = np.mean(norm_imputed[3],1)[:,np.newaxis]\n",
    "subvec = np.array([3,7])\n",
    "\n",
    "gnvec = gene_names[w_order[subvec]]\n",
    "\n",
    "cov_mat = np.cov(norm_imputed[3][subvec,:])\n",
    "whiten = np.diag(np.diag(cov_mat)**(-0.5))\n",
    "unwhiten = np.diag(np.diag(cov_mat)**(0.5))\n",
    "\n",
    "norm_imputed2 = [np.dot(whiten,(normin - norm_adj)[subvec,:]) for normin in norm_imputed]\n",
    "\n",
    "train_data = norm_imputed2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 time points have been observed\n",
      "933 2 tensor([-2.3410, -3.7850], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "epoch:  0 training:  [1.32451593875885, 4.614095687866211, 7.783506870269775]\n",
      "epoch:  10 training:  [0.23683612048625946, 0.33031779527664185, 0.04719405621290207]\n",
      "epoch:  20 training:  [0.16710378229618073, 0.24282582104206085, 0.07422839105129242]\n",
      "epoch:  30 training:  [0.2877744436264038, 0.1751551330089569, 0.048849303275346756]\n",
      "epoch:  40 training:  [0.29298001527786255, 0.08472757786512375, 0.2747907042503357]\n",
      "epoch:  50 training:  [0.15940210223197937, 0.3763771653175354, 0.09240724891424179]\n",
      "epoch:  60 training:  [0.18050657212734222, 0.15011844038963318, 0.055368080735206604]\n",
      "epoch:  70 training:  [0.21605640649795532, 0.10083488374948502, 0.10211674869060516]\n",
      "epoch:  80 training:  [0.17942486703395844, 0.12052836269140244, 0.00978100299835205]\n",
      "epoch:  90 training:  [0.13572803139686584, 0.3425350487232208, 0.11001603305339813]\n",
      "epoch:  100 training:  [0.2304874211549759, 0.06967796385288239, 0.10572101175785065]\n",
      "epoch:  110 training:  [0.12266944348812103, 0.24307096004486084, 0.061387039721012115]\n",
      "epoch:  120 training:  [0.25344300270080566, 0.08465564996004105, 0.08459191024303436]\n",
      "epoch:  130 training:  [0.12124267220497131, 0.225664883852005, 0.007955759763717651]\n",
      "epoch:  140 training:  [0.23286816477775574, 0.07987909018993378, 0.09942961484193802]\n",
      "epoch:  150 training:  [0.15560971200466156, 0.18676772713661194, 0.021403931081295013]\n",
      "epoch:  160 training:  [0.18876619637012482, 0.07579214870929718, 0.07810372114181519]\n",
      "epoch:  170 training:  [0.17952527105808258, 0.11417259275913239, -0.004326656460762024]\n",
      "epoch:  180 training:  [0.13782329857349396, 0.14497815072536469, 0.02536833845078945]\n",
      "epoch:  190 training:  [0.17602592706680298, 0.10591183602809906, 0.01703544333577156]\n",
      "epoch:  200 training:  [0.17881178855895996, 0.05888224393129349, 0.046593308448791504]\n",
      "epoch:  210 training:  [0.13264243304729462, 0.11949742585420609, 0.024031221866607666]\n",
      "epoch:  220 training:  [0.15327629446983337, 0.07765783369541168, 0.03653843700885773]\n",
      "epoch:  230 training:  [0.17059943079948425, 0.0473606176674366, 0.15120311081409454]\n",
      "epoch:  240 training:  [0.13370627164840698, 0.08722096681594849, 0.011050339788198471]\n",
      "epoch:  250 training:  [0.13971608877182007, 0.14111866056919098, 0.039500974118709564]\n",
      "epoch:  260 training:  [0.13016435503959656, 0.10369986295700073, 0.020085828378796577]\n",
      "epoch:  270 training:  [0.11981259286403656, 0.08846088498830795, 0.029182318598031998]\n",
      "epoch:  280 training:  [0.11537201702594757, 0.08412517607212067, 0.01776454597711563]\n",
      "epoch:  290 training:  [0.0760233998298645, 0.09831869602203369, 0.018924865871667862]\n",
      "epoch:  300 training:  [0.11144182085990906, 0.18986280262470245, 0.0790233165025711]\n",
      "epoch:  310 training:  [0.07831624895334244, 0.13602527976036072, 0.023410068824887276]\n",
      "epoch:  320 training:  [0.09702728688716888, 0.09401495009660721, 0.053685493767261505]\n",
      "epoch:  330 training:  [0.11732031404972076, 0.018566634505987167, 0.09472878277301788]\n",
      "epoch:  340 training:  [0.1266801804304123, 0.03150896728038788, 0.10841216146945953]\n",
      "epoch:  350 training:  [0.0995665043592453, 0.04708775132894516, 0.04631062597036362]\n",
      "epoch:  360 training:  [0.07714509963989258, 0.08242958784103394, 0.021009143441915512]\n",
      "epoch:  370 training:  [0.0731029137969017, 0.06560350209474564, 0.017399970442056656]\n",
      "epoch:  380 training:  [0.07379104197025299, 0.05183124914765358, 0.01531379297375679]\n",
      "epoch:  390 training:  [0.1129160225391388, 0.047441303730010986, 0.02019616961479187]\n",
      "epoch:  400 training:  [0.06097724288702011, 0.04899020120501518, 0.013183282688260078]\n",
      "epoch:  410 training:  [0.06945692002773285, 0.03259783983230591, 0.006541725248098373]\n",
      "epoch:  420 training:  [0.05842670053243637, 0.03852439671754837, 0.09202515333890915]\n",
      "epoch:  430 training:  [0.06674829125404358, 0.04948653280735016, 0.030362656340003014]\n",
      "epoch:  440 training:  [0.06008727848529816, 0.06045660376548767, 0.03304808959364891]\n",
      "epoch:  450 training:  [0.05824640765786171, 0.03875263035297394, 0.025165293365716934]\n",
      "epoch:  460 training:  [0.05740649253129959, 0.043297380208969116, 0.016228390857577324]\n",
      "epoch:  470 training:  [0.06480058282613754, 0.02886744774878025, 0.027141977101564407]\n",
      "epoch:  480 training:  [0.07049332559108734, 0.020402908325195312, 0.030776355415582657]\n",
      "epoch:  490 training:  [0.07484843581914902, 0.05948171764612198, 0.026040125638246536]\n",
      "epoch:  500 training:  [0.13111908733844757, 0.05889800190925598, 0.06317215412855148]\n",
      "epoch:  510 training:  [0.04833170399069786, 0.07002139091491699, 0.1409119963645935]\n",
      "epoch:  520 training:  [0.15839585661888123, 0.13151180744171143, 0.10338179767131805]\n",
      "epoch:  530 training:  [0.04834004119038582, 0.29116731882095337, 0.1365978717803955]\n",
      "epoch:  540 training:  [0.05810469761490822, 0.21088260412216187, 0.09534478932619095]\n",
      "epoch:  550 training:  [0.08970097452402115, 0.04721423238515854, 0.03083021193742752]\n",
      "epoch:  560 training:  [0.10623962432146072, 0.04598375782370567, 0.04495301842689514]\n",
      "epoch:  570 training:  [0.11739253252744675, 0.04417680948972702, 0.07320380955934525]\n",
      "epoch:  580 training:  [0.11559578031301498, 0.04036233574151993, 0.0919400155544281]\n",
      "epoch:  590 training:  [0.12117494642734528, 0.05519922450184822, 0.1028631255030632]\n",
      "epoch:  600 training:  [0.09926451742649078, 0.03661123663187027, 0.10597065091133118]\n",
      "epoch:  610 training:  [0.07021712511777878, 0.021396920084953308, 0.0522565133869648]\n",
      "epoch:  620 training:  [0.0843752846121788, 0.06445717066526413, 0.0433613620698452]\n",
      "epoch:  630 training:  [0.10094989091157913, 0.06733370572328568, 0.05074140429496765]\n",
      "epoch:  640 training:  [0.04029744118452072, 0.03711768984794617, 0.05261668935418129]\n",
      "epoch:  650 training:  [0.05489904433488846, 0.04301949217915535, 0.021212153136730194]\n",
      "epoch:  660 training:  [0.054258935153484344, 0.027933191508054733, 0.03534024581313133]\n",
      "epoch:  670 training:  [0.05280126631259918, 0.039715178310871124, 0.04990056902170181]\n",
      "epoch:  680 training:  [0.0449117086827755, 0.07569858431816101, 0.04444237798452377]\n",
      "epoch:  690 training:  [0.07089294493198395, 0.15613166987895966, 0.06846947968006134]\n",
      "epoch:  700 training:  [0.05007437244057655, 0.16260114312171936, 0.050585974007844925]\n",
      "epoch:  710 training:  [0.03064851090312004, 0.060922589153051376, 0.03583987057209015]\n",
      "epoch:  720 training:  [0.060247838497161865, 0.04651937633752823, 0.057936474680900574]\n",
      "epoch:  730 training:  [0.09065637737512589, 0.0794825404882431, 0.1347893625497818]\n",
      "epoch:  740 training:  [0.11016952991485596, 0.08347992599010468, 0.09795816987752914]\n",
      "epoch:  750 training:  [0.026370320469141006, 0.011300332844257355, 0.058516599237918854]\n",
      "epoch:  760 training:  [0.04973088204860687, 0.057346925139427185, 0.04027073457837105]\n",
      "epoch:  770 training:  [0.07091731578111649, 0.11194256693124771, 0.08080749213695526]\n",
      "epoch:  780 training:  [0.07948046922683716, 0.046202871948480606, 0.08181996643543243]\n",
      "epoch:  790 training:  [0.0748538002371788, 0.035392314195632935, 0.05369425565004349]\n",
      "epoch:  800 training:  [0.04374833405017853, -0.009366858750581741, 0.04206017404794693]\n",
      "epoch:  810 training:  [0.04669337719678879, 0.06135459989309311, 0.08157198131084442]\n",
      "epoch:  820 training:  [0.025002170354127884, 0.06115712225437164, 0.039443761110305786]\n",
      "epoch:  830 training:  [0.0321284756064415, 0.03874989598989487, 0.020470378920435905]\n",
      "epoch:  840 training:  [0.054963573813438416, 0.020137425512075424, 0.06742892414331436]\n",
      "epoch:  850 training:  [0.05393201485276222, 0.019900716841220856, 0.05942083150148392]\n",
      "epoch:  860 training:  [0.04738646745681763, 0.01638079434633255, 0.015865854918956757]\n",
      "epoch:  870 training:  [0.05029506981372833, 0.03580208122730255, 0.048118092119693756]\n",
      "epoch:  880 training:  [0.057162702083587646, 0.01837332174181938, 0.0497559979557991]\n",
      "epoch:  890 training:  [0.04321790486574173, 0.0049897171556949615, 0.023288782685995102]\n",
      "epoch:  900 training:  [0.0408017635345459, 0.012213785201311111, 0.10343395173549652]\n",
      "epoch:  910 training:  [0.05571189150214195, 0.05983852222561836, 0.09443731606006622]\n",
      "epoch:  920 training:  [0.03811158239841461, 0.012555025517940521, 0.09092008322477341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  930 training:  [0.032793182879686356, 0.014565370976924896, 0.05449560284614563]\n",
      "epoch:  940 training:  [0.02832268737256527, 0.009817056357860565, 0.020097212865948677]\n",
      "epoch:  950 training:  [0.03642052412033081, 0.011674363166093826, 0.046858105808496475]\n",
      "epoch:  960 training:  [0.04481282830238342, -0.015736058354377747, 0.049625590443611145]\n",
      "epoch:  970 training:  [0.05344946309924126, 0.07154548913240433, 0.08976475149393082]\n",
      "epoch:  980 training:  [0.02761252596974373, 0.040257152169942856, 0.06497224420309067]\n",
      "epoch:  990 training:  [0.05715734511613846, 0.010157670825719833, 0.04515911266207695]\n",
      "epoch:  1000 training:  [0.030996818095445633, 0.0065995994955301285, 0.06136413663625717]\n",
      "epoch:  1010 training:  [0.03241056948900223, 0.012193076312541962, 0.048627886921167374]\n",
      "epoch:  1020 training:  [0.060212258249521255, 0.0667809545993805, 0.08462502807378769]\n",
      "epoch:  1030 training:  [0.04181305319070816, 0.010978959500789642, 0.06522159278392792]\n",
      "epoch:  1040 training:  [0.027067694813013077, 0.02253120020031929, 0.07058663666248322]\n",
      "epoch:  1050 training:  [0.021653901785612106, 0.021957892924547195, 0.05323644354939461]\n",
      "epoch:  1060 training:  [0.03107462264597416, 0.021723831072449684, 0.05148361995816231]\n",
      "epoch:  1070 training:  [0.032871589064598083, 0.020473800599575043, 0.04410198703408241]\n",
      "epoch:  1080 training:  [0.037942785769701004, 0.06848779320716858, 0.06953949481248856]\n",
      "epoch:  1090 training:  [0.03499958664178848, 0.014275562018156052, 0.05556778982281685]\n",
      "epoch:  1100 training:  [0.023696914315223694, 0.0007014274597167969, 0.03501209616661072]\n",
      "epoch:  1110 training:  [0.040216270834207535, 0.008590538054704666, 0.03590567409992218]\n",
      "epoch:  1120 training:  [0.021279405802488327, -0.0020871013402938843, 0.07765205204486847]\n",
      "epoch:  1130 training:  [0.022797420620918274, 0.010877653956413269, 0.1475735604763031]\n",
      "epoch:  1140 training:  [0.04532133787870407, 0.03630486875772476, 0.07418698817491531]\n",
      "epoch:  1150 training:  [0.02627156674861908, 0.08403606712818146, 0.0814792737364769]\n",
      "epoch:  1160 training:  [0.029176758602261543, -0.006782650947570801, 0.06727151572704315]\n",
      "epoch:  1170 training:  [0.037806157022714615, 0.026469014585018158, 0.05883340165019035]\n",
      "epoch:  1180 training:  [0.024303000420331955, 0.11697537451982498, 0.10937019437551498]\n",
      "epoch:  1190 training:  [0.021422704681754112, 0.009287849068641663, 0.0177021361887455]\n",
      "epoch:  1200 training:  [0.0218048058450222, 0.00951090082526207, 0.030618838965892792]\n",
      "epoch:  1210 training:  [0.02673713117837906, 0.01607087254524231, 0.02028501033782959]\n",
      "epoch:  1220 training:  [0.026038046926259995, 0.02720075473189354, 0.0836733877658844]\n",
      "epoch:  1230 training:  [0.03151312470436096, 0.011493347585201263, 0.058718763291835785]\n",
      "epoch:  1240 training:  [0.04339289665222168, 0.012509826570749283, 0.0453002005815506]\n",
      "epoch:  1250 training:  [0.02347203530371189, 0.03016868233680725, 0.03518076241016388]\n",
      "epoch:  1260 training:  [0.02890399843454361, 0.035424407571554184, 0.037401627749204636]\n",
      "epoch:  1270 training:  [0.032852087169885635, 0.005086220800876617, 0.03260373696684837]\n",
      "epoch:  1280 training:  [0.028106652200222015, 0.02729172259569168, 0.08372281491756439]\n",
      "epoch:  1290 training:  [0.03127724677324295, 0.005358874797821045, 0.09290356934070587]\n",
      "epoch:  1300 training:  [0.011338545940816402, -0.0033010095357894897, 0.030681971460580826]\n",
      "epoch:  1310 training:  [0.034827373921871185, 0.03220437467098236, 0.07111270725727081]\n",
      "epoch:  1320 training:  [0.02341248281300068, 0.023076578974723816, 0.057581618428230286]\n",
      "epoch:  1330 training:  [0.03721384331583977, 0.00380803644657135, 0.008335165679454803]\n",
      "epoch:  1340 training:  [0.019580217078328133, 0.03533226251602173, 0.08493208140134811]\n",
      "epoch:  1350 training:  [0.03537406027317047, 0.03648662567138672, 0.07061947882175446]\n",
      "epoch:  1360 training:  [0.05644490569829941, 0.0409722775220871, 0.03420969843864441]\n",
      "epoch:  1370 training:  [0.0295412577688694, 0.018012896180152893, 0.042707350105047226]\n",
      "epoch:  1380 training:  [0.02835344523191452, 0.07216000556945801, 0.047504063695669174]\n",
      "epoch:  1390 training:  [0.045570749789476395, 0.07072646915912628, 0.03250729292631149]\n",
      "epoch:  1400 training:  [0.022672325372695923, 0.06839080154895782, 0.15056546032428741]\n",
      "epoch:  1410 training:  [0.0239433441311121, 0.009991705417633057, 0.04685770720243454]\n",
      "epoch:  1420 training:  [0.017124302685260773, 0.05432013422250748, 0.044581927359104156]\n",
      "epoch:  1430 training:  [0.015176743268966675, 0.007403954863548279, 0.018562786281108856]\n",
      "epoch:  1440 training:  [0.02643236145377159, 0.029571659862995148, 0.031201235949993134]\n",
      "epoch:  1450 training:  [0.01777670532464981, -0.022311098873615265, 0.0071139708161354065]\n",
      "epoch:  1460 training:  [0.023076115176081657, 0.011979952454566956, 0.05807461217045784]\n",
      "epoch:  1470 training:  [0.010314930230379105, 0.1266528069972992, 0.10047511756420135]\n",
      "epoch:  1480 training:  [0.017404157668352127, 0.022509636357426643, 0.03898605704307556]\n",
      "epoch:  1490 training:  [0.033708952367305756, 0.038737114518880844, 0.10679995268583298]\n",
      "epoch:  1500 training:  [0.0740736797451973, 0.13934829831123352, 0.08841510117053986]\n",
      "epoch:  1510 training:  [0.018418775871396065, 0.027626782655715942, 0.11203188449144363]\n",
      "epoch:  1520 training:  [0.03947187215089798, 0.05929482728242874, 0.019536346197128296]\n",
      "epoch:  1530 training:  [0.02171686291694641, -0.0015598908066749573, 0.029452092945575714]\n",
      "epoch:  1540 training:  [0.015403727069497108, 0.09011809527873993, 0.08837492763996124]\n",
      "epoch:  1550 training:  [0.012190427631139755, 0.002495907247066498, 0.046277664601802826]\n",
      "epoch:  1560 training:  [0.03662855923175812, 0.03581961616873741, 0.08660747855901718]\n",
      "epoch:  1570 training:  [0.015198801644146442, -0.01574159413576126, 0.026150688529014587]\n",
      "epoch:  1580 training:  [0.01809730753302574, -0.0002300366759300232, 0.051995571702718735]\n",
      "epoch:  1590 training:  [0.019428042694926262, -0.018816694617271423, 0.02225668914616108]\n",
      "epoch:  1600 training:  [0.0052183568477630615, -0.0011057853698730469, 0.013927064836025238]\n",
      "epoch:  1610 training:  [0.017607418820261955, 0.014899328351020813, 0.08654305338859558]\n",
      "epoch:  1620 training:  [0.009572029113769531, 0.025554504245519638, 0.10507570207118988]\n",
      "epoch:  1630 training:  [0.005369579419493675, 0.004643097519874573, 0.03220341354608536]\n",
      "epoch:  1640 training:  [-0.005926966667175293, 0.0019764378666877747, 0.022341303527355194]\n",
      "epoch:  1650 training:  [0.003970924764871597, 0.01058652251958847, 0.02540261298418045]\n",
      "epoch:  1660 training:  [0.0023056361824274063, -0.006189450621604919, 0.02123759314417839]\n",
      "epoch:  1670 training:  [0.011331121437251568, -0.003376644104719162, 0.019681187346577644]\n",
      "epoch:  1680 training:  [0.002225887030363083, 0.026762638241052628, 0.03306272625923157]\n",
      "epoch:  1690 training:  [0.008582593873143196, 0.016680806875228882, 0.025335045531392097]\n",
      "epoch:  1700 training:  [0.020545197650790215, 0.032954975962638855, 0.07338135689496994]\n",
      "epoch:  1710 training:  [0.016787346452474594, 0.07165824621915817, 0.06392621994018555]\n",
      "epoch:  1720 training:  [0.02155335620045662, 0.0060876235365867615, 0.07344397902488708]\n",
      "epoch:  1730 training:  [0.02084161713719368, 0.13304738700389862, 0.13830254971981049]\n",
      "epoch:  1740 training:  [0.030688509345054626, 0.035994574427604675, 0.07868197560310364]\n",
      "epoch:  1750 training:  [-0.0005119629204273224, 0.004359781742095947, 0.009655259549617767]\n",
      "epoch:  1760 training:  [0.009587042033672333, 0.047485850751399994, 0.050697579979896545]\n",
      "epoch:  1770 training:  [0.008659779094159603, -0.005374781787395477, 0.019645068794488907]\n",
      "epoch:  1780 training:  [0.010201063007116318, 0.011639609932899475, 0.026580192148685455]\n",
      "epoch:  1790 training:  [0.0068673379719257355, 0.022439822554588318, 0.13576382398605347]\n",
      "epoch:  1800 training:  [0.03276322782039642, 0.06378097832202911, 0.0065743327140808105]\n",
      "epoch:  1810 training:  [0.022569766268134117, 0.042151130735874176, 0.053869955241680145]\n",
      "epoch:  1820 training:  [0.0025996938347816467, 0.009017795324325562, 0.03211714327335358]\n",
      "epoch:  1830 training:  [0.007690420374274254, 0.020947620272636414, 0.037087272852659225]\n",
      "epoch:  1840 training:  [0.012931399047374725, -0.003014657646417618, 0.037177495658397675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1850 training:  [0.006973080337047577, 0.017855454236268997, 0.0559210367500782]\n",
      "epoch:  1860 training:  [0.0065390244126319885, 0.06795811653137207, 0.12372131645679474]\n",
      "epoch:  1870 training:  [0.016286468133330345, 0.026805385947227478, 0.044765230268239975]\n",
      "epoch:  1880 training:  [0.010330302640795708, 0.008847996592521667, 0.03628695011138916]\n",
      "epoch:  1890 training:  [0.01432364247739315, 0.054821498692035675, 0.06492931395769119]\n",
      "epoch:  1900 training:  [0.02367084100842476, 0.10729160904884338, 0.0568460188806057]\n",
      "epoch:  1910 training:  [0.01862134411931038, 0.020668577402830124, 0.015585611574351788]\n",
      "epoch:  1920 training:  [0.011328469961881638, 0.0061258673667907715, 0.034870557487010956]\n",
      "epoch:  1930 training:  [0.008364027366042137, 0.01092321053147316, 0.07346336543560028]\n",
      "epoch:  1940 training:  [0.014184268191456795, 0.005559124052524567, 0.018604308366775513]\n",
      "epoch:  1950 training:  [0.006963448598980904, 0.011600784957408905, 0.028196323662996292]\n",
      "epoch:  1960 training:  [0.015508425422012806, 0.015736691653728485, 0.04984339326620102]\n",
      "epoch:  1970 training:  [0.013420458883047104, 0.02692427486181259, 0.08018020540475845]\n",
      "epoch:  1980 training:  [0.028764959424734116, 0.05699533969163895, 0.03783411905169487]\n",
      "epoch:  1990 training:  [0.023689374327659607, 0.012375153601169586, 0.0723852813243866]\n",
      "epoch:  2000 training:  [0.014674028381705284, 0.013046082109212875, 0.04749084264039993]\n",
      "epoch:  2010 training:  [0.01694532297551632, 0.021260187029838562, 0.09276410937309265]\n",
      "epoch:  2020 training:  [0.02437705546617508, 0.04123174026608467, 0.05781365931034088]\n",
      "epoch:  2030 training:  [0.007993383333086967, 0.005591146647930145, 0.027030201628804207]\n",
      "epoch:  2040 training:  [0.015520084649324417, 0.010109815746545792, 0.049747344106435776]\n",
      "epoch:  2050 training:  [0.00969856046140194, -0.017815299332141876, 0.02660844661295414]\n",
      "epoch:  2060 training:  [-0.002560023218393326, 0.009078063070774078, 0.018604736775159836]\n",
      "epoch:  2070 training:  [-0.001486215740442276, 0.023505941033363342, 0.03608274832367897]\n",
      "epoch:  2080 training:  [0.004154738038778305, 0.012040231376886368, 0.004804443567991257]\n",
      "epoch:  2090 training:  [0.007984228432178497, -0.006200488656759262, 0.01959870755672455]\n",
      "epoch:  2100 training:  [0.0042978450655937195, -0.004260800778865814, 0.01678321324288845]\n",
      "epoch:  2110 training:  [0.012704259715974331, 0.011814091354608536, 0.012838702648878098]\n",
      "epoch:  2120 training:  [0.016595318913459778, 0.007640622556209564, 0.005329892039299011]\n",
      "epoch:  2130 training:  [-0.001936107873916626, 0.014816779643297195, 0.025189440697431564]\n",
      "epoch:  2140 training:  [0.027425691485404968, 0.03425208106637001, 0.048799775540828705]\n",
      "epoch:  2150 training:  [0.021068895235657692, 0.02413947880268097, 0.06568515300750732]\n",
      "epoch:  2160 training:  [0.014467250555753708, 0.03428710252046585, 0.1483030766248703]\n",
      "epoch:  2170 training:  [0.017830392345786095, 0.04766865819692612, 0.07586614042520523]\n",
      "epoch:  2180 training:  [0.016120150685310364, 0.01765751838684082, 0.042080461978912354]\n",
      "epoch:  2190 training:  [0.01812589354813099, 0.04125083237886429, 0.12241093069314957]\n",
      "epoch:  2200 training:  [0.011071689426898956, -0.0024047642946243286, 0.04597342014312744]\n",
      "epoch:  2210 training:  [0.009860215708613396, 0.009788557887077332, 0.0694614127278328]\n",
      "epoch:  2220 training:  [0.008064592257142067, 0.005143031477928162, 0.03531263768672943]\n",
      "epoch:  2230 training:  [0.00909726694226265, 0.013165272772312164, 0.015784909948706627]\n",
      "epoch:  2240 training:  [0.0046285828575491905, 0.010907772928476334, 0.04086150974035263]\n",
      "epoch:  2250 training:  [0.014673862606287003, 0.031936340034008026, 0.059259720146656036]\n",
      "epoch:  2260 training:  [0.012026900425553322, 0.030617032200098038, 0.051932260394096375]\n",
      "epoch:  2270 training:  [0.015613297000527382, 0.02350694313645363, 0.03779244050383568]\n",
      "epoch:  2280 training:  [0.009817872196435928, 0.005177799612283707, 0.023096349090337753]\n",
      "epoch:  2290 training:  [0.014774572104215622, 0.012992087751626968, 0.04795863851904869]\n",
      "epoch:  2300 training:  [0.004276065155863762, 0.023551464080810547, 0.06226704642176628]\n",
      "epoch:  2310 training:  [0.0215386301279068, 0.053747229278087616, 0.12169403582811356]\n",
      "epoch:  2320 training:  [0.021966004744172096, 0.020554039627313614, 0.07941137999296188]\n",
      "epoch:  2330 training:  [0.027988286688923836, 0.04672238230705261, 0.17908790707588196]\n",
      "epoch:  2340 training:  [0.011671707965433598, 0.017550352960824966, 0.04087822884321213]\n",
      "epoch:  2350 training:  [0.001177290454506874, 0.011804349720478058, 0.03078596293926239]\n",
      "epoch:  2360 training:  [0.01853426732122898, -0.006245635449886322, 0.05805797874927521]\n",
      "epoch:  2370 training:  [0.022058920934796333, 0.0223768949508667, 0.07080917805433273]\n",
      "epoch:  2380 training:  [0.019538911059498787, 0.0016597285866737366, 0.05538778752088547]\n",
      "epoch:  2390 training:  [0.01993340253829956, 0.03979377821087837, 0.06279688328504562]\n",
      "epoch:  2400 training:  [0.010137110948562622, 0.0038909167051315308, 0.01881394535303116]\n",
      "epoch:  2410 training:  [0.006524115800857544, 0.006315164268016815, 0.04190436005592346]\n",
      "epoch:  2420 training:  [0.012028688564896584, 0.0019126459956169128, 0.0343591570854187]\n",
      "epoch:  2430 training:  [0.0052816541865468025, -0.0010744482278823853, 0.014642201364040375]\n",
      "epoch:  2440 training:  [0.01182018406689167, 0.001954440027475357, 0.040047407150268555]\n",
      "epoch:  2450 training:  [0.00849938578903675, 0.026580318808555603, 0.04479677230119705]\n",
      "epoch:  2460 training:  [0.007903452962636948, 0.0033456385135650635, 0.03974508121609688]\n",
      "epoch:  2470 training:  [0.006903966888785362, 0.004085119813680649, 0.010545842349529266]\n",
      "epoch:  2480 training:  [0.002077728509902954, 0.0033549219369888306, 0.015871405601501465]\n",
      "epoch:  2490 training:  [0.007793998811393976, 0.0009370297193527222, 0.027153898030519485]\n",
      "epoch:  2500 training:  [0.007216950878500938, 0.006086142733693123, 0.029043737798929214]\n",
      "epoch:  2510 training:  [0.00047456473112106323, -0.012111306190490723, 0.019392147660255432]\n",
      "epoch:  2520 training:  [0.006346368230879307, 0.015422333031892776, 0.01387740671634674]\n",
      "epoch:  2530 training:  [0.00835137628018856, 0.0172731950879097, 0.023497089743614197]\n",
      "epoch:  2540 training:  [-0.0015455102548003197, 0.004081349819898605, 0.017812058329582214]\n",
      "epoch:  2550 training:  [0.008538290858268738, 0.046235691756010056, 0.026546798646450043]\n",
      "epoch:  2560 training:  [0.015652019530534744, 0.008983470499515533, 0.02056870609521866]\n",
      "epoch:  2570 training:  [0.0036845002323389053, 0.016639508306980133, 0.044558700174093246]\n",
      "epoch:  2580 training:  [0.0053621381521224976, 0.03034568578004837, 0.02706748992204666]\n",
      "epoch:  2590 training:  [0.011513940989971161, -0.00023246556520462036, 0.04771719127893448]\n",
      "epoch:  2600 training:  [0.01007615216076374, 0.013159532099962234, 0.021135475486516953]\n",
      "epoch:  2610 training:  [0.028175782412290573, 0.01076933741569519, 0.06438654661178589]\n",
      "epoch:  2620 training:  [0.017447303980588913, 0.00010141357779502869, 0.010529851540923119]\n",
      "epoch:  2630 training:  [0.004942743107676506, 0.0037907585501670837, 0.030032657086849213]\n",
      "epoch:  2640 training:  [0.013018222525715828, 0.0015515387058258057, 0.008666068315505981]\n",
      "epoch:  2650 training:  [0.011891167610883713, 0.015902001410722733, 0.02552880346775055]\n",
      "epoch:  2660 training:  [0.011024298146367073, -0.0019484162330627441, 0.029690425843000412]\n",
      "epoch:  2670 training:  [0.004069432616233826, -0.010875068604946136, 0.00671803206205368]\n",
      "epoch:  2680 training:  [0.0075396462343633175, 0.0009450390934944153, 0.02849172055721283]\n",
      "epoch:  2690 training:  [0.00797304604202509, -0.000721566379070282, 0.016375621780753136]\n",
      "epoch:  2700 training:  [0.004216167144477367, 0.0035989880561828613, 0.019817791879177094]\n",
      "epoch:  2710 training:  [0.0028855414129793644, 0.0027201026678085327, 0.014915350824594498]\n",
      "epoch:  2720 training:  [0.009387712925672531, 0.014934875071048737, 0.03794252127408981]\n",
      "epoch:  2730 training:  [0.014355096034705639, 0.033921923488378525, 0.04503558203577995]\n",
      "epoch:  2740 training:  [0.010550246573984623, -0.001149989664554596, 0.019224293529987335]\n",
      "epoch:  2750 training:  [0.011007300578057766, 0.011138513684272766, 0.03666362166404724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2760 training:  [0.012330565601587296, 0.015370678156614304, 0.03830689936876297]\n",
      "epoch:  2770 training:  [0.012301363050937653, 0.0076060667634010315, 0.03996308147907257]\n",
      "epoch:  2780 training:  [0.021229347214102745, 0.025635823607444763, 0.022101737558841705]\n",
      "epoch:  2790 training:  [0.013555431738495827, -0.0011057443916797638, 0.05951905995607376]\n",
      "epoch:  2800 training:  [0.013531045988202095, -0.007452338933944702, 0.019014030694961548]\n",
      "epoch:  2810 training:  [0.010328955948352814, 0.005980063229799271, 0.01847117394208908]\n",
      "epoch:  2820 training:  [0.008055157959461212, 0.0016368776559829712, 0.016589198261499405]\n",
      "epoch:  2830 training:  [0.014667998999357224, 0.026677973568439484, 0.04487504065036774]\n",
      "epoch:  2840 training:  [0.01106935366988182, 0.016098812222480774, 0.07239707559347153]\n",
      "epoch:  2850 training:  [0.006292965263128281, 0.04233580455183983, 0.02791139855980873]\n",
      "epoch:  2860 training:  [0.011967789381742477, 0.01890932023525238, 0.02968434803187847]\n",
      "epoch:  2870 training:  [0.018249008804559708, 0.006732761859893799, 0.06332896649837494]\n",
      "epoch:  2880 training:  [0.017295926809310913, 0.012353643774986267, 0.09287070482969284]\n",
      "epoch:  2890 training:  [0.011185096576809883, 0.020842960104346275, 0.04395268112421036]\n",
      "epoch:  2900 training:  [0.01543300598859787, 0.013243507593870163, 0.019837653264403343]\n",
      "epoch:  2910 training:  [0.013718300499022007, 0.0412120558321476, 0.017749235033988953]\n",
      "epoch:  2920 training:  [0.0028480365872383118, 0.007611356675624847, 0.02512393519282341]\n",
      "epoch:  2930 training:  [0.022575391456484795, 0.02733062580227852, 0.09273733198642731]\n",
      "epoch:  2940 training:  [0.01833329163491726, 0.011237174272537231, 0.06054399907588959]\n",
      "epoch:  2950 training:  [0.007119230926036835, 0.012596622109413147, 0.017053231596946716]\n",
      "epoch:  2960 training:  [0.011957995593547821, 0.07245559245347977, 0.07129893451929092]\n",
      "epoch:  2970 training:  [0.013035156764090061, 0.006168235093355179, 0.0732496827840805]\n",
      "epoch:  2980 training:  [0.000896969810128212, 0.009669207036495209, 0.02693537250161171]\n",
      "epoch:  2990 training:  [0.008958408609032631, -0.0033461153507232666, 0.020436853170394897]\n",
      "epoch:  3000 training:  [0.008185265585780144, 0.010471295565366745, 0.01940310001373291]\n",
      "epoch:  3010 training:  [0.02216567099094391, 0.015632551163434982, 0.04912872612476349]\n",
      "epoch:  3020 training:  [0.008544095791876316, 0.005217559635639191, 0.023378292098641396]\n",
      "epoch:  3030 training:  [0.029348745942115784, 0.0456700474023819, 0.020897401496767998]\n",
      "epoch:  3040 training:  [0.021516427397727966, 0.03828377649188042, 0.05363913252949715]\n",
      "epoch:  3050 training:  [0.010569916106760502, 0.021228618919849396, 0.06494063884019852]\n",
      "epoch:  3060 training:  [0.014098652638494968, 0.016320250928401947, 0.023283032700419426]\n",
      "epoch:  3070 training:  [0.0018049497157335281, -0.007376424968242645, 0.020075451582670212]\n",
      "epoch:  3080 training:  [0.013532040640711784, 0.022429613396525383, 0.05562737211585045]\n",
      "epoch:  3090 training:  [0.012127755209803581, 0.009320512413978577, 0.009207990020513535]\n",
      "epoch:  3100 training:  [0.010513395071029663, -0.004764102399349213, 0.009014997631311417]\n",
      "epoch:  3110 training:  [0.009280284866690636, 0.0028448477387428284, 0.01652323454618454]\n",
      "epoch:  3120 training:  [0.007995503023266792, 0.008880797773599625, 0.026694294065237045]\n",
      "epoch:  3130 training:  [0.0038836337625980377, 0.01125270128250122, 0.02600100263953209]\n",
      "epoch:  3140 training:  [0.01537440624088049, -0.0058739930391311646, 0.014929469674825668]\n",
      "epoch:  3150 training:  [0.005598240066319704, 0.01721709966659546, 0.030231766402721405]\n",
      "epoch:  3160 training:  [0.007549382746219635, 0.013430863618850708, 0.028489597141742706]\n",
      "epoch:  3170 training:  [0.0025432035326957703, 0.022029772400856018, 0.052426304668188095]\n",
      "epoch:  3180 training:  [0.015167105942964554, 0.02923394739627838, 0.040429212152957916]\n",
      "epoch:  3190 training:  [0.03814676031470299, 0.04035738483071327, 0.08049099147319794]\n",
      "epoch:  3200 training:  [0.010473065078258514, 0.021365270018577576, 0.032211363315582275]\n",
      "epoch:  3210 training:  [0.004891578108072281, -0.0015702806413173676, 0.03270082548260689]\n",
      "epoch:  3220 training:  [0.004120960831642151, 0.005283191800117493, 0.04689377546310425]\n",
      "epoch:  3230 training:  [0.010124001652002335, -0.007493868470191956, 0.03518105298280716]\n",
      "epoch:  3240 training:  [0.006466470658779144, 0.015636511147022247, 0.07260836660861969]\n",
      "epoch:  3250 training:  [0.012431127950549126, 0.011455874890089035, 0.053182415664196014]\n",
      "epoch:  3260 training:  [0.005320310592651367, 0.001231275498867035, 0.021744923666119576]\n",
      "epoch:  3270 training:  [0.010486905463039875, 0.005058404058218002, 0.011562922969460487]\n",
      "epoch:  3280 training:  [0.007059197872877121, -0.001969262957572937, 0.034553222358226776]\n",
      "epoch:  3290 training:  [0.006997084245085716, 0.04001506790518761, 0.0860837921500206]\n",
      "epoch:  3300 training:  [0.006170113105326891, 0.006075009703636169, 0.014386247843503952]\n",
      "epoch:  3310 training:  [0.006018894724547863, 0.024433135986328125, 0.05421143025159836]\n",
      "epoch:  3320 training:  [0.006182187236845493, 0.026952702552080154, 0.03652283176779747]\n",
      "epoch:  3330 training:  [0.018274767324328423, 0.019593380391597748, 0.03987515717744827]\n",
      "epoch:  3340 training:  [0.009087132290005684, 0.031001701951026917, 0.08457411825656891]\n",
      "epoch:  3350 training:  [0.01405191421508789, 0.047716379165649414, 0.06833134591579437]\n",
      "epoch:  3360 training:  [0.0016875900328159332, 0.014011751860380173, 0.05577104911208153]\n",
      "epoch:  3370 training:  [0.01213537622243166, 0.023400481790304184, 0.040535978972911835]\n",
      "epoch:  3380 training:  [0.009295070543885231, 0.0013429932296276093, 0.01219593733549118]\n",
      "epoch:  3390 training:  [0.0021010423079133034, 0.00512830913066864, 0.05262228474020958]\n",
      "epoch:  3400 training:  [0.01238691434264183, 0.01575753092765808, 0.04808023199439049]\n",
      "epoch:  3410 training:  [0.00274021178483963, 0.0095069520175457, 0.04815424233675003]\n",
      "epoch:  3420 training:  [0.02611064910888672, 0.018365640193223953, 0.07479587942361832]\n",
      "epoch:  3430 training:  [0.004439458250999451, -0.01703523099422455, 0.005057908594608307]\n",
      "epoch:  3440 training:  [0.014657741412520409, 0.03309355676174164, 0.07218249887228012]\n",
      "epoch:  3450 training:  [0.0117507204413414, 0.0030618607997894287, 0.010734230279922485]\n",
      "epoch:  3460 training:  [0.010032732039690018, 0.004313521087169647, 0.03291430324316025]\n",
      "epoch:  3470 training:  [0.01032630167901516, 0.02138007991015911, 0.05528354272246361]\n",
      "epoch:  3480 training:  [0.020900579169392586, 0.003758452832698822, 0.021513335406780243]\n",
      "epoch:  3490 training:  [0.013158397749066353, 0.027468904852867126, 0.037700310349464417]\n",
      "epoch:  3500 training:  [-0.001712121069431305, -0.010758452117443085, 0.008786827325820923]\n",
      "epoch:  3510 training:  [0.022823898121714592, 0.014030098915100098, 0.06134428083896637]\n",
      "epoch:  3520 training:  [0.017306260764598846, 0.02677806466817856, 0.06986309587955475]\n",
      "epoch:  3530 training:  [0.005776524543762207, 0.007612481713294983, 0.02116650715470314]\n",
      "epoch:  3540 training:  [0.031186964362859726, 0.01506645604968071, 0.02161312848329544]\n",
      "epoch:  3550 training:  [0.007573127746582031, -3.746151924133301e-05, 0.01579892262816429]\n",
      "epoch:  3560 training:  [0.0076761264353990555, 0.0020415112376213074, 0.01872207224369049]\n",
      "epoch:  3570 training:  [0.007735351100564003, 0.011118743568658829, 0.034493036568164825]\n",
      "epoch:  3580 training:  [0.016619889065623283, 0.024985112249851227, 0.08226390182971954]\n",
      "epoch:  3590 training:  [0.011094840243458748, 0.004957087337970734, 0.03727886825799942]\n",
      "epoch:  3600 training:  [0.02252724952995777, 0.021020811051130295, 0.09035034477710724]\n",
      "epoch:  3610 training:  [0.006642857566475868, 0.00158584862947464, 0.003816939890384674]\n",
      "epoch:  3620 training:  [0.01577187143266201, 0.014110848307609558, 0.031180335208773613]\n",
      "epoch:  3630 training:  [-0.001996152102947235, 0.0029921792447566986, 0.0186130590736866]\n",
      "epoch:  3640 training:  [0.003723694011569023, 0.0023301132023334503, 0.03641152381896973]\n",
      "epoch:  3650 training:  [0.00952629093080759, 0.018031079322099686, 0.01772984489798546]\n",
      "epoch:  3660 training:  [0.0038636811077594757, 0.0017579421401023865, -0.0006879791617393494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  3670 training:  [0.005728365853428841, 0.0004261434078216553, 0.009812891483306885]\n",
      "epoch:  3680 training:  [0.0023781675845384598, -0.003974303603172302, 0.027809370309114456]\n",
      "epoch:  3690 training:  [0.005373004823923111, 0.0075819529592990875, 0.027623549103736877]\n",
      "epoch:  3700 training:  [0.005721655674278736, 0.007194120436906815, 0.04508807510137558]\n",
      "epoch:  3710 training:  [0.004670598544180393, 0.009105071425437927, 0.04840580374002457]\n",
      "epoch:  3720 training:  [0.0034453272819519043, 0.0009968318045139313, 0.012265969067811966]\n",
      "epoch:  3730 training:  [0.013204891234636307, 0.0005998127162456512, 0.0016507506370544434]\n",
      "epoch:  3740 training:  [0.0033029038459062576, 0.004992540925741196, 0.0486859455704689]\n",
      "epoch:  3750 training:  [-0.0018662279471755028, 0.0003766566514968872, 0.023477332666516304]\n",
      "epoch:  3760 training:  [0.012055226601660252, 0.031221045181155205, 0.04067142307758331]\n",
      "epoch:  3770 training:  [0.02309131622314453, 0.029926005750894547, 0.05940253287553787]\n",
      "epoch:  3780 training:  [0.005389717407524586, 0.0033992379903793335, -0.008160579949617386]\n",
      "epoch:  3790 training:  [0.015197118744254112, 0.03018862009048462, 0.05852385610342026]\n",
      "epoch:  3800 training:  [0.0038149775937199593, 0.0012141428887844086, 0.01734934188425541]\n",
      "epoch:  3810 training:  [0.015856333076953888, 0.051312923431396484, 0.07643201947212219]\n",
      "epoch:  3820 training:  [0.0010735616087913513, -0.006851993501186371, 0.024738438427448273]\n",
      "epoch:  3830 training:  [0.016239605844020844, 0.011276811361312866, 0.0523664653301239]\n",
      "epoch:  3840 training:  [0.0199690330773592, 0.03412960469722748, 0.053216561675071716]\n",
      "epoch:  3850 training:  [0.002312745898962021, -0.0013570301234722137, 0.03675730153918266]\n",
      "epoch:  3860 training:  [0.022226285189390182, 0.03630749508738518, 0.053662948310375214]\n",
      "epoch:  3870 training:  [0.006084412336349487, -0.00852377712726593, 0.022352686151862144]\n",
      "epoch:  3880 training:  [0.004038732498884201, 0.0357494056224823, 0.012511339038610458]\n",
      "epoch:  3890 training:  [0.008137064054608345, 0.005614906549453735, 0.017010020092129707]\n",
      "epoch:  3900 training:  [0.0022901389747858047, 0.0023626796901226044, 0.010499297641217709]\n",
      "epoch:  3910 training:  [0.0077520087361335754, 0.00670415535569191, 0.015923699364066124]\n",
      "epoch:  3920 training:  [0.014648854732513428, 0.01049894094467163, 0.025792397558689117]\n",
      "epoch:  3930 training:  [0.019991066306829453, 0.021384645253419876, 0.010641541332006454]\n",
      "epoch:  3940 training:  [0.008893858641386032, 0.013824079185724258, 0.032509684562683105]\n",
      "epoch:  3950 training:  [0.0040701329708099365, 0.003706827759742737, 0.012516524642705917]\n",
      "epoch:  3960 training:  [0.01088039018213749, 0.00914023071527481, 0.07253943383693695]\n",
      "epoch:  3970 training:  [0.010984936729073524, -0.0056014284491539, 0.061311766505241394]\n",
      "epoch:  3980 training:  [0.0005348715931177139, -0.008691873401403427, 0.032080553472042084]\n",
      "epoch:  3990 training:  [0.015187708660960197, 0.057378023862838745, 0.05352061241865158]\n",
      "epoch:  4000 training:  [0.009920553304255009, 0.0022874437272548676, 0.025273369625210762]\n",
      "epoch:  4010 training:  [0.01571737229824066, -0.01243775337934494, 0.06870007514953613]\n",
      "epoch:  4020 training:  [0.016787586733698845, 0.010004578158259392, 0.05167151987552643]\n",
      "epoch:  4030 training:  [0.021961966529488564, 0.013381674885749817, 0.09296397119760513]\n",
      "epoch:  4040 training:  [0.015610065311193466, -0.0003947727382183075, 0.024345915764570236]\n",
      "epoch:  4050 training:  [0.013817921280860901, 0.017171557992696762, 0.06532618403434753]\n",
      "epoch:  4060 training:  [0.009947527199983597, -0.013297110795974731, 0.017085086554288864]\n",
      "epoch:  4070 training:  [0.006891574710607529, 0.01899830251932144, 0.03237763047218323]\n",
      "epoch:  4080 training:  [0.003568548709154129, 0.022512391209602356, 0.06267662346363068]\n",
      "epoch:  4090 training:  [0.012751981616020203, 0.014451906085014343, 0.0833989828824997]\n",
      "epoch:  4100 training:  [0.025530733168125153, 0.02218467742204666, 0.08829784393310547]\n",
      "epoch:  4110 training:  [0.0035283530596643686, 0.009087588638067245, 0.02278868667781353]\n",
      "epoch:  4120 training:  [0.015439707785844803, 0.06806226074695587, 0.05417421832680702]\n",
      "epoch:  4130 training:  [0.004949785768985748, 0.014682777225971222, 0.0690714418888092]\n",
      "epoch:  4140 training:  [0.0037055909633636475, 0.013633515685796738, 0.05197528004646301]\n",
      "epoch:  4150 training:  [0.010275758802890778, 0.036318641155958176, 0.018427573144435883]\n",
      "epoch:  4160 training:  [0.005861924961209297, -0.004489123821258545, 0.014427457004785538]\n",
      "epoch:  4170 training:  [0.0057283081114292145, 0.02704661339521408, 0.04044032096862793]\n",
      "epoch:  4180 training:  [0.007132647559046745, 0.019926778972148895, 0.01663445308804512]\n",
      "epoch:  4190 training:  [0.0016488353721797466, 0.003943227231502533, 0.015987303107976913]\n",
      "epoch:  4200 training:  [0.004031805321574211, 0.0013537779450416565, 0.00794946774840355]\n",
      "epoch:  4210 training:  [-0.0068581849336624146, 0.005931355059146881, -0.004771243780851364]\n",
      "epoch:  4220 training:  [0.01105793658643961, 0.007859334349632263, 0.02515432983636856]\n",
      "epoch:  4230 training:  [0.008509671315550804, 0.0074778273701667786, 0.02398821711540222]\n",
      "epoch:  4240 training:  [0.0035843821242451668, 0.007268507033586502, 0.008429575711488724]\n",
      "epoch:  4250 training:  [0.00011201202869415283, 0.0030641593039035797, 0.013288429006934166]\n",
      "epoch:  4260 training:  [0.006784532219171524, -0.0019390396773815155, 0.014761018566787243]\n",
      "epoch:  4270 training:  [0.0063118645921349525, -0.002220042049884796, 0.02217061258852482]\n",
      "epoch:  4280 training:  [0.0033847633749246597, 0.0014492571353912354, 0.010614078491926193]\n",
      "epoch:  4290 training:  [0.00035582762211561203, 0.007167790085077286, 0.014225320890545845]\n",
      "epoch:  4300 training:  [0.004202451556921005, 0.0032020211219787598, 0.026145562529563904]\n",
      "epoch:  4310 training:  [0.015353647992014885, 0.0027675600722432137, 0.013619540259242058]\n",
      "epoch:  4320 training:  [0.016034774482250214, 0.007575567811727524, 0.05915835127234459]\n",
      "epoch:  4330 training:  [0.005343712866306305, 0.0021328330039978027, 0.014403925277292728]\n",
      "epoch:  4340 training:  [0.008813219144940376, 0.0029121078550815582, 0.0196441188454628]\n",
      "epoch:  4350 training:  [0.012597780674695969, 0.0225522518157959, 0.08733299374580383]\n",
      "epoch:  4360 training:  [0.014963009394705296, 0.033335231244564056, 0.0805012434720993]\n",
      "epoch:  4370 training:  [0.007346948608756065, -0.002856586128473282, 0.01783829741179943]\n",
      "epoch:  4380 training:  [0.011197913438081741, 0.007353220134973526, 0.05951496958732605]\n",
      "epoch:  4390 training:  [0.014046949334442616, 0.05905367434024811, 0.06108683720231056]\n",
      "epoch:  4400 training:  [0.014334576204419136, 0.05357859283685684, 0.05271627753973007]\n",
      "epoch:  4410 training:  [0.006864368915557861, 0.021973654627799988, 0.05759108066558838]\n",
      "epoch:  4420 training:  [0.015523591078817844, 0.021801389753818512, 0.017544947564601898]\n",
      "epoch:  4430 training:  [0.004665415734052658, 0.007758080959320068, 0.023421673104166985]\n",
      "epoch:  4440 training:  [0.011280104517936707, 0.0401463508605957, 0.04794666916131973]\n",
      "epoch:  4450 training:  [0.008323617279529572, 0.047829948365688324, 0.03823171555995941]\n",
      "epoch:  4460 training:  [0.011060746386647224, 0.002158910036087036, 0.03026587888598442]\n",
      "epoch:  4470 training:  [0.02300112321972847, 0.019347000867128372, 0.053675565868616104]\n",
      "epoch:  4480 training:  [0.021025050431489944, 0.020840689539909363, 0.009952852502465248]\n",
      "epoch:  4490 training:  [0.006883678957819939, 0.009474337100982666, 0.023659687489271164]\n",
      "epoch:  4500 training:  [0.0032568518072366714, 0.02760184183716774, 0.015259109437465668]\n",
      "epoch:  4510 training:  [0.007495215628296137, 0.009619627147912979, 0.026104699820280075]\n",
      "epoch:  4520 training:  [0.012507048435509205, 0.004719309508800507, 0.015482302755117416]\n",
      "epoch:  4530 training:  [0.003758750855922699, 0.017867766320705414, 0.006796497851610184]\n",
      "epoch:  4540 training:  [0.00817783921957016, 0.010349363088607788, 0.04315192252397537]\n",
      "epoch:  4550 training:  [0.00895235687494278, 0.012750023975968361, 0.02605876326560974]\n",
      "epoch:  4560 training:  [0.012257679365575314, 0.03741192817687988, 0.013931699097156525]\n",
      "epoch:  4570 training:  [0.0015156064182519913, -0.0214858315885067, 0.028833653777837753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4580 training:  [0.01884627714753151, 0.037878476083278656, 0.08367260545492172]\n",
      "epoch:  4590 training:  [0.009058183059096336, 0.023271046578884125, 0.06722207367420197]\n",
      "epoch:  4600 training:  [0.009389450773596764, 0.0031278282403945923, 0.012172643095254898]\n",
      "epoch:  4610 training:  [0.020591584965586662, 0.018954038619995117, 0.06039438396692276]\n",
      "epoch:  4620 training:  [0.00963791273534298, 0.004891064018011093, 0.029116716235876083]\n",
      "epoch:  4630 training:  [0.004677146673202515, 0.00936858355998993, 0.037444960325956345]\n",
      "epoch:  4640 training:  [0.007834233343601227, 0.016708742827177048, 0.017455147579312325]\n",
      "epoch:  4650 training:  [0.011141810566186905, 0.027693919837474823, 0.06588390469551086]\n",
      "epoch:  4660 training:  [0.008896180428564548, 0.012890055775642395, 0.021397965028882027]\n",
      "epoch:  4670 training:  [0.003028072416782379, -0.004061639308929443, 0.004449591040611267]\n",
      "epoch:  4680 training:  [0.009375471621751785, 0.004770427942276001, 0.02318500727415085]\n",
      "epoch:  4690 training:  [0.011892909184098244, 0.014032788574695587, 0.03189188241958618]\n",
      "epoch:  4700 training:  [0.008965902030467987, 0.008047319948673248, 0.04478038474917412]\n",
      "epoch:  4710 training:  [0.0065144868567585945, 0.009562220424413681, 0.018196526914834976]\n",
      "epoch:  4720 training:  [0.010200697928667068, 0.011483661830425262, 0.01780535653233528]\n",
      "epoch:  4730 training:  [0.008716782554984093, 0.009412199258804321, 0.04366430640220642]\n",
      "epoch:  4740 training:  [0.0023850537836551666, 0.004880458116531372, 0.02846025675535202]\n",
      "epoch:  4750 training:  [0.0063730087131261826, 0.005848005414009094, 0.018595930188894272]\n",
      "epoch:  4760 training:  [0.00031344592571258545, 0.010186001658439636, 0.021458402276039124]\n",
      "epoch:  4770 training:  [0.006087558344006538, -0.0009012818336486816, 0.02260773815214634]\n",
      "epoch:  4780 training:  [0.0025288723409175873, -0.0005051009356975555, 0.006938043981790543]\n",
      "epoch:  4790 training:  [0.011457204818725586, 0.007333077490329742, 0.014998318627476692]\n",
      "epoch:  4800 training:  [0.00035195425152778625, 0.002659037709236145, -0.020806044340133667]\n",
      "epoch:  4810 training:  [0.005308609455823898, 0.002320893108844757, 0.02478274330496788]\n",
      "epoch:  4820 training:  [0.012080475687980652, 0.014578640460968018, 0.06829347461462021]\n",
      "epoch:  4830 training:  [0.006949812173843384, 0.002692297101020813, 0.023685583844780922]\n",
      "epoch:  4840 training:  [0.006538240239024162, -0.009542860090732574, -0.007825680077075958]\n",
      "epoch:  4850 training:  [0.018051911145448685, 0.042840562760829926, 0.12031485140323639]\n",
      "epoch:  4860 training:  [0.01540474034845829, 0.010727547109127045, 0.1257784068584442]\n",
      "epoch:  4870 training:  [0.012946492061018944, 0.02558884769678116, 0.13853925466537476]\n",
      "epoch:  4880 training:  [0.005772237665951252, 0.013763993978500366, 0.054444923996925354]\n",
      "epoch:  4890 training:  [0.019711539149284363, -0.0036008208990097046, 0.06314059346914291]\n",
      "epoch:  4900 training:  [0.012335319072008133, 0.019285963848233223, 0.07278560101985931]\n",
      "epoch:  4910 training:  [0.0036063529551029205, -0.008565392345190048, 0.004835218191146851]\n",
      "epoch:  4920 training:  [0.021000081673264503, 0.04295125603675842, 0.09042367339134216]\n",
      "epoch:  4930 training:  [0.012292449362576008, 0.013713948428630829, 0.0807890072464943]\n",
      "epoch:  4940 training:  [0.0014655590057373047, 0.003046829253435135, 0.04019533470273018]\n",
      "epoch:  4950 training:  [0.01669854111969471, 0.033001892268657684, 0.06398206204175949]\n",
      "epoch:  4960 training:  [0.006678041070699692, 0.005241416394710541, 0.021099552512168884]\n",
      "epoch:  4970 training:  [0.02094544656574726, 0.019271567463874817, 0.1620941013097763]\n",
      "epoch:  4980 training:  [-0.0008577592670917511, 0.017710678279399872, 0.06869778037071228]\n",
      "epoch:  4990 training:  [0.010707560926675797, 0.004281159490346909, 0.015081115067005157]\n",
      "epoch:  5000 training:  [0.009338444098830223, 0.01236603781580925, 0.016475776210427284]\n",
      "epoch:  5010 training:  [-0.005011027678847313, 0.002361513674259186, 0.06666868925094604]\n",
      "epoch:  5020 training:  [0.013568306341767311, 0.006683479994535446, 0.009530961513519287]\n",
      "epoch:  5030 training:  [0.005955997388809919, 0.008617982268333435, 0.01235407218337059]\n",
      "epoch:  5040 training:  [0.006883959285914898, 0.007827825844287872, 0.02407548390328884]\n",
      "epoch:  5050 training:  [0.006402384024113417, 0.0045204609632492065, 0.02826094627380371]\n",
      "epoch:  5060 training:  [0.001975740073248744, 0.002106577157974243, 0.049390196800231934]\n",
      "epoch:  5070 training:  [0.009062101133167744, -0.0037403441965579987, 0.02352149784564972]\n",
      "epoch:  5080 training:  [0.006237467750906944, 0.0038006380200386047, 0.062386468052864075]\n",
      "epoch:  5090 training:  [0.004667408764362335, 0.006279751658439636, 0.026444263756275177]\n",
      "epoch:  5100 training:  [0.015113599598407745, 0.0022360943257808685, 0.03534925729036331]\n",
      "epoch:  5110 training:  [0.006644760258495808, 0.04079412296414375, 0.031173642724752426]\n",
      "epoch:  5120 training:  [0.009630749002099037, 0.002477891743183136, 0.04501171410083771]\n",
      "epoch:  5130 training:  [0.014241865836083889, -0.0026972107589244843, 0.018453164026141167]\n",
      "epoch:  5140 training:  [0.01638048328459263, 0.03843972086906433, 0.061130743473768234]\n",
      "epoch:  5150 training:  [0.006092453841120005, 0.0028871074318885803, 0.01699228584766388]\n",
      "epoch:  5160 training:  [0.003493804484605789, 0.002932898700237274, 0.03649338334798813]\n",
      "epoch:  5170 training:  [0.021839991211891174, 0.022315524518489838, 0.09681305289268494]\n",
      "epoch:  5180 training:  [0.015388780273497105, 0.004682958126068115, 0.07267171889543533]\n",
      "epoch:  5190 training:  [0.010011401027441025, -0.0013550110161304474, 0.029728233814239502]\n",
      "epoch:  5200 training:  [0.010397590696811676, 0.044466566294431686, 0.026841863989830017]\n",
      "epoch:  5210 training:  [0.008800916373729706, 0.02375776693224907, 0.03324780985713005]\n",
      "epoch:  5220 training:  [0.007813941687345505, 0.014407727867364883, 0.037591733038425446]\n",
      "epoch:  5230 training:  [0.0057725850492715836, 0.009425725787878036, 0.028311043977737427]\n",
      "epoch:  5240 training:  [0.006176106631755829, 0.0025090649724006653, 0.02807113155722618]\n",
      "epoch:  5250 training:  [0.007461825385689735, 0.034388214349746704, 0.06746995449066162]\n",
      "epoch:  5260 training:  [0.006593823898583651, -0.00978744775056839, 0.021905360743403435]\n",
      "epoch:  5270 training:  [0.004424097016453743, 0.03567139059305191, 0.028234999626874924]\n",
      "epoch:  5280 training:  [0.007855460047721863, 0.044829096645116806, 0.04716040939092636]\n",
      "epoch:  5290 training:  [0.005064772441983223, 0.021321497857570648, 0.022778302431106567]\n",
      "epoch:  5300 training:  [0.0017924755811691284, 0.005874618887901306, 0.030739326030015945]\n",
      "epoch:  5310 training:  [0.004789970815181732, 0.0021756552159786224, 0.009240752086043358]\n",
      "epoch:  5320 training:  [0.009620973840355873, 0.004582324996590614, 0.013327522203326225]\n",
      "epoch:  5330 training:  [0.013937425799667835, 0.00876227393746376, 0.09286253154277802]\n",
      "epoch:  5340 training:  [0.0027705300599336624, 0.009973332285881042, 0.09437224268913269]\n",
      "epoch:  5350 training:  [0.003985654562711716, 0.01813618652522564, 0.03032226860523224]\n",
      "epoch:  5360 training:  [0.013375705108046532, 0.055715471506118774, 0.04126930981874466]\n",
      "epoch:  5370 training:  [0.007999611087143421, -0.014144062995910645, 0.01262996718287468]\n",
      "epoch:  5380 training:  [0.005716577637940645, -0.00405825674533844, 0.031021825969219208]\n",
      "epoch:  5390 training:  [0.0004115849733352661, 0.0019474774599075317, 0.015377882868051529]\n",
      "epoch:  5400 training:  [0.01893223635852337, 0.019284330308437347, 0.0704951360821724]\n",
      "epoch:  5410 training:  [0.007972298189997673, 0.014464613050222397, 0.07988214492797852]\n",
      "epoch:  5420 training:  [0.007677873596549034, 0.022867955267429352, 0.004236005246639252]\n",
      "epoch:  5430 training:  [0.014990448020398617, 0.001502998173236847, 0.02824103832244873]\n",
      "epoch:  5440 training:  [0.00900723785161972, -0.008599914610385895, 0.007265876978635788]\n",
      "epoch:  5450 training:  [0.0037160441279411316, -0.0015835613012313843, 0.04674966633319855]\n",
      "epoch:  5460 training:  [0.016557754948735237, 0.002348169684410095, 0.03031761944293976]\n",
      "epoch:  5470 training:  [0.006524762138724327, -0.025413289666175842, 0.006408233195543289]\n",
      "epoch:  5480 training:  [0.0016670720651745796, 0.014845065772533417, 0.054889872670173645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5490 training:  [0.004976317286491394, 0.0034876689314842224, 0.009755147621035576]\n",
      "epoch:  5500 training:  [0.005745317786931992, 0.0037778951227664948, 0.015789654105901718]\n",
      "epoch:  5510 training:  [0.010787718929350376, 0.029218921437859535, 0.04383781924843788]\n",
      "epoch:  5520 training:  [0.008355516009032726, 0.007677685469388962, 0.011583656072616577]\n",
      "epoch:  5530 training:  [0.004819036461412907, -0.005839407444000244, 0.01180979609489441]\n",
      "epoch:  5540 training:  [0.006873925216495991, 0.011250555515289307, 0.015317827463150024]\n",
      "epoch:  5550 training:  [0.0071716015227139, -0.0051339007914066315, 0.00647938996553421]\n",
      "epoch:  5560 training:  [0.005893852561712265, 0.008803647011518478, 0.05554334819316864]\n",
      "epoch:  5570 training:  [0.016029052436351776, 0.0194280706346035, 0.08222019672393799]\n",
      "epoch:  5580 training:  [0.012844890356063843, 0.004819206893444061, 0.017067084088921547]\n",
      "epoch:  5590 training:  [0.009352274239063263, 0.016955099999904633, 0.03917904943227768]\n",
      "epoch:  5600 training:  [0.006400724872946739, 0.003554210066795349, 0.03383095562458038]\n",
      "epoch:  5610 training:  [0.004314877092838287, -0.011532008647918701, 0.005156300961971283]\n",
      "epoch:  5620 training:  [0.01016034372150898, 0.012816019356250763, 0.046057432889938354]\n",
      "epoch:  5630 training:  [0.011060429736971855, 0.007628731429576874, 0.029068049043416977]\n",
      "epoch:  5640 training:  [0.007025925442576408, -0.006481640040874481, 0.006424538791179657]\n",
      "epoch:  5650 training:  [0.011320555582642555, 0.005275242030620575, 0.028423504903912544]\n",
      "epoch:  5660 training:  [0.009274106472730637, -0.011444393545389175, 0.011065445840358734]\n",
      "epoch:  5670 training:  [0.014285910874605179, 0.01238613948225975, 0.04793458431959152]\n",
      "epoch:  5680 training:  [0.009438657201826572, -0.005137547850608826, 0.023427780717611313]\n",
      "epoch:  5690 training:  [0.007207426708191633, 0.004583410918712616, 0.014326795935630798]\n",
      "epoch:  5700 training:  [0.005797717720270157, 0.015735261142253876, 0.024578750133514404]\n",
      "epoch:  5710 training:  [0.018182765692472458, 0.028792202472686768, 0.04900922626256943]\n",
      "epoch:  5720 training:  [-0.0036311913281679153, 0.009211141616106033, 0.05596918612718582]\n",
      "epoch:  5730 training:  [0.005317983217537403, 0.01035945862531662, 0.0311671644449234]\n",
      "epoch:  5740 training:  [0.05420669913291931, 0.2539241909980774, 0.09928791224956512]\n",
      "epoch:  5750 training:  [0.04451543837785721, 0.00944933295249939, 0.033414408564567566]\n",
      "epoch:  5760 training:  [0.01685154251754284, -0.003906160593032837, 0.019764140248298645]\n",
      "epoch:  5770 training:  [0.014803276397287846, -0.018712349236011505, 0.03644338250160217]\n",
      "epoch:  5780 training:  [0.019859671592712402, 0.015583742409944534, 0.07016147673130035]\n",
      "epoch:  5790 training:  [0.01703200303018093, 0.01518343947827816, 0.05646552890539169]\n",
      "epoch:  5800 training:  [0.008382733911275864, -0.00036098435521125793, 0.015070286579430103]\n",
      "epoch:  5810 training:  [0.014735320582985878, 0.0032389312982559204, 0.04281187430024147]\n",
      "epoch:  5820 training:  [0.015358055010437965, 0.012322641909122467, 0.07099556177854538]\n",
      "epoch:  5830 training:  [0.011367745697498322, -0.01055905595421791, 0.035541050136089325]\n",
      "epoch:  5840 training:  [0.020115887746214867, 0.02377886325120926, 0.11319722980260849]\n",
      "epoch:  5850 training:  [0.01943429745733738, 0.024874422699213028, 0.06354948878288269]\n",
      "epoch:  5860 training:  [0.014018267393112183, -0.00903543084859848, 0.03340169042348862]\n",
      "epoch:  5870 training:  [0.008784692734479904, 0.010688401758670807, 0.06537047028541565]\n",
      "epoch:  5880 training:  [0.004112991504371166, -0.0024983808398246765, 0.020986050367355347]\n",
      "epoch:  5890 training:  [0.003567265346646309, 0.0035586729645729065, 0.006981749087572098]\n",
      "epoch:  5900 training:  [0.010696183890104294, 0.003485221415758133, 0.005849406123161316]\n",
      "epoch:  5910 training:  [0.016902798786759377, 0.06035548448562622, 0.09743386507034302]\n",
      "epoch:  5920 training:  [0.007515232544392347, -0.02458229660987854, 0.018809985369443893]\n",
      "epoch:  5930 training:  [0.009166888892650604, 0.02824018895626068, 0.06353264302015305]\n",
      "epoch:  5940 training:  [0.005777282640337944, -0.017098069190979004, 0.015428449958562851]\n",
      "epoch:  5950 training:  [0.01442904956638813, 0.006646282970905304, 0.06663928925991058]\n",
      "epoch:  5960 training:  [0.0019240155816078186, -0.010687660425901413, 0.0031617656350135803]\n",
      "epoch:  5970 training:  [0.004528619349002838, -0.015000931918621063, 0.010881941765546799]\n",
      "epoch:  5980 training:  [0.0016327649354934692, 0.001575525850057602, 0.02771727181971073]\n",
      "epoch:  5990 training:  [0.010058700107038021, -0.002283584326505661, -0.0114106684923172]\n",
      "epoch:  6000 training:  [0.01345760002732277, -0.006069079041481018, 0.029391774907708168]\n",
      "epoch:  6010 training:  [0.013018246740102768, 0.01594528555870056, 0.014765296131372452]\n",
      "epoch:  6020 training:  [0.00400196760892868, 0.001991547644138336, 0.018957506865262985]\n",
      "epoch:  6030 training:  [0.015344025567173958, -0.00953337550163269, 0.011194974184036255]\n",
      "epoch:  6040 training:  [0.011245645582675934, 0.014261029660701752, 0.014988239854574203]\n",
      "epoch:  6050 training:  [0.00954386219382286, 0.018338553607463837, 0.07699693739414215]\n",
      "epoch:  6060 training:  [0.00965210236608982, -0.000992514193058014, 0.024598943069577217]\n",
      "epoch:  6070 training:  [0.018925098702311516, 0.01935620605945587, 0.028511136770248413]\n",
      "epoch:  6080 training:  [0.009321357123553753, 0.029960088431835175, -0.002860821783542633]\n",
      "epoch:  6090 training:  [0.0023307614028453827, 0.006909653544425964, 0.017417173832654953]\n",
      "epoch:  6100 training:  [0.009587546810507774, 0.013486895710229874, 0.01730995997786522]\n",
      "epoch:  6110 training:  [0.002622125204652548, 0.0011433549225330353, -0.009914878755807877]\n",
      "epoch:  6120 training:  [0.006923297420144081, 0.00046466290950775146, 0.030207248404622078]\n",
      "epoch:  6130 training:  [0.008789966814219952, 0.005433917045593262, 0.04179886355996132]\n",
      "epoch:  6140 training:  [0.011511933989822865, 0.007781838998198509, 0.01533515751361847]\n",
      "epoch:  6150 training:  [0.009825067594647408, 0.002638019621372223, 0.022502699866890907]\n",
      "epoch:  6160 training:  [0.0049740104004740715, -0.006059832870960236, 0.013347268104553223]\n",
      "epoch:  6170 training:  [0.01019169669598341, -0.00437348335981369, 0.034509994089603424]\n",
      "epoch:  6180 training:  [0.008000260218977928, 0.01438366249203682, 0.013937640935182571]\n",
      "epoch:  6190 training:  [0.002855554223060608, -0.00629059225320816, 0.04087693989276886]\n",
      "epoch:  6200 training:  [0.00911210011690855, 0.004555676132440567, 0.027613375335931778]\n",
      "epoch:  6210 training:  [0.0040390812791883945, -0.02415652573108673, 0.018974512815475464]\n",
      "epoch:  6220 training:  [0.004869275260716677, -0.007123865187168121, 0.022859172895550728]\n",
      "epoch:  6230 training:  [0.0052314642816782, -0.011209174990653992, 0.0040140338242053986]\n",
      "epoch:  6240 training:  [0.004324786365032196, -0.013438485562801361, -0.010710857808589935]\n",
      "epoch:  6250 training:  [0.0051953233778476715, 0.0017274580895900726, 0.015363719314336777]\n",
      "epoch:  6260 training:  [0.009546646848320961, -0.03491251915693283, 0.007450997829437256]\n",
      "epoch:  6270 training:  [0.006644894368946552, 0.007531648501753807, 0.02213468961417675]\n",
      "epoch:  6280 training:  [0.013385429047048092, 0.0070128366351127625, 0.06401754915714264]\n",
      "epoch:  6290 training:  [0.010121284984052181, 0.007691230624914169, 0.016306474804878235]\n",
      "epoch:  6300 training:  [0.004150133579969406, 6.444752216339111e-05, 0.011651096865534782]\n",
      "epoch:  6310 training:  [0.006029143929481506, -0.0008816458284854889, 0.018485698848962784]\n",
      "epoch:  6320 training:  [0.006989357061684132, 0.0033578798174858093, 0.027204051613807678]\n",
      "epoch:  6330 training:  [0.011907964944839478, 0.009274769574403763, 0.018697338178753853]\n",
      "epoch:  6340 training:  [0.014562336727976799, 0.011191055178642273, 0.05006956309080124]\n",
      "epoch:  6350 training:  [0.008008034899830818, 0.008325845003128052, 0.009600058197975159]\n",
      "epoch:  6360 training:  [0.007539687678217888, -0.0031928755342960358, 0.008326813578605652]\n",
      "epoch:  6370 training:  [0.007673792541027069, 0.0068007707595825195, 0.027359500527381897]\n",
      "epoch:  6380 training:  [0.01129963994026184, -0.0021289661526679993, 0.027305617928504944]\n",
      "epoch:  6390 training:  [0.005444305017590523, 0.004160724580287933, 0.05840941518545151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  6400 training:  [0.005945183336734772, -0.00186823308467865, 0.008921835571527481]\n",
      "epoch:  6410 training:  [0.007030405104160309, -0.004828270524740219, 0.02091626077890396]\n",
      "epoch:  6420 training:  [0.0022013895213603973, -0.0020813755691051483, 0.028050530701875687]\n",
      "epoch:  6430 training:  [0.009535808116197586, 0.0009705610573291779, 0.005396362394094467]\n",
      "epoch:  6440 training:  [0.004389575682580471, -0.001167062669992447, 0.006407037377357483]\n",
      "epoch:  6450 training:  [-7.750256918370724e-05, 0.011410720646381378, 0.017075348645448685]\n",
      "epoch:  6460 training:  [0.004770344123244286, -0.00036294758319854736, 0.014822715893387794]\n",
      "epoch:  6470 training:  [0.00862930715084076, -0.008254598826169968, 0.011670105159282684]\n",
      "epoch:  6480 training:  [0.006848193239420652, 0.0012676939368247986, 0.009342636913061142]\n",
      "epoch:  6490 training:  [0.0063864691182971, -0.0077655985951423645, 0.012020917609333992]\n",
      "epoch:  6500 training:  [0.0057397219352424145, -0.008248087018728256, 0.013081485405564308]\n",
      "epoch:  6510 training:  [0.005596696399152279, -0.00210682675242424, 0.02492573857307434]\n",
      "epoch:  6520 training:  [0.006722001358866692, -0.007437378168106079, 0.028529241681098938]\n",
      "epoch:  6530 training:  [0.008036412298679352, 0.008272718638181686, 0.020662546157836914]\n",
      "epoch:  6540 training:  [0.005040810443460941, 0.003592878580093384, 0.024006977677345276]\n",
      "epoch:  6550 training:  [0.010551435872912407, 0.0022738687694072723, 0.06808430701494217]\n",
      "epoch:  6560 training:  [0.013602470979094505, 0.010732822120189667, 0.05211406946182251]\n",
      "epoch:  6570 training:  [0.0008511021733283997, 0.0015932917594909668, 0.0200376957654953]\n",
      "epoch:  6580 training:  [0.004518645349889994, 0.005270775407552719, 0.03042547032237053]\n",
      "epoch:  6590 training:  [0.002349982038140297, -0.001661401242017746, 0.03207314386963844]\n",
      "epoch:  6600 training:  [0.0037849978543817997, -0.005339041352272034, 0.02678379788994789]\n",
      "epoch:  6610 training:  [0.005975771229714155, 0.009403964504599571, 0.046323262155056]\n",
      "epoch:  6620 training:  [0.012941602617502213, 0.019739702343940735, 0.04842056706547737]\n",
      "epoch:  6630 training:  [0.004578601568937302, -0.0027800649404525757, 0.009164266288280487]\n",
      "epoch:  6640 training:  [0.003664704505354166, 0.008776990696787834, 0.036116380244493484]\n",
      "epoch:  6650 training:  [0.005444735288619995, -0.00461190938949585, 0.029026959091424942]\n",
      "epoch:  6660 training:  [0.0025761686265468597, 0.004242006689310074, 0.0036610476672649384]\n",
      "epoch:  6670 training:  [0.017074253410100937, 0.030552273616194725, 0.036783065646886826]\n",
      "epoch:  6680 training:  [0.015690436586737633, 0.014197872951626778, 0.018682383000850677]\n",
      "epoch:  6690 training:  [0.007400570437312126, 0.012093719094991684, 0.04137066751718521]\n",
      "epoch:  6700 training:  [0.004715036600828171, 0.014940692111849785, 0.0082680843770504]\n",
      "epoch:  6710 training:  [0.007941805757582188, 0.013639748096466064, 0.04269442334771156]\n",
      "epoch:  6720 training:  [0.005116927437484264, 0.005278091877698898, 0.01112697646021843]\n",
      "epoch:  6730 training:  [0.010665787383913994, -0.001019548624753952, 0.02488200180232525]\n",
      "epoch:  6740 training:  [0.010343898087739944, 0.013095051050186157, 0.033899880945682526]\n",
      "epoch:  6750 training:  [0.0038208086043596268, -0.0024038106203079224, 0.010965457186102867]\n",
      "epoch:  6760 training:  [0.015209179371595383, 0.0031789839267730713, 0.029937177896499634]\n",
      "epoch:  6770 training:  [0.002357036806643009, 0.00863654911518097, 0.0141057800501585]\n",
      "epoch:  6780 training:  [-0.000634804368019104, -0.01764392852783203, 0.0075333938002586365]\n",
      "epoch:  6790 training:  [0.007342159748077393, 0.009465515613555908, 0.0167430080473423]\n",
      "epoch:  6800 training:  [0.012201093137264252, 0.058537743985652924, 0.031172417104244232]\n",
      "epoch:  6810 training:  [0.010696234181523323, 0.0006164126098155975, 0.12490157783031464]\n",
      "epoch:  6820 training:  [0.006098231300711632, 0.03445331007242203, 0.03900550305843353]\n",
      "epoch:  6830 training:  [0.008414098992943764, -0.005187362432479858, 0.07143257558345795]\n",
      "epoch:  6840 training:  [0.010803332552313805, 0.012560062110424042, 0.06740434467792511]\n",
      "epoch:  6850 training:  [0.006422581151127815, -0.0005106180906295776, 0.036219410598278046]\n",
      "epoch:  6860 training:  [0.009779534302651882, 0.009826496243476868, 0.06184889376163483]\n",
      "epoch:  6870 training:  [0.012265583500266075, 0.003594290465116501, 0.040082983672618866]\n",
      "epoch:  6880 training:  [0.00333993136882782, 0.005076929926872253, 0.0567464679479599]\n",
      "epoch:  6890 training:  [0.003697841428220272, -0.00724874809384346, 0.012855296954512596]\n",
      "epoch:  6900 training:  [0.0021435655653476715, -0.0031539201736450195, -0.0035688169300556183]\n",
      "epoch:  6910 training:  [0.039917610585689545, 0.055827248841524124, 0.08663385361433029]\n",
      "epoch:  6920 training:  [0.0015999895986169577, -0.003108881413936615, 0.0026933103799819946]\n",
      "epoch:  6930 training:  [0.02365371771156788, 0.020165860652923584, 0.006056502461433411]\n",
      "epoch:  6940 training:  [0.001954060047864914, 0.004567041993141174, 0.06513352692127228]\n",
      "epoch:  6950 training:  [-0.0037409551441669464, -0.003428518772125244, 0.01805693656206131]\n",
      "epoch:  6960 training:  [0.011475883424282074, 0.03840651735663414, 0.08957615494728088]\n",
      "epoch:  6970 training:  [0.05147862806916237, 0.05179039388895035, 0.068416528403759]\n",
      "epoch:  6980 training:  [0.041070885956287384, 0.029976222664117813, 0.02856665849685669]\n",
      "epoch:  6990 training:  [0.00640646368265152, 0.0021688230335712433, 0.012511186301708221]\n",
      "epoch:  7000 training:  [0.010758791118860245, 0.003462977707386017, 0.020703725516796112]\n",
      "epoch:  7010 training:  [0.015969865024089813, 0.01096968725323677, 0.06679148972034454]\n",
      "epoch:  7020 training:  [0.015239531174302101, 0.017697647213935852, 0.01299203559756279]\n",
      "epoch:  7030 training:  [0.047229789197444916, 0.01153244823217392, 0.017171746119856834]\n",
      "epoch:  7040 training:  [0.006305687129497528, 0.005796104669570923, 0.044132016599178314]\n",
      "epoch:  7050 training:  [0.004958918318152428, -0.002103704959154129, 0.01972164586186409]\n",
      "epoch:  7060 training:  [0.01684945821762085, -0.008465096354484558, 0.03319152072072029]\n",
      "epoch:  7070 training:  [0.006385044660419226, 0.008737191557884216, 0.021148880943655968]\n",
      "epoch:  7080 training:  [0.006515780463814735, -0.0024506375193595886, 0.013741500675678253]\n",
      "epoch:  7090 training:  [0.00897829420864582, 0.008660657331347466, 0.002261117100715637]\n",
      "epoch:  7100 training:  [0.009816891513764858, -0.009689554572105408, 0.007251877337694168]\n",
      "epoch:  7110 training:  [0.010567927733063698, -0.0019078664481639862, 0.020434729754924774]\n",
      "epoch:  7120 training:  [0.008796279318630695, -0.001849580556154251, 0.019506162032485008]\n",
      "epoch:  7130 training:  [0.0025632791221141815, 0.005845785140991211, 0.05251891165971756]\n",
      "epoch:  7140 training:  [0.0029569007456302643, 0.004214618355035782, 0.014962640590965748]\n",
      "epoch:  7150 training:  [0.0010959319770336151, -0.00919308140873909, 0.01461805123835802]\n",
      "epoch:  7160 training:  [0.00592748261988163, -0.003571823239326477, 0.004676831886172295]\n",
      "epoch:  7170 training:  [0.005340247415006161, 0.00545184500515461, 0.012800626456737518]\n",
      "epoch:  7180 training:  [0.008408309891819954, 0.0018926002085208893, 0.010030491277575493]\n",
      "epoch:  7190 training:  [0.0023923590779304504, 0.03410715609788895, 0.018465187400579453]\n",
      "epoch:  7200 training:  [0.008530562743544579, -0.0006933696568012238, 0.003891322761774063]\n",
      "epoch:  7210 training:  [0.0069857011549174786, 0.008606143295764923, 0.015486156567931175]\n",
      "epoch:  7220 training:  [0.004654197953641415, -0.004257716238498688, 0.022457394748926163]\n",
      "epoch:  7230 training:  [0.00877419114112854, 0.0011523030698299408, 0.010923928581178188]\n",
      "epoch:  7240 training:  [0.0123189277946949, 0.009345917962491512, 0.008397568017244339]\n",
      "epoch:  7250 training:  [0.007082291878759861, 0.02606429159641266, 0.04412129521369934]\n",
      "epoch:  7260 training:  [0.01005188375711441, 0.02900322526693344, 0.046938326209783554]\n",
      "epoch:  7270 training:  [0.005759689025580883, 0.0036878176033496857, 0.019680118188261986]\n",
      "epoch:  7280 training:  [0.006537307985126972, 0.01898542046546936, 0.02865859493613243]\n",
      "epoch:  7290 training:  [0.0075668953359127045, 0.007064372301101685, 0.02723994478583336]\n",
      "epoch:  7300 training:  [0.008525114506483078, 0.018472857773303986, 0.061256684362888336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  7310 training:  [0.005890618544071913, 0.010082900524139404, 0.020968126133084297]\n",
      "epoch:  7320 training:  [0.0063730692490935326, -0.001373153179883957, 0.013765616342425346]\n",
      "epoch:  7330 training:  [0.007942816242575645, 0.005243424326181412, 0.02190428227186203]\n",
      "epoch:  7340 training:  [0.007212920114398003, -0.012301869690418243, 0.009088100865483284]\n",
      "epoch:  7350 training:  [0.007604809477925301, 0.007089890539646149, 0.02595124952495098]\n",
      "epoch:  7360 training:  [0.003246057778596878, -0.0015097446739673615, 0.04823035001754761]\n",
      "epoch:  7370 training:  [0.0023728152737021446, -4.4930726289749146e-05, 0.00698448158800602]\n",
      "epoch:  7380 training:  [0.010975482873618603, 0.02673671394586563, 0.050693221390247345]\n",
      "epoch:  7390 training:  [0.008527224883437157, 0.004163149744272232, 0.01139771193265915]\n",
      "epoch:  7400 training:  [0.030942775309085846, 0.10154944658279419, 0.05478862673044205]\n",
      "epoch:  7410 training:  [0.01072391215711832, 0.011706776916980743, 0.028104711323976517]\n",
      "epoch:  7420 training:  [0.0112973153591156, -0.014366310089826584, 0.0108867846429348]\n",
      "epoch:  7430 training:  [0.007080175913870335, -0.010515041649341583, 0.021017998456954956]\n",
      "epoch:  7440 training:  [0.006657411344349384, 0.00190000981092453, 0.022772423923015594]\n",
      "epoch:  7450 training:  [-5.330238491296768e-05, -0.0029678568243980408, 0.02173861302435398]\n",
      "epoch:  7460 training:  [0.0027228184044361115, 0.00402122363448143, 0.022819925099611282]\n",
      "epoch:  7470 training:  [0.0026748115196824074, 0.006142573431134224, 0.03389611840248108]\n",
      "epoch:  7480 training:  [0.006159230135381222, -0.006470639258623123, 0.01106942631304264]\n",
      "epoch:  7490 training:  [0.011507905088365078, 0.0029775090515613556, 0.018415579572319984]\n",
      "epoch:  7500 training:  [-0.0005414579063653946, -0.011910241097211838, 0.0068874964490532875]\n",
      "epoch:  7510 training:  [0.009038003161549568, -0.0028930678963661194, 0.0214138925075531]\n",
      "epoch:  7520 training:  [0.0009775515645742416, 0.0014402307569980621, -0.00012909993529319763]\n",
      "epoch:  7530 training:  [2.154335379600525e-05, -0.0034927837550640106, 0.014461472630500793]\n",
      "epoch:  7540 training:  [0.010200407356023788, 0.02125600352883339, 0.03783213719725609]\n",
      "epoch:  7550 training:  [0.031091831624507904, 0.022783026099205017, 0.044670794159173965]\n",
      "epoch:  7560 training:  [0.002987033687531948, 0.008546672761440277, 0.031520478427410126]\n",
      "epoch:  7570 training:  [0.005501157604157925, -0.006287954747676849, 0.020670121535658836]\n",
      "epoch:  7580 training:  [0.003953352104872465, 0.0021843835711479187, 0.011756675317883492]\n",
      "epoch:  7590 training:  [0.0033504576422274113, 0.005847908556461334, 0.02289218083024025]\n",
      "epoch:  7600 training:  [0.0007781609892845154, 0.006451785564422607, 0.014178325422108173]\n",
      "epoch:  7610 training:  [0.005142121575772762, 0.020128652453422546, 0.023299237713217735]\n",
      "epoch:  7620 training:  [0.007043273188173771, 0.00946972519159317, 0.022014740854501724]\n",
      "epoch:  7630 training:  [0.00444450881332159, 0.004392942413687706, 0.012069599702954292]\n",
      "epoch:  7640 training:  [0.0108055854216218, 0.003913775086402893, 0.01914060115814209]\n",
      "epoch:  7650 training:  [0.0004261098802089691, -0.019152462482452393, 0.0004668012261390686]\n",
      "epoch:  7660 training:  [0.0028857700526714325, -0.004434734582901001, 0.03835200518369675]\n",
      "epoch:  7670 training:  [0.0035168956965208054, 0.007894501090049744, 0.03400709107518196]\n",
      "epoch:  7680 training:  [0.006857856176793575, 0.004947450011968613, 0.015426583588123322]\n",
      "epoch:  7690 training:  [0.0011064084246754646, 0.002095825970172882, 0.02553970366716385]\n",
      "epoch:  7700 training:  [0.0021351808682084084, 0.00420791283249855, 0.01313256099820137]\n",
      "epoch:  7710 training:  [0.00949606578797102, 0.005894944071769714, 0.026783287525177002]\n",
      "epoch:  7720 training:  [0.010187754407525063, 0.007005985826253891, 0.013786349445581436]\n",
      "epoch:  7730 training:  [0.006707894615828991, -0.002184838056564331, 0.01309625431895256]\n",
      "epoch:  7740 training:  [0.00189928337931633, -0.00493178516626358, 0.008587703108787537]\n",
      "epoch:  7750 training:  [0.00470932200551033, -0.0008955392986536026, 0.01936270482838154]\n",
      "epoch:  7760 training:  [0.00010018236935138702, 0.0017317607998847961, 0.012653499841690063]\n",
      "epoch:  7770 training:  [0.005286174360662699, -0.005667470395565033, 0.017446471378207207]\n",
      "epoch:  7780 training:  [0.008962173014879227, 0.0013852901756763458, 0.010794652625918388]\n",
      "epoch:  7790 training:  [0.01726352423429489, 0.02083333395421505, 0.016029907390475273]\n",
      "epoch:  7800 training:  [0.020587803795933723, 0.05172594636678696, 0.042879462242126465]\n",
      "epoch:  7810 training:  [0.011330178007483482, 0.033350080251693726, 0.02534581534564495]\n",
      "epoch:  7820 training:  [-0.0031371768563985825, 0.015201281756162643, 0.021297380328178406]\n",
      "epoch:  7830 training:  [0.0067526549100875854, -0.00772114098072052, 0.011094683781266212]\n",
      "epoch:  7840 training:  [0.010077038779854774, -0.0032749585807323456, 0.014967553317546844]\n",
      "epoch:  7850 training:  [0.004690274596214294, 0.005456604063510895, 0.02017747424542904]\n",
      "epoch:  7860 training:  [0.006826426833868027, -0.0013755448162555695, 0.005014402791857719]\n",
      "epoch:  7870 training:  [-0.003826584666967392, -0.00026179105043411255, 0.01029026135802269]\n",
      "epoch:  7880 training:  [0.00540253845974803, 0.01574425958096981, 0.01254742406308651]\n",
      "epoch:  7890 training:  [9.861774742603302e-06, 0.0040474943816661835, 0.029184365645051003]\n",
      "epoch:  7900 training:  [0.0019662969280034304, -0.00210711732506752, 0.030398862436413765]\n",
      "epoch:  7910 training:  [0.002499271184206009, 0.0020164120942354202, 0.009385835379362106]\n",
      "epoch:  7920 training:  [0.010657589882612228, 0.012126952409744263, 0.0664057582616806]\n",
      "epoch:  7930 training:  [0.001406518742442131, -0.000840313732624054, 0.017340045422315598]\n",
      "epoch:  7940 training:  [0.006132002919912338, 0.0017011500895023346, 0.014540945179760456]\n",
      "epoch:  7950 training:  [0.003044523298740387, -0.0021980442106723785, -0.001344338059425354]\n",
      "epoch:  7960 training:  [0.0007069539278745651, 0.006859965622425079, 0.016552772372961044]\n",
      "epoch:  7970 training:  [0.000609230250120163, 0.00579080730676651, 0.010240580886602402]\n",
      "epoch:  7980 training:  [0.006716850213706493, 0.009749732911586761, 0.011540759354829788]\n",
      "epoch:  7990 training:  [0.005059370305389166, -0.006458107382059097, -0.003489196300506592]\n",
      "epoch:  8000 training:  [-0.0019858144223690033, 0.0051962509751319885, 0.0031998157501220703]\n",
      "epoch:  8010 training:  [0.0035796756856143475, 0.0067637041211128235, 0.022857964038848877]\n",
      "epoch:  8020 training:  [0.005447493866086006, 8.612871170043945e-05, 0.009002098813652992]\n",
      "epoch:  8030 training:  [0.004070580936968327, -0.006107110530138016, -0.007403142750263214]\n",
      "epoch:  8040 training:  [0.0005001667886972427, 0.002989422529935837, 0.003921758383512497]\n",
      "epoch:  8050 training:  [0.006261888891458511, -0.002309150993824005, 0.02073989063501358]\n",
      "epoch:  8060 training:  [0.006461761891841888, -0.00043784454464912415, 0.015151187777519226]\n",
      "epoch:  8070 training:  [0.000950345303863287, -0.006495889276266098, 0.016017865389585495]\n",
      "epoch:  8080 training:  [-0.0023195184767246246, -0.0020002350211143494, 0.015289634466171265]\n",
      "epoch:  8090 training:  [0.008708050474524498, -0.015103548765182495, 0.012498765252530575]\n",
      "epoch:  8100 training:  [0.005136952735483646, 0.0006941929459571838, 0.012232731096446514]\n",
      "epoch:  8110 training:  [0.004901855252683163, 0.004645310342311859, 0.02028699778020382]\n",
      "epoch:  8120 training:  [0.009294872172176838, 0.008166728541254997, 0.02970554679632187]\n",
      "epoch:  8130 training:  [0.006784970872104168, -0.00016325339674949646, 0.027476703748106956]\n",
      "epoch:  8140 training:  [-0.00046481937170028687, -0.010701097548007965, 0.011546246707439423]\n",
      "epoch:  8150 training:  [0.007485826499760151, 0.006202243268489838, 0.019321206957101822]\n",
      "epoch:  8160 training:  [0.005314011126756668, 0.006317466497421265, 0.008195437490940094]\n",
      "epoch:  8170 training:  [0.008451269939541817, 0.0033979900181293488, 0.018753189593553543]\n",
      "epoch:  8180 training:  [0.0030194539576768875, -0.0008914992213249207, 0.009750477969646454]\n",
      "epoch:  8190 training:  [0.011958998627960682, 0.0065782926976680756, 0.0119937090203166]\n",
      "epoch:  8200 training:  [0.005307940766215324, 0.004051895812153816, 0.00321204774081707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  8210 training:  [-5.7213008403778076e-05, 0.0095624178647995, 0.014674107544124126]\n",
      "epoch:  8220 training:  [0.0036231400445103645, 0.0068156179040670395, 0.015498293563723564]\n",
      "epoch:  8230 training:  [0.0014820594806224108, -0.002166435122489929, 0.014201968908309937]\n",
      "epoch:  8240 training:  [0.007108610589057207, -0.0008702129125595093, -0.002384886145591736]\n",
      "epoch:  8250 training:  [0.00949124339967966, -0.0008346512913703918, 0.015209324657917023]\n",
      "epoch:  8260 training:  [0.0035068076103925705, -0.004930291324853897, 0.016914110630750656]\n",
      "epoch:  8270 training:  [0.005655445158481598, 0.00029375962913036346, 0.01863902062177658]\n",
      "epoch:  8280 training:  [0.0031988490372896194, 0.0008484832942485809, 0.01472623273730278]\n",
      "epoch:  8290 training:  [0.010725751519203186, 0.013959303498268127, 0.01077151857316494]\n",
      "epoch:  8300 training:  [0.006763029843568802, 0.011217277497053146, 0.023688968271017075]\n",
      "epoch:  8310 training:  [-0.0012415405362844467, 0.0070552825927734375, 0.03839411959052086]\n",
      "epoch:  8320 training:  [0.009436756372451782, -0.005847625434398651, 0.04501168429851532]\n",
      "epoch:  8330 training:  [0.009898052550852299, 0.005821306258440018, 0.009937286376953125]\n",
      "epoch:  8340 training:  [0.007833481766283512, 0.003814995288848877, 0.018003374338150024]\n",
      "epoch:  8350 training:  [0.006096133962273598, 0.008981291204690933, -0.0057675763964653015]\n",
      "epoch:  8360 training:  [0.008154589682817459, 0.003496873192489147, 0.020369425415992737]\n",
      "epoch:  8370 training:  [4.3577514588832855e-05, -0.009187575429677963, 0.01592983677983284]\n",
      "epoch:  8380 training:  [0.006031325086951256, 0.0019220206886529922, 0.015611966140568256]\n",
      "epoch:  8390 training:  [0.004425090737640858, 0.009070955216884613, 0.02799338847398758]\n",
      "epoch:  8400 training:  [0.0035276375710964203, 0.003943083807826042, 0.016740567982196808]\n",
      "epoch:  8410 training:  [0.0030427342280745506, 0.003096986562013626, 0.02445966936647892]\n",
      "epoch:  8420 training:  [0.002708105370402336, 0.007761813700199127, 0.026851465925574303]\n",
      "epoch:  8430 training:  [0.0036538084968924522, -0.024548858404159546, 0.015703696757555008]\n",
      "epoch:  8440 training:  [-0.006783287972211838, -0.002637002617120743, 0.009062707424163818]\n",
      "epoch:  8450 training:  [0.006081011146306992, 0.006885889917612076, 0.049417734146118164]\n",
      "epoch:  8460 training:  [0.004128485918045044, 0.004631757736206055, 0.025085298344492912]\n",
      "epoch:  8470 training:  [0.005912078078836203, 0.004623785614967346, 0.035391587764024734]\n",
      "epoch:  8480 training:  [0.00522087886929512, -0.00038238242268562317, 0.01050621923059225]\n",
      "epoch:  8490 training:  [0.006731971632689238, 0.0029092542827129364, 0.020787373185157776]\n",
      "epoch:  8500 training:  [0.003222879022359848, 0.015604009851813316, 0.03199189156293869]\n",
      "epoch:  8510 training:  [0.0009123953059315681, 0.0006275400519371033, 0.012929391115903854]\n",
      "epoch:  8520 training:  [0.008971676230430603, -0.008984465152025223, 0.013519257307052612]\n",
      "epoch:  8530 training:  [0.036898285150527954, 0.029310235753655434, 0.015919938683509827]\n",
      "epoch:  8540 training:  [0.009195538237690926, 0.026015883311629295, 0.030634770169854164]\n",
      "epoch:  8550 training:  [0.008833012543618679, -0.0006032511591911316, 0.010787161998450756]\n",
      "epoch:  8560 training:  [-4.9876049160957336e-05, 0.010397415608167648, 0.01573696732521057]\n",
      "epoch:  8570 training:  [0.0021288488060235977, 0.003085292875766754, 0.016530346125364304]\n",
      "epoch:  8580 training:  [0.00895318016409874, 0.006415322422981262, 0.0028853006660938263]\n",
      "epoch:  8590 training:  [0.004597825929522514, 0.006221810355782509, 0.012716487050056458]\n",
      "epoch:  8600 training:  [0.002454631496220827, -0.003681175410747528, 0.023107130080461502]\n",
      "epoch:  8610 training:  [0.0033294400200247765, 0.008711772039532661, 0.030599094927310944]\n",
      "epoch:  8620 training:  [0.0028408030048012733, -0.00862157717347145, 0.01254899799823761]\n",
      "epoch:  8630 training:  [0.0008348822593688965, 0.010789450258016586, 0.018550904467701912]\n",
      "epoch:  8640 training:  [0.00448612729087472, 0.008158810436725616, 0.020379530265927315]\n",
      "epoch:  8650 training:  [0.006107046268880367, 0.01590092107653618, 0.020261196419596672]\n",
      "epoch:  8660 training:  [0.0013680605916306376, 0.008683055639266968, 0.03608134388923645]\n",
      "epoch:  8670 training:  [0.0013807816430926323, 0.004316333681344986, 0.02636328712105751]\n",
      "epoch:  8680 training:  [0.007491393946111202, 0.0014959052205085754, 0.02877817116677761]\n",
      "epoch:  8690 training:  [0.0072641074657440186, 0.009969551116228104, 0.032165661454200745]\n",
      "epoch:  8700 training:  [0.014680298045277596, 0.01784869283437729, 0.029360808432102203]\n",
      "epoch:  8710 training:  [0.004743685945868492, -0.0012703537940979004, -0.0051779672503471375]\n",
      "epoch:  8720 training:  [-0.003693271428346634, -0.005510922521352768, 0.01798318699002266]\n",
      "epoch:  8730 training:  [0.0033977441489696503, -0.0038442984223365784, 0.01005169004201889]\n",
      "epoch:  8740 training:  [0.0024001020938158035, -0.00641101598739624, 0.016564246267080307]\n",
      "epoch:  8750 training:  [0.022325415164232254, 0.022013770416378975, 0.01837182603776455]\n",
      "epoch:  8760 training:  [0.0077459122985601425, 0.009517515078186989, -0.0023612752556800842]\n",
      "epoch:  8770 training:  [-0.0028926339000463486, 0.00395733118057251, 0.017253350466489792]\n",
      "epoch:  8780 training:  [0.008587531745433807, -0.004336841404438019, 0.015426108613610268]\n",
      "epoch:  8790 training:  [0.004373989999294281, 0.0030720923095941544, 0.012960568070411682]\n",
      "epoch:  8800 training:  [0.0009662304073572159, -0.011072937399148941, 0.01966147869825363]\n",
      "epoch:  8810 training:  [0.004624539986252785, 0.0019762739539146423, 0.01499673631042242]\n",
      "epoch:  8820 training:  [0.005541145335882902, 0.012727979570627213, 0.015482109040021896]\n",
      "epoch:  8830 training:  [0.005861328914761543, 0.004089973866939545, 0.026078224182128906]\n",
      "epoch:  8840 training:  [0.00792405754327774, 0.005080856382846832, 0.012507466599345207]\n",
      "epoch:  8850 training:  [0.003152330406010151, 0.008203290402889252, 0.06308319419622421]\n",
      "epoch:  8860 training:  [0.005360493436455727, 0.02322070486843586, 0.05731713026762009]\n",
      "epoch:  8870 training:  [0.0019090804271399975, -0.01767328754067421, 0.013625435531139374]\n",
      "epoch:  8880 training:  [0.006657741032540798, 0.0005411505699157715, 0.013149291276931763]\n",
      "epoch:  8890 training:  [0.009488090872764587, 0.0044762082397937775, 0.03560406714677811]\n",
      "epoch:  8900 training:  [0.013697640039026737, -0.013967998325824738, 0.03581252321600914]\n",
      "epoch:  8910 training:  [0.00816731620579958, 0.004175426438450813, 0.0104700131341815]\n",
      "epoch:  8920 training:  [0.003535045310854912, -0.00782422348856926, 0.008311860263347626]\n",
      "epoch:  8930 training:  [0.0059683118015527725, 0.0010893158614635468, 0.00818578526377678]\n",
      "epoch:  8940 training:  [0.0005889749154448509, -0.0018817707896232605, 0.009632164612412453]\n",
      "epoch:  8950 training:  [0.007185604423284531, 0.010060127824544907, 0.02043173462152481]\n",
      "epoch:  8960 training:  [0.004805245436728001, -0.003120746463537216, 0.017100289463996887]\n",
      "epoch:  8970 training:  [0.007563388906419277, 0.004606613889336586, 0.027404190972447395]\n",
      "epoch:  8980 training:  [0.0016638562083244324, 0.005564957857131958, 0.020214052870869637]\n",
      "epoch:  8990 training:  [-0.005655795335769653, 0.002861902117729187, 0.01641817018389702]\n",
      "epoch:  9000 training:  [0.00042852573096752167, -0.0006231069564819336, 0.021890144795179367]\n",
      "epoch:  9010 training:  [0.011755895800888538, -0.0020392723381519318, 0.023795664310455322]\n",
      "epoch:  9020 training:  [0.007628260180354118, 0.018377991393208504, 0.02709151804447174]\n",
      "epoch:  9030 training:  [0.004453905858099461, 0.003248445689678192, 0.010156082920730114]\n",
      "epoch:  9040 training:  [0.006571315694600344, 0.0029106512665748596, 0.020167430862784386]\n",
      "epoch:  9050 training:  [0.0033376701176166534, 0.004350986331701279, 0.0074035171419382095]\n",
      "epoch:  9060 training:  [0.005815853364765644, 0.0026184581220149994, 0.017104169353842735]\n",
      "epoch:  9070 training:  [0.0077350991778075695, -0.02497362345457077, 0.007664583623409271]\n",
      "epoch:  9080 training:  [0.0036715595051646233, -0.0006890781223773956, 0.0007345490157604218]\n",
      "epoch:  9090 training:  [0.0002297535538673401, -0.0052466318011283875, 0.025673694908618927]\n",
      "epoch:  9100 training:  [0.004887573421001434, -0.011359639465808868, 0.015182457864284515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  9110 training:  [0.004104383289813995, -0.003806479275226593, 0.02808769792318344]\n",
      "epoch:  9120 training:  [0.007639472372829914, 0.0014281868934631348, 0.06251034885644913]\n",
      "epoch:  9130 training:  [-0.0023503918200731277, 0.0012405291199684143, 0.030529240146279335]\n",
      "epoch:  9140 training:  [0.0009766267612576485, 0.0029128268361091614, 0.07644138485193253]\n",
      "epoch:  9150 training:  [-0.0013024769723415375, -0.004170779138803482, -0.019804365932941437]\n",
      "epoch:  9160 training:  [0.003377344459295273, 0.003838673233985901, 0.018161065876483917]\n",
      "epoch:  9170 training:  [0.0025435006245970726, 0.007150474935770035, 0.03852391242980957]\n",
      "epoch:  9180 training:  [0.005924563389271498, 0.012373041361570358, 0.01705927401781082]\n",
      "epoch:  9190 training:  [0.0012844651937484741, 0.00935000367462635, 0.03619740158319473]\n",
      "epoch:  9200 training:  [0.005126042291522026, -0.00740446150302887, 0.023850662633776665]\n",
      "epoch:  9210 training:  [0.0046427687630057335, 0.0010163169354200363, 0.013266976922750473]\n",
      "epoch:  9220 training:  [0.009525998495519161, -0.0068377964198589325, 0.03028908371925354]\n",
      "epoch:  9230 training:  [0.00783497467637062, 0.009638923220336437, 0.015099121257662773]\n",
      "epoch:  9240 training:  [0.00984693132340908, 0.008916173130273819, 0.02686401829123497]\n",
      "epoch:  9250 training:  [0.011361030861735344, 0.007047569379210472, 0.016563527286052704]\n",
      "epoch:  9260 training:  [0.0027997298166155815, 0.019858738407492638, 0.03686220943927765]\n",
      "epoch:  9270 training:  [0.007257250137627125, 0.0010023638606071472, 0.001957964152097702]\n",
      "epoch:  9280 training:  [0.008733567781746387, 0.0010381415486335754, 0.029244858771562576]\n",
      "epoch:  9290 training:  [0.004694652743637562, -0.0038672424852848053, 0.01932293176651001]\n",
      "epoch:  9300 training:  [0.006014473736286163, 0.007955756038427353, 0.03200174868106842]\n",
      "epoch:  9310 training:  [0.004273869097232819, -0.0010879579931497574, 0.01937767304480076]\n",
      "epoch:  9320 training:  [0.007107554003596306, 0.007466649636626244, 0.03363720327615738]\n",
      "epoch:  9330 training:  [0.007924385368824005, 0.013738598674535751, 0.054535575211048126]\n",
      "epoch:  9340 training:  [0.006757413502782583, 0.0040097348392009735, -0.001347653567790985]\n",
      "epoch:  9350 training:  [0.004134398885071278, 0.013754524290561676, 0.03047662228345871]\n",
      "epoch:  9360 training:  [-0.0012107780203223228, 0.000556187704205513, -0.00548810139298439]\n",
      "epoch:  9370 training:  [0.007165476214140654, 0.009120197966694832, 0.04902637004852295]\n",
      "epoch:  9380 training:  [-0.0011818143539130688, -0.0019939597696065903, 0.01065165176987648]\n",
      "epoch:  9390 training:  [0.00534219853579998, 0.0008749067783355713, -8.131936192512512e-05]\n",
      "epoch:  9400 training:  [0.010302368551492691, 0.010010670870542526, 0.020000915974378586]\n",
      "epoch:  9410 training:  [0.011514108628034592, -0.0071602314710617065, 0.029827211052179337]\n",
      "epoch:  9420 training:  [0.0068896012380719185, -0.0007775053381919861, 0.047953490167856216]\n",
      "epoch:  9430 training:  [-0.0016119927167892456, -0.00035633891820907593, 0.03433161973953247]\n",
      "epoch:  9440 training:  [0.006346933543682098, 0.00583253800868988, 0.02681136690080166]\n",
      "epoch:  9450 training:  [-0.0033896341919898987, -0.014548283070325851, 0.011565843597054482]\n",
      "epoch:  9460 training:  [-0.0013444144278764725, 0.03484322875738144, 0.10566587746143341]\n",
      "epoch:  9470 training:  [0.010189611464738846, -0.0005150660872459412, 0.01844486966729164]\n",
      "epoch:  9480 training:  [0.00871230661869049, -0.003488261252641678, 0.00026595592498779297]\n",
      "epoch:  9490 training:  [0.0027886780444532633, -0.0017514601349830627, 0.0068456800654530525]\n",
      "epoch:  9500 training:  [0.0036372090689837933, -0.005310025066137314, 0.02196582965552807]\n",
      "epoch:  9510 training:  [0.004341152496635914, 0.003697335720062256, 0.015476644039154053]\n",
      "epoch:  9520 training:  [0.0024407990276813507, 0.003568291664123535, 0.023746110498905182]\n",
      "epoch:  9530 training:  [5.9604644775390625e-05, 0.015828385949134827, 0.01910117268562317]\n",
      "epoch:  9540 training:  [0.004331193398684263, -0.002321392297744751, 0.033310793340206146]\n",
      "epoch:  9550 training:  [0.006242201663553715, 0.005251765251159668, 0.04048767685890198]\n",
      "epoch:  9560 training:  [0.0009167436510324478, 0.011473815888166428, 0.05374789610505104]\n",
      "epoch:  9570 training:  [0.004383133724331856, 0.009190764278173447, 0.0396040603518486]\n",
      "epoch:  9580 training:  [0.001250091940164566, 0.022974494844675064, 0.05262502282857895]\n",
      "epoch:  9590 training:  [-0.00042664632201194763, 0.004404187202453613, 0.015066586434841156]\n",
      "epoch:  9600 training:  [0.0015550348907709122, 0.0005070492625236511, 0.008543327450752258]\n",
      "epoch:  9610 training:  [0.006417903117835522, 0.010808758437633514, 0.058017536997795105]\n",
      "epoch:  9620 training:  [0.007171351462602615, -0.006385862827301025, 0.015611827373504639]\n",
      "epoch:  9630 training:  [0.0007896469905972481, -0.012871973216533661, 0.007795833051204681]\n",
      "epoch:  9640 training:  [0.0032871533185243607, -0.007629595696926117, 0.014084519818425179]\n",
      "epoch:  9650 training:  [0.00733557716012001, 0.011190101504325867, 0.013955703005194664]\n",
      "epoch:  9660 training:  [0.002584276720881462, -0.0029594972729682922, 0.019301075488328934]\n",
      "epoch:  9670 training:  [-0.0009058788418769836, -0.0008324310183525085, 0.021161744371056557]\n",
      "epoch:  9680 training:  [0.008496144786477089, 0.004396162927150726, 0.025706738233566284]\n",
      "epoch:  9690 training:  [0.007685644552111626, 0.021455949172377586, 0.06106385588645935]\n",
      "epoch:  9700 training:  [0.0116873849183321, 0.0119236521422863, 0.023787297308444977]\n",
      "epoch:  9710 training:  [0.00462543498724699, -0.0038152188062667847, 0.02495134249329567]\n",
      "epoch:  9720 training:  [0.012053029611706734, 0.012384705245494843, 0.026631101965904236]\n",
      "epoch:  9730 training:  [0.011357435956597328, 0.014775864779949188, 0.021144866943359375]\n",
      "epoch:  9740 training:  [0.0022346284240484238, -0.012900784611701965, 0.0179208405315876]\n",
      "epoch:  9750 training:  [0.009637322276830673, 0.005702260881662369, 0.00917418859899044]\n",
      "epoch:  9760 training:  [0.01715122163295746, 0.01049744337797165, 0.020033344626426697]\n",
      "epoch:  9770 training:  [0.0016804048791527748, -0.01025150716304779, 0.017087986692786217]\n",
      "epoch:  9780 training:  [0.006772918626666069, 0.022532980889081955, 0.03087344393134117]\n",
      "epoch:  9790 training:  [0.006593815516680479, -0.00814390555024147, 0.0025245100259780884]\n",
      "epoch:  9800 training:  [0.007843274623155594, -0.003078494220972061, 0.020711349323391914]\n",
      "epoch:  9810 training:  [0.0018434412777423859, -0.014179080724716187, 0.01853775419294834]\n",
      "epoch:  9820 training:  [0.0033959662541747093, 0.007889486849308014, 0.012089919298887253]\n",
      "epoch:  9830 training:  [0.006866798736155033, 0.018455399200320244, 0.02791060321033001]\n",
      "epoch:  9840 training:  [0.007722319103777409, -0.003947298973798752, 0.03392067179083824]\n",
      "epoch:  9850 training:  [0.007639105897396803, 0.017626574262976646, 0.017204459756612778]\n",
      "epoch:  9860 training:  [0.0050289444625377655, 0.0012843422591686249, 0.011735573410987854]\n",
      "epoch:  9870 training:  [0.010612711310386658, 0.008083697408437729, 0.014710668474435806]\n",
      "epoch:  9880 training:  [0.00768468901515007, -0.0036726966500282288, 0.02048642188310623]\n",
      "epoch:  9890 training:  [0.004884174559265375, 0.003033110871911049, -0.0021710991859436035]\n",
      "epoch:  9900 training:  [0.010076187551021576, 0.0033149830996990204, -0.00103006511926651]\n",
      "epoch:  9910 training:  [0.004304340109229088, -0.004164882004261017, 0.0027717649936676025]\n",
      "epoch:  9920 training:  [0.010669548064470291, 0.008361540734767914, 0.014698980376124382]\n",
      "epoch:  9930 training:  [0.011525909416377544, 0.008713562041521072, -0.014818966388702393]\n",
      "epoch:  9940 training:  [0.004754976369440556, 0.015091314911842346, 0.023374386131763458]\n",
      "epoch:  9950 training:  [0.00335683673620224, 0.011939659714698792, 0.03177575021982193]\n",
      "epoch:  9960 training:  [0.004553733393549919, 0.004907835274934769, 0.021535955369472504]\n",
      "epoch:  9970 training:  [0.008311038836836815, 0.005671095103025436, 0.042199183255434036]\n",
      "epoch:  9980 training:  [0.011134331114590168, 0.003767751157283783, 0.046926598995923996]\n",
      "epoch:  9990 training:  [0.004454645328223705, -0.0004383474588394165, 0.005413919687271118]\n",
      "epoch:  10000 training:  [0.006490279454737902, 0.00615735724568367, 0.01897173747420311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  10010 training:  [0.004943943582475185, -0.01453610509634018, 0.008921608328819275]\n",
      "epoch:  10020 training:  [0.003387789474800229, 0.006080232560634613, 0.032891325652599335]\n",
      "epoch:  10030 training:  [0.010408295318484306, 0.02489619143307209, 0.024177556857466698]\n",
      "epoch:  10040 training:  [0.006859591230750084, 0.00473962165415287, 0.013626154512166977]\n",
      "epoch:  10050 training:  [0.006114325486123562, 0.0022168196737766266, 0.025240249931812286]\n",
      "epoch:  10060 training:  [0.004251587204635143, 0.005317842587828636, 0.009555086493492126]\n",
      "epoch:  10070 training:  [-0.0023012589663267136, -0.0021792612969875336, -0.006239354610443115]\n",
      "epoch:  10080 training:  [0.0035457946360111237, 0.0051682740449905396, 0.04014141485095024]\n",
      "epoch:  10090 training:  [0.005386459641158581, 0.014940984547138214, 0.05282285809516907]\n",
      "epoch:  10100 training:  [0.002648906782269478, 0.0033187195658683777, 0.0245568435639143]\n",
      "epoch:  10110 training:  [0.012286238372325897, 0.00993461161851883, 0.023166052997112274]\n",
      "epoch:  10120 training:  [0.004411325789988041, 0.0006180144846439362, 0.012201135978102684]\n",
      "epoch:  10130 training:  [0.011795444414019585, 0.008298426866531372, 0.03153899684548378]\n",
      "epoch:  10140 training:  [0.01244577020406723, 0.017076585441827774, 0.07196470350027084]\n",
      "epoch:  10150 training:  [0.010006644763052464, 0.0005926117300987244, 0.01418047770857811]\n",
      "epoch:  10160 training:  [0.03023482859134674, 0.07700156420469284, 0.08230283856391907]\n",
      "epoch:  10170 training:  [0.011990795843303204, 0.007509678602218628, 0.02917618490755558]\n",
      "epoch:  10180 training:  [0.009766384959220886, 0.001841168850660324, 0.049066927284002304]\n",
      "epoch:  10190 training:  [0.003485478460788727, 0.002307988703250885, 0.031184479594230652]\n",
      "epoch:  10200 training:  [0.012089655734598637, 0.012188233435153961, 0.04488653317093849]\n",
      "epoch:  10210 training:  [-0.0007115118205547333, 0.00028514862060546875, 0.014167500659823418]\n",
      "epoch:  10220 training:  [0.01726456731557846, 0.022721577435731888, 0.0746375322341919]\n",
      "epoch:  10230 training:  [0.007074861787259579, -0.012325633317232132, 0.0503244549036026]\n",
      "epoch:  10240 training:  [0.005798471625894308, 0.0019493214786052704, 0.008731450885534286]\n",
      "epoch:  10250 training:  [0.01190726738423109, 0.010497711598873138, 0.051795460283756256]\n",
      "epoch:  10260 training:  [0.01058003306388855, 0.010658647865056992, 0.03835315257310867]\n",
      "epoch:  10270 training:  [0.005773485638201237, -0.005456447601318359, 0.013350043445825577]\n",
      "epoch:  10280 training:  [0.007852775976061821, 0.00340067595243454, 0.017974022775888443]\n",
      "epoch:  10290 training:  [0.020961064845323563, -0.0016395822167396545, 0.008702352643013]\n",
      "epoch:  10300 training:  [0.0073774587363004684, 0.0024667978286743164, 0.007818274199962616]\n",
      "epoch:  10310 training:  [0.01169737707823515, 0.004922498017549515, 0.02315732277929783]\n",
      "epoch:  10320 training:  [0.021437302231788635, 0.03805532678961754, 0.06977533549070358]\n",
      "epoch:  10330 training:  [0.005059609189629555, 0.008276574313640594, 0.018030688166618347]\n",
      "epoch:  10340 training:  [0.015956329181790352, 0.007806936278939247, 0.013877320103347301]\n",
      "epoch:  10350 training:  [0.020870668813586235, 0.017218908295035362, 0.016768116503953934]\n",
      "epoch:  10360 training:  [0.0023055318742990494, 0.010905066505074501, 0.031057074666023254]\n",
      "epoch:  10370 training:  [0.01399151049554348, 0.00905025377869606, 0.04124002903699875]\n",
      "epoch:  10380 training:  [0.013334520161151886, 0.009643875062465668, 0.0281534306704998]\n",
      "epoch:  10390 training:  [0.011778478510677814, 0.009388741105794907, 0.02059529535472393]\n",
      "epoch:  10400 training:  [0.00805006455630064, 0.019702337682247162, 0.0174395889043808]\n",
      "epoch:  10410 training:  [0.0012643523514270782, -0.002481915056705475, 0.027804439887404442]\n",
      "epoch:  10420 training:  [-0.0033592786639928818, -0.0011133383959531784, 0.017972726374864578]\n",
      "epoch:  10430 training:  [0.004587473813444376, 0.000445295125246048, 0.013408370316028595]\n",
      "epoch:  10440 training:  [0.007782807108014822, -0.014435023069381714, 0.010605156421661377]\n",
      "epoch:  10450 training:  [0.010210924781858921, 0.005961926653981209, 0.015260805375874043]\n",
      "epoch:  10460 training:  [0.0024392493069171906, 0.0071023814380168915, 0.017602553591132164]\n",
      "epoch:  10470 training:  [0.007176570128649473, 0.02971258945763111, 0.038539256900548935]\n",
      "epoch:  10480 training:  [0.010779868811368942, 0.01798335090279579, 0.052050188183784485]\n",
      "epoch:  10490 training:  [0.011417632922530174, 0.005799628794193268, 0.012248240411281586]\n",
      "epoch:  10500 training:  [0.009290923364460468, -0.0029017888009548187, 0.007308594882488251]\n",
      "epoch:  10510 training:  [0.010975170880556107, 0.00698436051607132, 0.010215505957603455]\n",
      "epoch:  10520 training:  [0.0042480905540287495, -0.003315754234790802, -0.0008761696517467499]\n",
      "epoch:  10530 training:  [0.0064530149102211, 0.004479359835386276, 0.07999825477600098]\n",
      "epoch:  10540 training:  [0.005708965007215738, -0.0033852197229862213, 0.021869415417313576]\n",
      "epoch:  10550 training:  [0.005958729423582554, 0.005482770502567291, 0.04414019733667374]\n",
      "epoch:  10560 training:  [-0.0017713643610477448, 0.006569337099790573, -0.0010556615889072418]\n",
      "epoch:  10570 training:  [0.008885666728019714, 0.0011648871004581451, -0.009787440299987793]\n",
      "epoch:  10580 training:  [0.00891460757702589, -0.0011368319392204285, 0.02559986151754856]\n",
      "epoch:  10590 training:  [0.005513334646821022, -0.007458627223968506, 0.013306360691785812]\n",
      "epoch:  10600 training:  [0.00810155924409628, -0.005581267178058624, 0.005574045702815056]\n",
      "epoch:  10610 training:  [0.006743628531694412, 0.0024825409054756165, 0.019450196996331215]\n",
      "epoch:  10620 training:  [0.0008000712841749191, -0.006638038903474808, 0.007512453943490982]\n",
      "epoch:  10630 training:  [0.011904693208634853, -7.893890142440796e-06, 0.005976337939500809]\n",
      "epoch:  10640 training:  [0.00750722736120224, 0.007574066519737244, 0.01841586083173752]\n",
      "epoch:  10650 training:  [0.007154098711907864, 0.028464674949645996, 0.004102684557437897]\n",
      "epoch:  10660 training:  [0.021399710327386856, 0.03879071772098541, 0.020531443879008293]\n",
      "epoch:  10670 training:  [0.010797795839607716, 0.010878680273890495, 0.052518926560878754]\n",
      "epoch:  10680 training:  [0.021030180156230927, 0.007470682263374329, -0.004966318607330322]\n",
      "epoch:  10690 training:  [0.004232301376760006, 0.02390754595398903, 0.012560107745230198]\n",
      "epoch:  10700 training:  [0.0002591647207736969, -0.01366911455988884, 0.020457115024328232]\n",
      "epoch:  10710 training:  [0.006668653804808855, 0.01057242602109909, 0.02851847931742668]\n",
      "epoch:  10720 training:  [0.01735350489616394, 0.024744626134634018, 0.034028999507427216]\n",
      "epoch:  10730 training:  [0.012972673401236534, 0.013225611299276352, 0.023565366864204407]\n",
      "epoch:  10740 training:  [0.014369262382388115, 0.00615387037396431, 0.06509944051504135]\n",
      "epoch:  10750 training:  [0.0070334067568182945, 0.012766974046826363, 0.00697631761431694]\n",
      "epoch:  10760 training:  [0.013160349801182747, 0.024295907467603683, 0.047593504190444946]\n",
      "epoch:  10770 training:  [-0.0005501396954059601, 0.02069968730211258, 0.04188576713204384]\n",
      "epoch:  10780 training:  [0.010945556685328484, -0.003269258886575699, 0.005068108439445496]\n",
      "epoch:  10790 training:  [0.01132146641612053, -0.004245758056640625, -0.0003361031413078308]\n",
      "epoch:  10800 training:  [0.0049523599445819855, -0.005647420883178711, 0.007020138204097748]\n",
      "epoch:  10810 training:  [0.01033063419163227, -0.01314864307641983, 0.004056792706251144]\n",
      "epoch:  10820 training:  [0.00521055655553937, 0.0030691958963871002, 0.01495257206261158]\n",
      "epoch:  10830 training:  [0.003014802932739258, 0.026674602180719376, 0.06552254408597946]\n",
      "epoch:  10840 training:  [0.004359239712357521, 0.004466168582439423, 0.024371450766921043]\n",
      "epoch:  10850 training:  [0.00579838128760457, 0.012733768671751022, 0.0003994107246398926]\n",
      "epoch:  10860 training:  [0.005381562747061253, 0.0017330162227153778, 0.005647275596857071]\n",
      "epoch:  10870 training:  [0.008437197655439377, -0.00017068535089492798, 0.010133471339941025]\n",
      "epoch:  10880 training:  [0.004975578747689724, 0.002260316163301468, 0.009204946458339691]\n",
      "epoch:  10890 training:  [0.004136884119361639, -0.009371530264616013, 0.021786749362945557]\n",
      "epoch:  10900 training:  [0.006402421742677689, -0.013688433915376663, -0.0038229748606681824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  10910 training:  [0.008913140743970871, 0.005869373679161072, 0.03266327828168869]\n",
      "epoch:  10920 training:  [0.00884589459747076, 0.0009146071970462799, 0.014866746962070465]\n",
      "epoch:  10930 training:  [0.007093079388141632, 0.00646982342004776, 0.025669436901807785]\n",
      "epoch:  10940 training:  [0.007550298236310482, 0.005299631506204605, 0.008012592792510986]\n",
      "epoch:  10950 training:  [0.00020786747336387634, 0.008902417495846748, 0.022154677659273148]\n",
      "epoch:  10960 training:  [0.011489310301840305, 0.03379450738430023, 0.02605423331260681]\n",
      "epoch:  10970 training:  [0.010475628077983856, -0.0011925771832466125, 0.017074119299650192]\n",
      "epoch:  10980 training:  [0.024229807779192924, 0.01754888892173767, 0.019201116636395454]\n",
      "epoch:  10990 training:  [-0.0021824948489665985, -0.0012876428663730621, 0.020407911390066147]\n",
      "epoch:  11000 training:  [0.004882393404841423, 0.002987578511238098, 0.02474183589220047]\n",
      "epoch:  11010 training:  [-0.007008334621787071, -0.013164274394512177, 0.013339236378669739]\n",
      "epoch:  11020 training:  [0.0005004303529858589, 0.01869051903486252, 0.011681847274303436]\n",
      "epoch:  11030 training:  [0.010972615331411362, -0.00010301917791366577, 0.012477678246796131]\n",
      "epoch:  11040 training:  [0.008076890371739864, 0.010222612880170345, 0.025175677612423897]\n",
      "epoch:  11050 training:  [0.04453304409980774, 0.05920875817537308, 0.011718695983290672]\n",
      "epoch:  11060 training:  [0.0030848663300275803, -0.008440423756837845, 0.012096373364329338]\n",
      "epoch:  11070 training:  [0.006161250174045563, 0.013158934190869331, 0.017219366505742073]\n",
      "epoch:  11080 training:  [-0.0012049786746501923, 0.0031167566776275635, 0.018958108499646187]\n",
      "epoch:  11090 training:  [0.01121499016880989, 0.006029184907674789, 0.025805283337831497]\n",
      "epoch:  11100 training:  [0.010418727062642574, 0.00019153952598571777, 0.04059728980064392]\n",
      "epoch:  11110 training:  [0.0133232232183218, 0.04448994994163513, 0.09944617748260498]\n",
      "epoch:  11120 training:  [0.006285311188548803, 0.006111130118370056, 0.014717226848006248]\n",
      "epoch:  11130 training:  [0.006575348787009716, -0.0015552081167697906, 0.02400950714945793]\n",
      "epoch:  11140 training:  [0.011489540338516235, 0.005731532350182533, 0.012116502970457077]\n",
      "epoch:  11150 training:  [0.003852827474474907, 0.002342069521546364, 0.0069710370153188705]\n",
      "epoch:  11160 training:  [0.011692527681589127, 0.002842862159013748, 0.006346236914396286]\n",
      "epoch:  11170 training:  [0.006782611832022667, 0.011916521936655045, 0.016614457592368126]\n",
      "epoch:  11180 training:  [0.0076690735295414925, -0.00045230984687805176, 0.012140846811234951]\n",
      "epoch:  11190 training:  [0.012115279212594032, 0.008610300719738007, 0.06853248178958893]\n",
      "epoch:  11200 training:  [0.005932769272476435, 0.0006411932408809662, 0.006255965679883957]\n",
      "epoch:  11210 training:  [0.005058289505541325, -0.00024062767624855042, 0.011628620326519012]\n",
      "epoch:  11220 training:  [0.004972327500581741, -0.00675506517291069, 0.009019844233989716]\n",
      "epoch:  11230 training:  [0.024564016610383987, -0.0034721679985523224, 0.01974988356232643]\n",
      "epoch:  11240 training:  [0.033462196588516235, 0.0352415032684803, 0.05578673258423805]\n",
      "epoch:  11250 training:  [0.015048153698444366, 0.029608769342303276, 0.02763942815363407]\n",
      "epoch:  11260 training:  [0.007852398790419102, 0.01770821027457714, 0.03644261881709099]\n",
      "epoch:  11270 training:  [0.01218840479850769, 0.009414678439497948, 0.008940652012825012]\n",
      "epoch:  11280 training:  [0.008513865992426872, 0.004708390682935715, 0.021536611020565033]\n",
      "epoch:  11290 training:  [0.011268025264143944, 0.0039021652191877365, 0.015490200370550156]\n",
      "epoch:  11300 training:  [0.012192480266094208, 0.011173371225595474, -0.03198997676372528]\n",
      "epoch:  11310 training:  [0.011041868478059769, 0.015787305310368538, 0.016877958551049232]\n",
      "epoch:  11320 training:  [0.011420514434576035, 0.0018391124904155731, 0.005233310163021088]\n",
      "epoch:  11330 training:  [0.013008789159357548, 0.021388936787843704, 0.017523450776934624]\n",
      "epoch:  11340 training:  [0.008783763274550438, 0.01658172532916069, 0.012027442455291748]\n",
      "epoch:  11350 training:  [0.024975821375846863, 0.022763624787330627, 0.03886812552809715]\n",
      "epoch:  11360 training:  [0.008860770612955093, 0.010568056255578995, 0.01854970119893551]\n",
      "epoch:  11370 training:  [0.011826718226075172, 0.01622064784169197, 0.020597580820322037]\n",
      "epoch:  11380 training:  [0.003731849603354931, -0.003427468240261078, 0.01985429972410202]\n",
      "epoch:  11390 training:  [0.02068408951163292, 0.01695883832871914, 0.04333306849002838]\n",
      "epoch:  11400 training:  [0.010968705639243126, 0.005208980292081833, 0.02787812054157257]\n",
      "epoch:  11410 training:  [0.006722759921103716, -0.0023896731436252594, 0.009528852067887783]\n",
      "epoch:  11420 training:  [0.009300408884882927, 0.0159183070063591, 0.04680422320961952]\n",
      "epoch:  11430 training:  [0.00665912963449955, 0.009313568472862244, 0.014403587207198143]\n",
      "epoch:  11440 training:  [0.002781304996460676, -0.0011861547827720642, 0.010939151048660278]\n",
      "epoch:  11450 training:  [0.007737432606518269, 0.0017700493335723877, 0.0038677100092172623]\n",
      "epoch:  11460 training:  [0.009157845750451088, 0.006837710738182068, 0.017866969108581543]\n",
      "epoch:  11470 training:  [0.017996877431869507, 0.022750157862901688, 0.032238587737083435]\n",
      "epoch:  11480 training:  [0.010657336562871933, 0.003052443265914917, 0.03170167654752731]\n",
      "epoch:  11490 training:  [0.0052178408950567245, -0.009851481765508652, 0.013368709944188595]\n",
      "epoch:  11500 training:  [0.0191243514418602, 0.012677442282438278, 0.010340549051761627]\n",
      "epoch:  11510 training:  [0.0077362339943647385, -0.0031962431967258453, 0.004329228773713112]\n",
      "epoch:  11520 training:  [0.006207847967743874, 0.005558042787015438, -0.004952959716320038]\n",
      "epoch:  11530 training:  [0.013925101608037949, 0.011308327317237854, 0.03275454416871071]\n",
      "epoch:  11540 training:  [0.006579605862498283, 0.00016073137521743774, 0.012384328991174698]\n",
      "epoch:  11550 training:  [0.006074143573641777, 0.014298085123300552, 0.027196135371923447]\n",
      "epoch:  11560 training:  [0.007525081280618906, 0.010054446756839752, 0.008380977436900139]\n",
      "epoch:  11570 training:  [0.014661993831396103, 0.03355463594198227, 0.038695577532052994]\n",
      "epoch:  11580 training:  [0.005577458068728447, 0.01847114786505699, 0.027231581509113312]\n",
      "epoch:  11590 training:  [0.006223483011126518, 0.019177673384547234, 0.03198173642158508]\n",
      "epoch:  11600 training:  [0.010263831354677677, 0.00022297725081443787, 0.025414135307073593]\n",
      "epoch:  11610 training:  [0.007275802083313465, 0.005850240588188171, 0.01879270002245903]\n",
      "epoch:  11620 training:  [0.013487420976161957, 0.016618996858596802, 0.05167996883392334]\n",
      "epoch:  11630 training:  [-0.0008728541433811188, 0.006258957087993622, 0.01998363621532917]\n",
      "epoch:  11640 training:  [0.008307845331728458, 0.0017674919217824936, 0.02997162565588951]\n",
      "epoch:  11650 training:  [0.05054623633623123, 0.032425835728645325, 0.033135756850242615]\n",
      "epoch:  11660 training:  [0.009185676462948322, 0.03057686612010002, 0.033415816724300385]\n",
      "epoch:  11670 training:  [0.0029331576079130173, 0.0032382160425186157, 0.012690866366028786]\n",
      "epoch:  11680 training:  [0.010657861828804016, 0.007378829643130302, 0.01107210386544466]\n",
      "epoch:  11690 training:  [-0.005115237087011337, 0.0025723688304424286, 0.02508588507771492]\n",
      "epoch:  11700 training:  [0.003185547888278961, 0.005610845983028412, 0.014644675888121128]\n",
      "epoch:  11710 training:  [0.007719188462942839, 0.0050568897277116776, 0.007071869447827339]\n",
      "epoch:  11720 training:  [0.006097408011555672, 0.014983408153057098, 0.017221275717020035]\n",
      "epoch:  11730 training:  [0.006930493749678135, 0.0064736008644104, -0.0022493526339530945]\n",
      "epoch:  11740 training:  [0.012495708651840687, 0.03460590913891792, 0.016408683732151985]\n",
      "epoch:  11750 training:  [0.00996343046426773, 0.005793899297714233, 0.012384431436657906]\n",
      "epoch:  11760 training:  [0.008765393868088722, -0.01536785438656807, 0.009393921121954918]\n",
      "epoch:  11770 training:  [0.01203077845275402, 0.008317507803440094, 0.028459714725613594]\n",
      "epoch:  11780 training:  [0.0033166520297527313, -0.001809101551771164, 0.007058326154947281]\n",
      "epoch:  11790 training:  [0.007112913765013218, -0.031402066349983215, -0.004518918693065643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  11800 training:  [0.0113845095038414, 0.01539980061352253, 0.054301917552948]\n",
      "epoch:  11810 training:  [0.00890866108238697, 0.006008755415678024, -0.001812577247619629]\n",
      "epoch:  11820 training:  [0.0042508989572525024, 0.015345979481935501, 0.002497635781764984]\n",
      "epoch:  11830 training:  [0.0024270564317703247, 0.007851876318454742, 0.019731596112251282]\n",
      "epoch:  11840 training:  [0.008614814840257168, 0.002501577138900757, 0.03717944025993347]\n",
      "epoch:  11850 training:  [0.0031334715895354748, 0.016663599759340286, 0.022629983723163605]\n",
      "epoch:  11860 training:  [0.005677295848727226, -0.006702356040477753, 0.008465401828289032]\n",
      "epoch:  11870 training:  [0.015444694086909294, 0.010643616318702698, 0.04152859002351761]\n",
      "epoch:  11880 training:  [0.002886701375246048, 0.0179428793489933, 0.04789550602436066]\n",
      "epoch:  11890 training:  [0.010538242757320404, 0.01727796159684658, 0.024820469319820404]\n",
      "epoch:  11900 training:  [0.0023515173234045506, 0.0038644541054964066, -0.0017904266715049744]\n",
      "epoch:  11910 training:  [0.01203064527362585, 0.011640675365924835, 0.040202319622039795]\n",
      "epoch:  11920 training:  [0.016260476782917976, 0.03646877408027649, 0.033337511122226715]\n",
      "epoch:  11930 training:  [0.005893814377486706, -0.006481301039457321, 0.010217295959591866]\n",
      "epoch:  11940 training:  [0.0042143333703279495, 0.007843341678380966, 0.01054089143872261]\n",
      "epoch:  11950 training:  [0.009796269237995148, 0.02584681287407875, 0.013457863591611385]\n",
      "epoch:  11960 training:  [0.009843902662396431, 0.01258588582277298, 0.01646265760064125]\n",
      "epoch:  11970 training:  [0.02601514756679535, 0.04088251665234566, 0.044356755912303925]\n",
      "epoch:  11980 training:  [0.005832806695252657, -0.004762273281812668, 0.019778557121753693]\n",
      "epoch:  11990 training:  [0.0010682325810194016, 0.012757822871208191, 0.007184064015746117]\n",
      "epoch:  12000 training:  [0.00530967116355896, 0.03433433920145035, 0.025251861661672592]\n",
      "epoch:  12010 training:  [0.00808965414762497, -0.00036638230085372925, 0.01211836189031601]\n",
      "epoch:  12020 training:  [0.02557361125946045, 0.010545443743467331, 0.09018248319625854]\n",
      "epoch:  12030 training:  [0.015567016787827015, 0.004962518811225891, 0.06451041251420975]\n",
      "epoch:  12040 training:  [0.013183335773646832, 0.006853695958852768, 0.018656771630048752]\n",
      "epoch:  12050 training:  [0.007397049106657505, -0.0017937645316123962, 0.01780843362212181]\n",
      "epoch:  12060 training:  [0.0035602785646915436, -0.0032273009419441223, 0.011125907301902771]\n",
      "epoch:  12070 training:  [0.010784029960632324, -0.0006278455257415771, -0.009731687605381012]\n",
      "epoch:  12080 training:  [-0.004484470933675766, 0.019141878932714462, 0.01154261827468872]\n",
      "epoch:  12090 training:  [0.012055965140461922, 0.03364628553390503, 0.020747773349285126]\n",
      "epoch:  12100 training:  [0.0120933186262846, 0.038569726049900055, 0.01510995626449585]\n",
      "epoch:  12110 training:  [0.0013755988329648972, -0.004857458174228668, 0.015775933861732483]\n",
      "epoch:  12120 training:  [0.009288452565670013, 0.021748799830675125, 0.03465765714645386]\n",
      "epoch:  12130 training:  [0.008062807843089104, 0.008350800722837448, 0.019839690998196602]\n",
      "epoch:  12140 training:  [0.006240047514438629, 0.009074874222278595, 0.007984567433595657]\n",
      "epoch:  12150 training:  [0.00993808638304472, -0.00238124281167984, 0.011509738862514496]\n",
      "epoch:  12160 training:  [0.004701215773820877, 0.010167796164751053, 0.0029591843485832214]\n",
      "epoch:  12170 training:  [0.012165145017206669, 0.008860660716891289, 0.010755503550171852]\n",
      "epoch:  12180 training:  [0.015247462317347527, -0.0006321258842945099, 0.016914263367652893]\n",
      "epoch:  12190 training:  [0.009230040945112705, 0.004209669306874275, 0.030072059482336044]\n",
      "epoch:  12200 training:  [0.007908469997346401, 0.011606283485889435, 0.007090520113706589]\n",
      "epoch:  12210 training:  [0.0076274205930531025, -0.0044189877808094025, -0.0077711790800094604]\n",
      "epoch:  12220 training:  [0.005082624964416027, -0.0012316368520259857, 0.014563821256160736]\n",
      "epoch:  12230 training:  [0.00312053132802248, 0.004628889262676239, 0.024897707626223564]\n",
      "epoch:  12240 training:  [0.005527721717953682, 0.012215755879878998, 0.032886795699596405]\n",
      "epoch:  12250 training:  [-0.00018407590687274933, 0.004551839083433151, 0.009821562096476555]\n",
      "epoch:  12260 training:  [0.0032708263024687767, 0.0036038383841514587, -0.031768277287483215]\n",
      "epoch:  12270 training:  [0.007945612072944641, -0.007306959480047226, -0.007249221205711365]\n",
      "epoch:  12280 training:  [0.016676602885127068, 0.03555241599678993, 0.06703873723745346]\n",
      "epoch:  12290 training:  [0.01378016546368599, 0.026534931734204292, 0.07582444697618484]\n",
      "epoch:  12300 training:  [0.00678236735984683, 0.01470198854804039, 0.02872803434729576]\n",
      "epoch:  12310 training:  [0.013356841169297695, 0.047731880098581314, 0.06504704058170319]\n",
      "epoch:  12320 training:  [0.0039036348462104797, 0.001598350703716278, 0.022475944831967354]\n",
      "epoch:  12330 training:  [0.01254094298928976, 0.0027954205870628357, 0.0094384104013443]\n",
      "epoch:  12340 training:  [0.0038044024258852005, 0.01531974971294403, 0.02962227165699005]\n",
      "epoch:  12350 training:  [0.01311495155096054, 0.021519578993320465, 0.05871589854359627]\n",
      "epoch:  12360 training:  [0.010478581301867962, 0.008362997323274612, 0.03921250253915787]\n",
      "epoch:  12370 training:  [0.007686463650316, 0.009581181220710278, 0.04843444377183914]\n",
      "epoch:  12380 training:  [0.01634349301457405, 0.013188637793064117, 0.05703305825591087]\n",
      "epoch:  12390 training:  [0.0035696150735020638, 0.002307433634996414, 0.013484589755535126]\n",
      "epoch:  12400 training:  [0.0033996030688285828, 0.016558464616537094, 0.049402035772800446]\n",
      "epoch:  12410 training:  [0.012226104736328125, 0.012460868805646896, 0.029076505452394485]\n",
      "epoch:  12420 training:  [0.008250623010098934, 0.01542588323354721, 0.02351076528429985]\n",
      "epoch:  12430 training:  [0.009355918504297733, 0.03705471381545067, 0.08744964748620987]\n",
      "epoch:  12440 training:  [0.00884726457297802, 0.06342930346727371, 0.06300077587366104]\n",
      "epoch:  12450 training:  [0.004614860750734806, 0.009632609784603119, 0.04214802384376526]\n",
      "epoch:  12460 training:  [0.011365516111254692, 0.012170737609267235, 0.019620567560195923]\n",
      "epoch:  12470 training:  [0.009349514730274677, -0.00043910369277000427, 0.021043697372078896]\n",
      "epoch:  12480 training:  [0.013413198292255402, 0.00787990540266037, 0.02470821514725685]\n",
      "epoch:  12490 training:  [0.01707630231976509, 0.010450322180986404, 0.03981752693653107]\n",
      "epoch:  12500 training:  [0.01000332459807396, 0.02919304557144642, 0.08198334276676178]\n",
      "epoch:  12510 training:  [0.006975648924708366, -0.0007101744413375854, 0.0020222701132297516]\n",
      "epoch:  12520 training:  [0.006281294859945774, -0.01007721945643425, 0.03384130448102951]\n",
      "epoch:  12530 training:  [-0.004227999597787857, 0.0027414485812187195, 0.010119863785803318]\n",
      "epoch:  12540 training:  [0.007129449397325516, 0.006596215069293976, 0.024173667654395103]\n",
      "epoch:  12550 training:  [-0.0035186205059289932, 0.002241283655166626, 0.010428879410028458]\n",
      "epoch:  12560 training:  [-0.011252012103796005, -0.005669824779033661, 0.020556984469294548]\n",
      "epoch:  12570 training:  [0.007862390950322151, -0.00038166344165802, 0.020725399255752563]\n",
      "epoch:  12580 training:  [0.006385831627994776, 0.00024423748254776, 0.014967218972742558]\n",
      "epoch:  12590 training:  [0.004772702232003212, -0.0016903877258300781, 0.009215310215950012]\n",
      "epoch:  12600 training:  [0.007401172071695328, -0.000253874808549881, 0.008626416325569153]\n",
      "epoch:  12610 training:  [0.006044830195605755, -0.006307203322649002, 0.009611481800675392]\n",
      "epoch:  12620 training:  [0.009166625328361988, -0.014897678047418594, 0.009873947128653526]\n",
      "epoch:  12630 training:  [0.0030874190852046013, -0.014859747141599655, 0.0019274093210697174]\n",
      "epoch:  12640 training:  [-0.0008241254836320877, 0.00980769470334053, 0.01585264876484871]\n",
      "epoch:  12650 training:  [0.0070977527648210526, 0.019056441262364388, 0.018375176936388016]\n",
      "epoch:  12660 training:  [0.0032507111318409443, -0.0014276616275310516, 0.02031068131327629]\n",
      "epoch:  12670 training:  [0.013389289379119873, 0.02839186042547226, 0.031202640384435654]\n",
      "epoch:  12680 training:  [0.013536784797906876, -0.005089636892080307, 0.010857675224542618]\n",
      "epoch:  12690 training:  [0.009986517950892448, 0.009751584380865097, 0.05129377916455269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  12700 training:  [0.005290934816002846, 0.005692293867468834, 0.03458026051521301]\n",
      "epoch:  12710 training:  [0.007454621605575085, -0.007509306073188782, 0.010178787633776665]\n",
      "epoch:  12720 training:  [0.0063688792288303375, 0.0007297024130821228, 0.01584881916642189]\n",
      "epoch:  12730 training:  [0.009522100910544395, 0.00939573347568512, 0.017286548390984535]\n",
      "epoch:  12740 training:  [0.011039325967431068, 0.005103558301925659, 0.01820765808224678]\n",
      "epoch:  12750 training:  [0.004019483923912048, 0.0011397898197174072, 0.007327606435865164]\n",
      "epoch:  12760 training:  [0.006758246570825577, -0.042547039687633514, 0.00356413796544075]\n",
      "epoch:  12770 training:  [0.014507599174976349, 0.004688907414674759, 0.02227954752743244]\n",
      "epoch:  12780 training:  [0.012561287730932236, 0.01409432664513588, 0.010174544528126717]\n",
      "epoch:  12790 training:  [0.006397917866706848, 0.00504109263420105, 0.00722329318523407]\n",
      "epoch:  12800 training:  [0.006390244700014591, 0.005491364747285843, 0.012098489329218864]\n",
      "epoch:  12810 training:  [0.009076007641851902, 0.0030306391417980194, -0.01051921397447586]\n",
      "epoch:  12820 training:  [0.0009147073142230511, -0.006141725927591324, 0.0077226776629686356]\n",
      "epoch:  12830 training:  [0.00766032375395298, 0.013356022536754608, 0.01645476371049881]\n",
      "epoch:  12840 training:  [0.014939879067242146, 0.024754412472248077, 0.03583979234099388]\n",
      "epoch:  12850 training:  [0.021180270239710808, 0.06373882293701172, 0.037878479808568954]\n",
      "epoch:  12860 training:  [0.003829037770628929, 0.008805286139249802, 0.03855806216597557]\n",
      "epoch:  12870 training:  [0.009682448580861092, 0.014861023984849453, 0.008199024945497513]\n",
      "epoch:  12880 training:  [0.006852480582892895, 0.006969824433326721, 0.024247456341981888]\n",
      "epoch:  12890 training:  [0.009211983531713486, -0.00018096715211868286, -0.01882719248533249]\n",
      "epoch:  12900 training:  [0.004990940913558006, -0.0018755719065666199, 0.007193313911557198]\n",
      "epoch:  12910 training:  [0.006612651981413364, -0.0025199726223945618, 0.009130438789725304]\n",
      "epoch:  12920 training:  [0.004770207218825817, 0.011401142925024033, 0.022309277206659317]\n",
      "epoch:  12930 training:  [0.006222984753549099, 0.008045248687267303, 0.04192300885915756]\n",
      "epoch:  12940 training:  [0.011204780079424381, -0.008256569504737854, 0.029351510107517242]\n",
      "epoch:  12950 training:  [0.0064932298846542835, 0.004548624157905579, 0.012567572295665741]\n",
      "epoch:  12960 training:  [0.007025416474789381, -0.006021350622177124, 0.020228547975420952]\n",
      "epoch:  12970 training:  [0.003464015666395426, 0.0077012814581394196, 0.009308857843279839]\n",
      "epoch:  12980 training:  [0.0017509814351797104, 0.004708629101514816, 0.00778941810131073]\n",
      "epoch:  12990 training:  [0.011488823220133781, 0.004171848297119141, 0.025877054780721664]\n",
      "epoch:  13000 training:  [0.008627694100141525, -0.0025857873260974884, 0.014784879051148891]\n",
      "epoch:  13010 training:  [0.0009544864296913147, 0.021419085562229156, 0.007812152616679668]\n",
      "epoch:  13020 training:  [0.004451763816177845, 0.005745274946093559, 0.015517371706664562]\n",
      "epoch:  13030 training:  [0.008441822603344917, 0.00893530622124672, 0.02742193453013897]\n",
      "epoch:  13040 training:  [0.0036150948144495487, -0.007133569568395615, 0.013031292706727982]\n",
      "epoch:  13050 training:  [0.008186295628547668, 0.008006926625967026, 0.018711434677243233]\n",
      "epoch:  13060 training:  [0.008636008016765118, 0.009957380592823029, 0.03501823544502258]\n",
      "epoch:  13070 training:  [0.010851594619452953, 0.011223094537854195, 0.017442915588617325]\n",
      "epoch:  13080 training:  [0.02521231211721897, 0.016967974603176117, 0.10923145711421967]\n",
      "epoch:  13090 training:  [0.014128895476460457, 0.03390996530652046, 0.04901169613003731]\n",
      "epoch:  13100 training:  [0.0042888387106359005, -0.0012190397828817368, 0.008578651584684849]\n",
      "epoch:  13110 training:  [0.006324881687760353, -0.026495635509490967, 0.014425967819988728]\n",
      "epoch:  13120 training:  [0.007151950150728226, -0.004290593788027763, 0.008485686033964157]\n",
      "epoch:  13130 training:  [0.017476210370659828, 0.014939587563276291, 0.06304197758436203]\n",
      "epoch:  13140 training:  [0.0031026042997837067, 0.003855746239423752, 0.01251661404967308]\n",
      "epoch:  13150 training:  [0.0056807962246239185, 0.001756308600306511, 0.01653803139925003]\n",
      "epoch:  13160 training:  [0.004346552304923534, 0.0046169571578502655, 0.009103415533900261]\n",
      "epoch:  13170 training:  [-0.0025134552270174026, -0.0008803736418485641, -0.001416873186826706]\n",
      "epoch:  13180 training:  [0.004673031158745289, 3.189407289028168e-05, 0.011940691620111465]\n",
      "epoch:  13190 training:  [0.0021905917674303055, 0.030125483870506287, 0.024313338100910187]\n",
      "epoch:  13200 training:  [0.039707183837890625, 0.03486882150173187, 0.06744587421417236]\n",
      "epoch:  13210 training:  [0.01352840568870306, 0.004913456737995148, 0.019916586577892303]\n",
      "epoch:  13220 training:  [0.010357093065977097, 0.006722576916217804, 0.024084923788905144]\n",
      "epoch:  13230 training:  [0.007667959667742252, -0.0013171471655368805, 0.01283213496208191]\n",
      "epoch:  13240 training:  [0.01022089459002018, -0.0008558481931686401, 0.010976511985063553]\n",
      "epoch:  13250 training:  [0.008613868616521358, 0.008932054974138737, 0.028887499123811722]\n",
      "epoch:  13260 training:  [0.008797978982329369, -0.019390195608139038, 0.01193627342581749]\n",
      "epoch:  13270 training:  [0.008755102753639221, 0.022363875061273575, 0.03447618708014488]\n",
      "epoch:  13280 training:  [0.012037926353514194, 0.03231349587440491, 0.031585026532411575]\n",
      "epoch:  13290 training:  [0.0021334984339773655, 0.008169826120138168, 0.023450184613466263]\n",
      "epoch:  13300 training:  [0.002692822366952896, 0.006593521684408188, 0.016992339864373207]\n",
      "epoch:  13310 training:  [0.005608060397207737, -0.0001603439450263977, 0.031230363994836807]\n",
      "epoch:  13320 training:  [0.007577942684292793, 0.01831386424601078, 0.038660865277051926]\n",
      "epoch:  13330 training:  [0.035244639962911606, 0.037695132195949554, 0.07771234959363937]\n",
      "epoch:  13340 training:  [0.01791439764201641, 0.05131535232067108, 0.02650502696633339]\n",
      "epoch:  13350 training:  [0.0038471571169793606, 0.028966907411813736, 0.014269184321165085]\n",
      "epoch:  13360 training:  [0.004019917920231819, 0.035388365387916565, 0.025894954800605774]\n",
      "epoch:  13370 training:  [0.005172174423933029, 0.0147794708609581, 0.009775449521839619]\n",
      "epoch:  13380 training:  [0.009750178083777428, 0.017083797603845596, 0.014440972357988358]\n",
      "epoch:  13390 training:  [0.01306334137916565, 0.010324563831090927, 0.056617606431245804]\n",
      "epoch:  13400 training:  [0.008620982989668846, 0.040963608771562576, 0.04633207991719246]\n",
      "epoch:  13410 training:  [0.0179144274443388, 0.09303689748048782, 0.0540953166782856]\n",
      "epoch:  13420 training:  [0.010061295703053474, 0.021828435361385345, 0.0990048348903656]\n",
      "epoch:  13430 training:  [0.018280496820807457, 0.09662555158138275, 0.027418114244937897]\n",
      "epoch:  13440 training:  [0.01470407284796238, 0.0348045751452446, 0.013086102902889252]\n",
      "epoch:  13450 training:  [0.018573356792330742, 0.000744204968214035, 0.018902074545621872]\n",
      "epoch:  13460 training:  [0.008573727682232857, -0.0012569781392812729, 0.02281377464532852]\n",
      "epoch:  13470 training:  [0.012931348755955696, -0.004249516874551773, 0.05403485894203186]\n",
      "epoch:  13480 training:  [0.020171336829662323, 0.020390376448631287, 0.05550052970647812]\n",
      "epoch:  13490 training:  [0.017341915518045425, 0.019498731940984726, 0.016481120139360428]\n",
      "epoch:  13500 training:  [0.02036399021744728, 0.011838577687740326, 0.1181875467300415]\n",
      "epoch:  13510 training:  [0.003509487956762314, 0.0007256492972373962, -0.0013628937304019928]\n",
      "epoch:  13520 training:  [0.0017614541575312614, -0.00571819394826889, 0.02191561460494995]\n",
      "epoch:  13530 training:  [0.011895429342985153, 0.02285612002015114, 0.03660869970917702]\n",
      "epoch:  13540 training:  [0.011624889448285103, 0.01761726289987564, 0.018498145043849945]\n",
      "epoch:  13550 training:  [0.009129227139055729, 0.0004398338496685028, 0.010012143291532993]\n",
      "epoch:  13560 training:  [0.010997877456247807, 0.004970036447048187, 0.009632796049118042]\n",
      "epoch:  13570 training:  [0.014242948032915592, 0.02229573205113411, 0.05406540259718895]\n",
      "epoch:  13580 training:  [0.012807950377464294, 0.008254734799265862, 0.015930814668536186]\n",
      "epoch:  13590 training:  [0.010062789544463158, 0.017967548221349716, 0.026857713237404823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  13600 training:  [0.014230367727577686, 0.006239522248506546, 0.0394924134016037]\n",
      "epoch:  13610 training:  [0.01075243204832077, -0.004063326865434647, 0.021727412939071655]\n",
      "epoch:  13620 training:  [0.12177062779664993, 0.27099186182022095, 0.4705240726470947]\n",
      "epoch:  13630 training:  [0.3510120213031769, 0.48148345947265625, 0.6868048906326294]\n",
      "epoch:  13640 training:  [0.5357881784439087, 0.385509192943573, 0.28769880533218384]\n",
      "epoch:  13650 training:  [0.3544826805591583, 0.18116174638271332, 0.33813387155532837]\n",
      "epoch:  13660 training:  [0.5671097040176392, 0.3608956038951874, 0.2500133514404297]\n",
      "epoch:  13670 training:  [0.5707197785377502, 0.3535971939563751, 0.40471988916397095]\n",
      "epoch:  13680 training:  [0.48613566160202026, 0.3008081912994385, 0.31364864110946655]\n",
      "epoch:  13690 training:  [0.41467320919036865, 0.2716352343559265, 0.30276739597320557]\n",
      "epoch:  13700 training:  [0.2922760248184204, 0.24385195970535278, 0.20511290431022644]\n",
      "epoch:  13710 training:  [0.5066825747489929, 0.2961103916168213, 0.18943136930465698]\n",
      "epoch:  13720 training:  [1.8873132467269897, 4.304895877838135, 10.240335464477539]\n",
      "epoch:  13730 training:  [0.3250761926174164, 0.10435736924409866, 0.3809364438056946]\n",
      "epoch:  13740 training:  [0.45883142948150635, 0.0527351051568985, 0.3544681668281555]\n",
      "epoch:  13750 training:  [0.39567679166793823, 0.11555693298578262, 0.22922833263874054]\n",
      "epoch:  13760 training:  [0.5741304159164429, 0.2324661910533905, 0.45125719904899597]\n",
      "epoch:  13770 training:  [0.4926677346229553, 0.20109790563583374, 0.44048765301704407]\n",
      "epoch:  13780 training:  [0.4162535071372986, 0.18285301327705383, 0.55093914270401]\n",
      "epoch:  13790 training:  [0.3946629464626312, 0.12486682832241058, 0.6302976608276367]\n",
      "epoch:  13800 training:  [0.5550417304039001, 0.2878718972206116, 0.5085226893424988]\n",
      "epoch:  13810 training:  [0.3981100916862488, 0.20090577006340027, 0.4671604037284851]\n",
      "epoch:  13820 training:  [0.34777459502220154, 0.11830790340900421, 0.40725627541542053]\n",
      "epoch:  13830 training:  [0.4547467827796936, 0.15002208948135376, 0.46928155422210693]\n",
      "epoch:  13840 training:  [0.4094100594520569, 0.20400208234786987, 0.42921632528305054]\n",
      "epoch:  13850 training:  [0.8453600406646729, 0.34813621640205383, 0.5610957145690918]\n",
      "epoch:  13860 training:  [0.6775760054588318, 0.3162758946418762, 0.606795072555542]\n",
      "epoch:  13870 training:  [0.8105399012565613, 3.201446533203125, 3.687675952911377]\n",
      "epoch:  13880 training:  [0.1401086151599884, 0.24555815756320953, 0.7278479337692261]\n",
      "epoch:  13890 training:  [0.2225380390882492, 0.703281044960022, 0.77164226770401]\n",
      "epoch:  13900 training:  [0.20368675887584686, 0.27849483489990234, 0.7589554786682129]\n",
      "epoch:  13910 training:  [0.31421446800231934, 0.21239642798900604, 0.7690041065216064]\n",
      "epoch:  13920 training:  [0.4452740550041199, 0.21778349578380585, 0.7913599014282227]\n",
      "epoch:  13930 training:  [0.26559770107269287, 0.19015082716941833, 0.7975537776947021]\n",
      "epoch:  13940 training:  [0.3491246700286865, 0.1949234902858734, 0.7478482723236084]\n",
      "epoch:  13950 training:  [0.2904026508331299, 0.2647412419319153, 0.7415813207626343]\n",
      "epoch:  13960 training:  [0.3324132263660431, 0.255084365606308, 0.7265265583992004]\n",
      "epoch:  13970 training:  [0.29206332564353943, 0.2879841923713684, 0.7221013903617859]\n",
      "epoch:  13980 training:  [0.27166908979415894, 0.176237553358078, 0.7189434766769409]\n",
      "epoch:  13990 training:  [0.24676986038684845, 0.1301945000886917, 0.6428069472312927]\n",
      "epoch:  14000 training:  [0.30479979515075684, 0.09878717362880707, 0.6171267032623291]\n",
      "epoch:  14010 training:  [0.31846123933792114, 0.09262996166944504, 0.6237226724624634]\n",
      "epoch:  14020 training:  [0.3209260106086731, 0.11203089356422424, 0.6064637899398804]\n",
      "epoch:  14030 training:  [0.26511701941490173, 0.13795337080955505, 0.5287067294120789]\n",
      "epoch:  14040 training:  [0.2374865710735321, 0.05630049854516983, 0.5558599233627319]\n",
      "epoch:  14050 training:  [0.27177318930625916, 0.02180364727973938, 0.5953629016876221]\n",
      "epoch:  14060 training:  [0.25383132696151733, 0.025151759386062622, 0.5862005949020386]\n",
      "epoch:  14070 training:  [0.23852528631687164, 0.03225530683994293, 0.603968620300293]\n",
      "epoch:  14080 training:  [0.1452050507068634, 0.0738183856010437, 0.5689956545829773]\n",
      "epoch:  14090 training:  [0.2677607238292694, 0.052096888422966, 0.5649725794792175]\n",
      "epoch:  14100 training:  [0.2818298935890198, 0.006568580865859985, 0.5408637523651123]\n",
      "epoch:  14110 training:  [0.5316640138626099, 0.1980084478855133, 0.4742200970649719]\n",
      "epoch:  14120 training:  [0.53798907995224, 0.2214244306087494, 0.4629324674606323]\n",
      "epoch:  14130 training:  [1.1336987018585205, 0.9121224880218506, 1.1162500381469727]\n",
      "epoch:  14140 training:  [0.7015869617462158, 0.2846525311470032, 0.46420523524284363]\n",
      "epoch:  14150 training:  [0.4731391668319702, 0.15534307062625885, 0.4532727599143982]\n",
      "epoch:  14160 training:  [0.4722708463668823, 0.16510172188282013, 0.4124497175216675]\n",
      "epoch:  14170 training:  [0.5879631638526917, 0.1897934079170227, 0.4045877456665039]\n",
      "epoch:  14180 training:  [0.5764338970184326, 0.24611389636993408, 0.4012683033943176]\n",
      "epoch:  14190 training:  [0.42572513222694397, 0.1254740208387375, 0.3593818247318268]\n",
      "epoch:  14200 training:  [0.30871307849884033, 0.08925187587738037, 0.34098219871520996]\n",
      "epoch:  14210 training:  [0.2499314397573471, 0.09071086347103119, 0.33285167813301086]\n",
      "epoch:  14220 training:  [0.20800647139549255, 0.08400247991085052, 0.3591192960739136]\n",
      "epoch:  14230 training:  [0.20823222398757935, 0.06243567168712616, 0.36707550287246704]\n",
      "epoch:  14240 training:  [0.23673951625823975, 0.0650855302810669, 0.35071271657943726]\n",
      "epoch:  14250 training:  [0.2343309223651886, 0.074504554271698, 0.38282275199890137]\n",
      "epoch:  14260 training:  [0.19063034653663635, 0.08864766359329224, 0.34216344356536865]\n",
      "epoch:  14270 training:  [0.14934563636779785, 0.07438328862190247, 0.36091238260269165]\n",
      "epoch:  14280 training:  [0.12160402536392212, 0.034085214138031006, 0.33209550380706787]\n",
      "epoch:  14290 training:  [0.09092837572097778, 0.011888176202774048, 0.33308130502700806]\n",
      "epoch:  14300 training:  [0.13422280550003052, 0.06258438527584076, 0.31212317943573]\n",
      "epoch:  14310 training:  [0.11977909505367279, 0.07789000868797302, 0.3234856426715851]\n",
      "epoch:  14320 training:  [0.22613272070884705, 0.1298755258321762, 0.27922219038009644]\n",
      "epoch:  14330 training:  [0.18705368041992188, 0.08143007755279541, 0.301491379737854]\n",
      "epoch:  14340 training:  [0.0996687263250351, 0.04379516839981079, 0.3153894245624542]\n",
      "epoch:  14350 training:  [0.10222357511520386, 0.029814641922712326, 0.3432796895503998]\n",
      "epoch:  14360 training:  [0.10066228359937668, 0.028676696121692657, 0.3235056698322296]\n",
      "epoch:  14370 training:  [0.18626344203948975, 0.12695631384849548, 0.31574007868766785]\n",
      "epoch:  14380 training:  [0.24387390911579132, 0.10078021138906479, 0.26819533109664917]\n",
      "epoch:  14390 training:  [0.3518551290035248, 0.14787879586219788, 0.2863214612007141]\n",
      "epoch:  14400 training:  [0.18672603368759155, 0.07651814818382263, 0.24049824476242065]\n",
      "epoch:  14410 training:  [0.20662686228752136, 0.06450814753770828, 0.2566683292388916]\n",
      "epoch:  14420 training:  [0.1661824882030487, 0.06706126034259796, 0.26173633337020874]\n",
      "epoch:  14430 training:  [0.26986879110336304, 0.05010806769132614, 0.240921288728714]\n",
      "epoch:  14440 training:  [0.2842835485935211, 0.04853789880871773, 0.24494922161102295]\n",
      "epoch:  14450 training:  [0.4215353727340698, 0.065494604408741, 0.27171653509140015]\n",
      "epoch:  14460 training:  [0.29555922746658325, 0.055311255156993866, 0.25458595156669617]\n",
      "epoch:  14470 training:  [0.2760738134384155, 0.04308696836233139, 0.23595315217971802]\n",
      "epoch:  14480 training:  [0.32378631830215454, 0.04579868167638779, 0.23061496019363403]\n",
      "epoch:  14490 training:  [0.3180225193500519, 0.05723277106881142, 0.2494732141494751]\n",
      "epoch:  14500 training:  [0.2808857262134552, 0.05189825966954231, 0.22556598484516144]\n",
      "epoch:  14510 training:  [0.28750765323638916, 0.040535200387239456, 0.21996517479419708]\n",
      "epoch:  14520 training:  [0.2350032776594162, 0.03723178803920746, 0.22143277525901794]\n",
      "epoch:  14530 training:  [0.17619238793849945, 0.049636200070381165, 0.23116442561149597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  14540 training:  [0.1071319729089737, 0.08432646840810776, 0.24572056531906128]\n",
      "epoch:  14550 training:  [0.16429737210273743, 0.03433641791343689, 0.24839527904987335]\n",
      "epoch:  14560 training:  [0.12415195256471634, 0.039758265018463135, 0.23262682557106018]\n",
      "epoch:  14570 training:  [0.1285201907157898, 0.03608466684818268, 0.22275635600090027]\n",
      "epoch:  14580 training:  [0.14039620757102966, 0.123603455722332, 0.21664366126060486]\n",
      "epoch:  14590 training:  [0.14630207419395447, 0.03625396639108658, 0.2198704183101654]\n",
      "epoch:  14600 training:  [0.11555606126785278, 0.06893341988325119, 0.213828444480896]\n",
      "epoch:  14610 training:  [0.13364142179489136, 0.07481120526790619, 0.22696927189826965]\n",
      "epoch:  14620 training:  [0.12627995014190674, 0.05328689143061638, 0.250166654586792]\n",
      "epoch:  14630 training:  [0.10851223766803741, 0.047565121203660965, 0.23509031534194946]\n",
      "epoch:  14640 training:  [0.10299244523048401, 0.06696799397468567, 0.2298501431941986]\n",
      "epoch:  14650 training:  [0.11951853334903717, 0.14506986737251282, 0.2580275237560272]\n",
      "epoch:  14660 training:  [0.09686332195997238, 0.09449110925197601, 0.2401610165834427]\n",
      "epoch:  14670 training:  [0.12723655998706818, 0.054420843720436096, 0.24710190296173096]\n",
      "epoch:  14680 training:  [0.15118426084518433, 0.037234265357255936, 0.2464466392993927]\n",
      "epoch:  14690 training:  [0.21983209252357483, 0.03766715154051781, 0.20594468712806702]\n",
      "epoch:  14700 training:  [0.13393613696098328, 0.057991743087768555, 0.21106481552124023]\n",
      "epoch:  14710 training:  [0.12241226434707642, 0.05548162758350372, 0.20534774661064148]\n",
      "epoch:  14720 training:  [0.13879747688770294, 0.05227777361869812, 0.21637633442878723]\n",
      "epoch:  14730 training:  [0.1429101973772049, 0.049650534987449646, 0.21927808225154877]\n",
      "epoch:  14740 training:  [0.2489209771156311, 0.2524128258228302, 0.36282476782798767]\n",
      "epoch:  14750 training:  [0.34499993920326233, 0.5808521509170532, 0.2405971735715866]\n",
      "epoch:  14760 training:  [0.33847880363464355, 0.49125394225120544, 0.20311632752418518]\n",
      "epoch:  14770 training:  [0.35391736030578613, 0.49513500928878784, 0.18163880705833435]\n",
      "epoch:  14780 training:  [0.3009953498840332, 0.33213743567466736, 0.17897695302963257]\n",
      "epoch:  14790 training:  [0.2325444221496582, 0.4362920820713043, 0.19982898235321045]\n",
      "epoch:  14800 training:  [0.23101937770843506, 0.4150150716304779, 0.19624295830726624]\n",
      "epoch:  14810 training:  [0.19433675706386566, 0.3846330940723419, 0.15185704827308655]\n",
      "epoch:  14820 training:  [0.18835973739624023, 0.3672667145729065, 0.1371387541294098]\n",
      "epoch:  14830 training:  [0.17733189463615417, 0.3595167398452759, 0.11794175207614899]\n",
      "epoch:  14840 training:  [0.1552225947380066, 0.31860780715942383, 0.11537852138280869]\n",
      "epoch:  14850 training:  [0.1334465891122818, 0.31874608993530273, 0.09878584742546082]\n",
      "epoch:  14860 training:  [0.12051389366388321, 0.28834986686706543, 0.08183952420949936]\n",
      "epoch:  14870 training:  [0.1469051092863083, 0.2045130580663681, 0.08567258715629578]\n",
      "epoch:  14880 training:  [0.1082431823015213, 0.24309608340263367, 0.05611855536699295]\n",
      "epoch:  14890 training:  [0.09693984687328339, 0.18255579471588135, 0.04474141076207161]\n",
      "epoch:  14900 training:  [0.09494619816541672, 0.14376655220985413, 0.0374377965927124]\n",
      "epoch:  14910 training:  [0.11458049714565277, 0.0867108479142189, 0.05420031398534775]\n",
      "epoch:  14920 training:  [0.09498316049575806, 0.15922772884368896, 0.0344574861228466]\n",
      "epoch:  14930 training:  [0.07527144998311996, 0.09968674927949905, 0.02960835210978985]\n",
      "epoch:  14940 training:  [0.0757371112704277, 0.1635807305574417, 0.026723504066467285]\n",
      "epoch:  14950 training:  [0.07921533286571503, 0.17928089201450348, 0.02951357513666153]\n",
      "epoch:  14960 training:  [0.05036085098981857, 0.19642430543899536, 0.03871201351284981]\n",
      "epoch:  14970 training:  [0.054651834070682526, 0.16470685601234436, 0.02279362455010414]\n",
      "epoch:  14980 training:  [0.06373769044876099, 0.13619720935821533, 0.03730882704257965]\n",
      "epoch:  14990 training:  [0.04453309252858162, 0.21792222559452057, 0.02645876631140709]\n",
      "epoch:  15000 training:  [0.1314631998538971, 0.34480905532836914, 0.17837122082710266]\n",
      "epoch:  15010 training:  [0.13526947796344757, 0.1619473695755005, 0.04037633538246155]\n",
      "epoch:  15020 training:  [0.14616180956363678, 0.24053025245666504, 0.044068168848752975]\n",
      "epoch:  15030 training:  [0.11005352437496185, 0.11563052237033844, 0.05517297238111496]\n",
      "epoch:  15040 training:  [0.10668438673019409, 0.12189758569002151, 0.03242187201976776]\n",
      "epoch:  15050 training:  [0.10538218915462494, 0.14503216743469238, 0.021622925996780396]\n",
      "epoch:  15060 training:  [0.09066763520240784, 0.1528591513633728, 0.041050996631383896]\n",
      "epoch:  15070 training:  [0.115721195936203, 0.11045898497104645, 0.012183777987957]\n",
      "epoch:  15080 training:  [0.1067899614572525, 0.08510202169418335, 0.00956571102142334]\n",
      "epoch:  15090 training:  [0.10200877487659454, 0.09089554846286774, 0.008672043681144714]\n",
      "epoch:  15100 training:  [0.11864418536424637, 0.07296137511730194, 0.014039825648069382]\n",
      "epoch:  15110 training:  [0.09928184747695923, 0.05544240027666092, 0.009392894804477692]\n",
      "epoch:  15120 training:  [0.07806478440761566, 0.08291320502758026, 0.02195819653570652]\n",
      "epoch:  15130 training:  [0.08792940527200699, 0.09758424013853073, -0.001440875232219696]\n",
      "epoch:  15140 training:  [0.10347025841474533, 0.07724195718765259, 0.018690742552280426]\n",
      "epoch:  15150 training:  [0.08185429871082306, 0.07922787219285965, 0.018136553466320038]\n",
      "epoch:  15160 training:  [0.10145747661590576, 0.04848893731832504, 0.044747572392225266]\n",
      "epoch:  15170 training:  [0.0921044647693634, 0.09684186428785324, 0.022798363119363785]\n",
      "epoch:  15180 training:  [0.09730207920074463, 0.06568318605422974, 0.009213745594024658]\n",
      "epoch:  15190 training:  [0.28713250160217285, 0.16930440068244934, 0.5115004777908325]\n",
      "epoch:  15200 training:  [0.22528967261314392, 0.08888708800077438, 0.03897382318973541]\n",
      "epoch:  15210 training:  [0.1851496696472168, 0.08913300186395645, 0.03268538415431976]\n",
      "epoch:  15220 training:  [0.1989225447177887, 0.07669462263584137, 0.03485039621591568]\n",
      "epoch:  15230 training:  [0.20099082589149475, 0.036839693784713745, 0.04685908555984497]\n",
      "epoch:  15240 training:  [0.16487839818000793, 0.04693887382745743, 0.0523737408220768]\n",
      "epoch:  15250 training:  [0.20240101218223572, 0.06684722006320953, 0.03903768211603165]\n",
      "epoch:  15260 training:  [0.16924096643924713, 0.07550141215324402, 0.02108842134475708]\n",
      "epoch:  15270 training:  [0.17641958594322205, 0.061702415347099304, 0.01419079303741455]\n",
      "epoch:  15280 training:  [0.16724520921707153, 0.05085308849811554, 0.0200166255235672]\n",
      "epoch:  15290 training:  [0.19337424635887146, 0.0504988357424736, 0.05987171083688736]\n",
      "epoch:  15300 training:  [0.17454729974269867, 0.05817615985870361, 0.02151493728160858]\n",
      "epoch:  15310 training:  [0.13355116546154022, 0.0448186881840229, 0.03127636760473251]\n",
      "epoch:  15320 training:  [0.14060121774673462, 0.054109036922454834, 0.005416218191385269]\n",
      "epoch:  15330 training:  [0.1196281835436821, 0.058897748589515686, 0.0046732425689697266]\n",
      "epoch:  15340 training:  [0.1465330719947815, 0.05434486269950867, 0.02281256951391697]\n",
      "epoch:  15350 training:  [0.14821569621562958, 0.032363880425691605, 0.04288734495639801]\n",
      "epoch:  15360 training:  [0.13917699456214905, 0.03421690687537193, 0.02032848820090294]\n",
      "epoch:  15370 training:  [0.14858508110046387, 0.03428512066602707, 0.005772754549980164]\n",
      "epoch:  15380 training:  [0.1232137531042099, 0.04499126225709915, 0.014849387109279633]\n",
      "epoch:  15390 training:  [0.14137732982635498, 0.04049530625343323, -0.020710758864879608]\n",
      "epoch:  15400 training:  [0.12611545622348785, 0.0407257005572319, 0.00942986086010933]\n",
      "epoch:  15410 training:  [0.11568750441074371, 0.04574476182460785, -0.011440999805927277]\n",
      "epoch:  15420 training:  [0.1317078173160553, 0.019390352070331573, 0.008010119199752808]\n",
      "epoch:  15430 training:  [0.09700534492731094, 0.0404798723757267, -0.000524129718542099]\n",
      "epoch:  15440 training:  [0.11223320662975311, 0.04764606058597565, 0.015468243509531021]\n",
      "epoch:  15450 training:  [0.10448750108480453, 0.04637999087572098, -0.018065571784973145]\n",
      "epoch:  15460 training:  [0.09028887748718262, 0.05300847068428993, 0.01929295063018799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  15470 training:  [0.3747072219848633, 0.27142179012298584, 0.6294237971305847]\n",
      "epoch:  15480 training:  [0.07851645350456238, 0.2981310188770294, 0.12660107016563416]\n",
      "epoch:  15490 training:  [0.19913369417190552, 0.7211155891418457, 0.48209524154663086]\n",
      "epoch:  15500 training:  [0.125849187374115, 0.40391814708709717, 0.24296411871910095]\n",
      "epoch:  15510 training:  [0.15380559861660004, 0.17943650484085083, 0.046559520065784454]\n",
      "epoch:  15520 training:  [0.13657821714878082, 0.1330501288175583, 0.0023761987686157227]\n",
      "epoch:  15530 training:  [0.11180196702480316, 0.12635289132595062, 0.024290598928928375]\n",
      "epoch:  15540 training:  [0.13134092092514038, 0.18467310070991516, 0.05760291963815689]\n",
      "epoch:  15550 training:  [0.13113103806972504, 0.1243838518857956, 0.0317947119474411]\n",
      "epoch:  15560 training:  [0.141945943236351, 0.10727758705615997, 0.04354492947459221]\n",
      "epoch:  15570 training:  [0.10520128905773163, 0.1268424242734909, 0.020882144570350647]\n",
      "epoch:  15580 training:  [0.10658957064151764, 0.10508957505226135, 0.01071460172533989]\n",
      "epoch:  15590 training:  [0.1387656331062317, 0.0692371129989624, 0.01271512359380722]\n",
      "epoch:  15600 training:  [0.10875491052865982, 0.10097329318523407, -0.001995585858821869]\n",
      "epoch:  15610 training:  [0.11683674901723862, 0.06454429775476456, 0.049033023416996]\n",
      "epoch:  15620 training:  [0.12574583292007446, 0.07774999737739563, 0.08692830801010132]\n",
      "epoch:  15630 training:  [0.1154620498418808, 0.12786969542503357, 0.052784476429224014]\n",
      "epoch:  15640 training:  [0.11258760094642639, 0.12705671787261963, 0.03965911641716957]\n",
      "epoch:  15650 training:  [0.12977051734924316, 0.04636315256357193, 0.046093977987766266]\n",
      "epoch:  15660 training:  [0.12232020497322083, 0.057424601167440414, 0.017766758799552917]\n",
      "epoch:  15670 training:  [0.12551113963127136, 0.05582131817936897, 0.016633290797472]\n",
      "epoch:  15680 training:  [0.13863711059093475, 0.04022349417209625, 0.015077807009220123]\n",
      "epoch:  15690 training:  [0.11742900311946869, 0.04120106250047684, 0.0030291900038719177]\n",
      "epoch:  15700 training:  [0.0953875333070755, 0.05475406348705292, 0.023313645273447037]\n",
      "epoch:  15710 training:  [0.09435990452766418, 0.22170841693878174, 0.04298068583011627]\n",
      "epoch:  15720 training:  [0.11585599929094315, 0.04146501421928406, 0.023013651371002197]\n",
      "epoch:  15730 training:  [0.1134529709815979, 0.05507155507802963, -0.013188391923904419]\n",
      "epoch:  15740 training:  [0.10313711315393448, 0.07845936715602875, 0.002813681960105896]\n",
      "epoch:  15750 training:  [0.09412455558776855, 0.18238379061222076, 0.04209517687559128]\n",
      "epoch:  15760 training:  [0.06769357621669769, 0.10919022560119629, 0.015771500766277313]\n",
      "epoch:  15770 training:  [0.09246334433555603, 0.0495607927441597, 0.07509277760982513]\n",
      "epoch:  15780 training:  [0.1087353304028511, 0.0834868922829628, -0.008728228509426117]\n",
      "epoch:  15790 training:  [0.11855338513851166, 0.10561435669660568, 0.01785331964492798]\n",
      "epoch:  15800 training:  [0.11870282143354416, 0.03872901201248169, 0.043571390211582184]\n",
      "epoch:  15810 training:  [0.10789158195257187, 0.06766372174024582, 0.019120126962661743]\n",
      "epoch:  15820 training:  [0.09599240869283676, 0.05545826628804207, 0.017381254583597183]\n",
      "epoch:  15830 training:  [0.09411028027534485, 0.07958970963954926, 0.02520207315683365]\n",
      "epoch:  15840 training:  [0.09880624711513519, 0.06940148770809174, 0.012729328125715256]\n",
      "epoch:  15850 training:  [0.0942036360502243, 0.06248549371957779, 0.014335911720991135]\n",
      "epoch:  15860 training:  [0.09264170378446579, 0.029790982604026794, 0.02716318517923355]\n",
      "epoch:  15870 training:  [0.0822259709239006, 0.04277355223894119, 0.024125486612319946]\n",
      "epoch:  15880 training:  [0.11011597514152527, 0.030847303569316864, 0.01856698840856552]\n",
      "epoch:  15890 training:  [0.19354578852653503, 0.07429899275302887, 0.0852918028831482]\n",
      "epoch:  15900 training:  [0.1475716531276703, 0.05930028855800629, 0.005018316209316254]\n",
      "epoch:  15910 training:  [0.08129804581403732, 0.07698221504688263, 0.05420461669564247]\n",
      "epoch:  15920 training:  [0.125563383102417, 0.030525170266628265, 0.0828944742679596]\n",
      "epoch:  15930 training:  [0.9513634443283081, 2.368452548980713, 11.690378189086914]\n",
      "epoch:  15940 training:  [0.9567941427230835, 2.072152614593506, 8.812456130981445]\n",
      "epoch:  15950 training:  [0.7425204515457153, 1.3280220031738281, 4.228353500366211]\n",
      "epoch:  15960 training:  [0.589451789855957, 0.649509072303772, 1.2653623819351196]\n",
      "epoch:  15970 training:  [0.45855873823165894, 0.3561093509197235, 0.3483394682407379]\n",
      "epoch:  15980 training:  [0.533065140247345, 0.38268429040908813, 0.23047342896461487]\n",
      "epoch:  15990 training:  [0.4869409203529358, 0.3117837905883789, 0.09465562552213669]\n",
      "epoch:  16000 training:  [0.4167648255825043, 0.22494202852249146, 0.03769705817103386]\n",
      "epoch:  16010 training:  [0.3888639211654663, 0.26422375440597534, 0.03723858296871185]\n",
      "epoch:  16020 training:  [0.37634795904159546, 0.26595255732536316, 0.049645185470581055]\n",
      "epoch:  16030 training:  [0.39715951681137085, 0.22560617327690125, 0.038177091628313065]\n",
      "epoch:  16040 training:  [0.34711751341819763, 0.2107318639755249, 0.02490384876728058]\n",
      "epoch:  16050 training:  [0.312435507774353, 0.20724447071552277, 0.02322867326438427]\n",
      "epoch:  16060 training:  [0.24423813819885254, 0.20965847373008728, 0.027137484401464462]\n",
      "epoch:  16070 training:  [0.22738054394721985, 0.1603151112794876, 0.02643989399075508]\n",
      "epoch:  16080 training:  [0.2121155560016632, 0.18076467514038086, 0.027502302080392838]\n",
      "epoch:  16090 training:  [0.1998281627893448, 0.15700049698352814, 0.031044840812683105]\n",
      "epoch:  16100 training:  [0.2867315411567688, 0.19807055592536926, 0.04620228707790375]\n",
      "epoch:  16110 training:  [0.240482896566391, 0.19032339751720428, 0.03604612872004509]\n",
      "epoch:  16120 training:  [0.193328857421875, 0.17949706315994263, 0.022273465991020203]\n",
      "epoch:  16130 training:  [0.16969498991966248, 0.17766894400119781, 0.008301343768835068]\n",
      "epoch:  16140 training:  [0.1490670144557953, 0.17116157710552216, 0.04082244634628296]\n",
      "epoch:  16150 training:  [0.13872738182544708, 0.17715531587600708, 0.021714508533477783]\n",
      "epoch:  16160 training:  [0.1250440925359726, 0.2005065381526947, 0.03791055455803871]\n",
      "epoch:  16170 training:  [0.11455734074115753, 0.17843502759933472, 0.0035436078906059265]\n",
      "epoch:  16180 training:  [0.09422880411148071, 0.1661299169063568, 0.025856900960206985]\n",
      "epoch:  16190 training:  [0.09757432341575623, 0.19178363680839539, 0.01776536926627159]\n",
      "epoch:  16200 training:  [0.10724861174821854, 0.1623854637145996, -0.0011635124683380127]\n",
      "epoch:  16210 training:  [0.10285863280296326, 0.16147448122501373, 0.008876610547304153]\n",
      "epoch:  16220 training:  [0.09232860803604126, 0.16601049900054932, 0.019043173640966415]\n",
      "epoch:  16230 training:  [0.08636808395385742, 0.16955049335956573, 0.023000476881861687]\n",
      "epoch:  16240 training:  [0.07957170903682709, 0.15469768643379211, 0.015670079737901688]\n",
      "epoch:  16250 training:  [0.05783824995160103, 0.17873966693878174, 0.007941201329231262]\n",
      "epoch:  16260 training:  [0.06111440807580948, 0.151615172624588, 0.01427273266017437]\n",
      "epoch:  16270 training:  [0.06934965401887894, 0.16566382348537445, 0.025018714368343353]\n",
      "epoch:  16280 training:  [0.06445866823196411, 0.1218828484416008, 0.00048570334911346436]\n",
      "epoch:  16290 training:  [0.06170222908258438, 0.13274268805980682, 0.02312253788113594]\n",
      "epoch:  16300 training:  [0.06247853487730026, 0.13733188807964325, 0.01798401027917862]\n",
      "epoch:  16310 training:  [0.07373075187206268, 0.1245155930519104, 0.0241251178085804]\n",
      "epoch:  16320 training:  [0.07294808328151703, 0.10937616974115372, 0.026803838089108467]\n",
      "epoch:  16330 training:  [0.06266942620277405, 0.14083583652973175, 0.027216045185923576]\n",
      "epoch:  16340 training:  [0.06383960694074631, 0.1318357139825821, 0.011585429310798645]\n",
      "epoch:  16350 training:  [0.03900863975286484, 0.1304352581501007, 0.01792944222688675]\n",
      "epoch:  16360 training:  [0.051663197576999664, 0.14359664916992188, 0.01581745222210884]\n",
      "epoch:  16370 training:  [0.05227439105510712, 0.139975443482399, 0.03141064569354057]\n",
      "epoch:  16380 training:  [0.048120953142642975, 0.12446929514408112, 0.022266898304224014]\n",
      "epoch:  16390 training:  [0.05606028437614441, 0.11023904383182526, 0.02643139287829399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  16400 training:  [0.05227595567703247, 0.12371111661195755, 0.02784419059753418]\n",
      "epoch:  16410 training:  [0.0556604340672493, 0.11212997883558273, 0.018577940762043]\n",
      "epoch:  16420 training:  [0.06203350052237511, 0.12892432510852814, 0.02492784708738327]\n",
      "epoch:  16430 training:  [0.04991636425256729, 0.12841089069843292, 0.017668467015028]\n",
      "epoch:  16440 training:  [0.04786491394042969, 0.11713629961013794, 0.01340075209736824]\n",
      "epoch:  16450 training:  [0.05634023994207382, 0.11666470021009445, 0.014608338475227356]\n",
      "epoch:  16460 training:  [0.0489199161529541, 0.11915792524814606, 0.01196671649813652]\n",
      "epoch:  16470 training:  [0.0411372184753418, 0.12501884996891022, 0.02397039532661438]\n",
      "epoch:  16480 training:  [0.05294078588485718, 0.11968865990638733, 0.02130740135908127]\n",
      "epoch:  16490 training:  [0.04950297623872757, 0.13584929704666138, 0.019918549805879593]\n",
      "epoch:  16500 training:  [0.047268666326999664, 0.1427176594734192, 0.01764826476573944]\n",
      "epoch:  16510 training:  [0.05054248496890068, 0.11560326814651489, 0.022845961153507233]\n",
      "epoch:  16520 training:  [0.04184139892458916, 0.15675082802772522, 0.03048788569867611]\n",
      "epoch:  16530 training:  [0.03528149053454399, 0.11617075651884079, 0.025542601943016052]\n",
      "epoch:  16540 training:  [0.05007246136665344, 0.11694061756134033, 0.022935274988412857]\n",
      "epoch:  16550 training:  [0.04089699685573578, 0.11812779307365417, 0.013243868947029114]\n",
      "epoch:  16560 training:  [0.04195791110396385, 0.10706526041030884, 0.01485445722937584]\n",
      "epoch:  16570 training:  [0.05301889032125473, 0.09255700558423996, 0.03882937878370285]\n",
      "epoch:  16580 training:  [0.09425387531518936, 0.8711111545562744, 1.3063533306121826]\n",
      "epoch:  16590 training:  [0.10282403230667114, 0.587995171546936, 0.7717767953872681]\n",
      "epoch:  16600 training:  [0.10560685396194458, 0.4329032897949219, 0.4907747507095337]\n",
      "epoch:  16610 training:  [0.09333711117506027, 0.38361120223999023, 0.29391711950302124]\n",
      "epoch:  16620 training:  [0.09652698785066605, 0.29130488634109497, 0.19586335122585297]\n",
      "epoch:  16630 training:  [0.08994854986667633, 0.2431928515434265, 0.16951468586921692]\n",
      "epoch:  16640 training:  [0.09194168448448181, 0.21833612024784088, 0.10952973365783691]\n",
      "epoch:  16650 training:  [0.07652567327022552, 0.19596849381923676, 0.09538449347019196]\n",
      "epoch:  16660 training:  [0.0625029057264328, 0.2656291723251343, 0.08705084025859833]\n",
      "epoch:  16670 training:  [0.06500358134508133, 0.21040566265583038, 0.060232896357774734]\n",
      "epoch:  16680 training:  [0.06910136342048645, 0.18037614226341248, 0.06054757535457611]\n",
      "epoch:  16690 training:  [0.06582948565483093, 0.21107977628707886, 0.04807751253247261]\n",
      "epoch:  16700 training:  [0.0651237741112709, 0.22906361520290375, 0.04992666095495224]\n",
      "epoch:  16710 training:  [0.050842102617025375, 0.20498022437095642, 0.04629284515976906]\n",
      "epoch:  16720 training:  [0.06262857466936111, 0.2089768499135971, 0.03320026397705078]\n",
      "epoch:  16730 training:  [0.05799512565135956, 0.16551417112350464, 0.043163202702999115]\n",
      "epoch:  16740 training:  [0.05026660114526749, 0.17984391748905182, 0.03796709328889847]\n",
      "epoch:  16750 training:  [0.05128959193825722, 0.16662858426570892, 0.03394404053688049]\n",
      "epoch:  16760 training:  [0.055909521877765656, 0.18404994904994965, 0.03136569261550903]\n",
      "epoch:  16770 training:  [0.052182216197252274, 0.16658282279968262, 0.03568074479699135]\n",
      "epoch:  16780 training:  [0.06079687923192978, 0.1456233263015747, 0.04061800241470337]\n",
      "epoch:  16790 training:  [0.05044465512037277, 0.14142952859401703, 0.028153812512755394]\n",
      "epoch:  16800 training:  [0.060980331152677536, 0.13258479535579681, 0.03708314150571823]\n",
      "epoch:  16810 training:  [0.051078103482723236, 0.1558300405740738, 0.03486408293247223]\n",
      "epoch:  16820 training:  [0.042513057589530945, 0.16165201365947723, 0.030984893441200256]\n",
      "epoch:  16830 training:  [0.05201202630996704, 0.12583929300308228, 0.05505504086613655]\n",
      "epoch:  16840 training:  [0.05013685300946236, 0.13238361477851868, 0.027713820338249207]\n",
      "epoch:  16850 training:  [0.052895158529281616, 0.12638549506664276, 0.030356058850884438]\n",
      "epoch:  16860 training:  [0.04057767242193222, 0.14319533109664917, 0.0205773264169693]\n",
      "epoch:  16870 training:  [0.05074310302734375, 0.12674275040626526, 0.03197817504405975]\n",
      "epoch:  16880 training:  [0.052774928510189056, 0.14255817234516144, 0.02385202795267105]\n",
      "epoch:  16890 training:  [0.05257437378168106, 0.13668648898601532, 0.03675267472863197]\n",
      "epoch:  16900 training:  [0.05535639449954033, 0.12223755568265915, 0.038250882178545]\n",
      "epoch:  16910 training:  [0.05537291616201401, 0.11707472801208496, 0.023090261965990067]\n",
      "epoch:  16920 training:  [0.052363745868206024, 0.10349712520837784, 0.02831549569964409]\n",
      "epoch:  16930 training:  [0.05119539052248001, 0.10466136783361435, 0.0241289921104908]\n",
      "epoch:  16940 training:  [0.047755166888237, 0.08966261148452759, 0.014093577861785889]\n",
      "epoch:  16950 training:  [0.05797911435365677, 0.10170213133096695, 0.03267914801836014]\n",
      "epoch:  16960 training:  [0.0626363754272461, 0.06332714110612869, 0.059944868087768555]\n",
      "epoch:  16970 training:  [0.06299807876348495, 0.09060639888048172, 0.043735332787036896]\n",
      "epoch:  16980 training:  [0.06498654186725616, 0.07270543277263641, 0.03344278037548065]\n",
      "epoch:  16990 training:  [0.06207042187452316, 0.07906022667884827, 0.029577014967799187]\n",
      "epoch:  17000 training:  [0.05411772057414055, 0.0951828584074974, 0.02855667844414711]\n",
      "epoch:  17010 training:  [0.041717126965522766, 0.06767184287309647, 0.023269101977348328]\n",
      "epoch:  17020 training:  [0.06894351541996002, 0.06712120026350021, 0.04805392026901245]\n",
      "epoch:  17030 training:  [0.05576859042048454, 0.06462763249874115, 0.039029765874147415]\n",
      "epoch:  17040 training:  [0.061635106801986694, 0.09588579833507538, 0.028043609112501144]\n",
      "epoch:  17050 training:  [0.056864455342292786, 0.07205361872911453, 0.032258156687021255]\n",
      "epoch:  17060 training:  [0.04747377336025238, 0.10914718359708786, 0.030583128333091736]\n",
      "epoch:  17070 training:  [0.06002630665898323, 0.08304089307785034, 0.019412562251091003]\n",
      "epoch:  17080 training:  [0.06255587190389633, 0.07460682839155197, 0.023129427805542946]\n",
      "epoch:  17090 training:  [0.0497962087392807, 0.07896031439304352, 0.024889152497053146]\n",
      "epoch:  17100 training:  [0.05331208556890488, 0.08128863573074341, 0.0251975916326046]\n",
      "epoch:  17110 training:  [0.05501971393823624, 0.061042703688144684, 0.02943197265267372]\n",
      "epoch:  17120 training:  [0.048839520663022995, 0.07091062515974045, 0.01866813562810421]\n",
      "epoch:  17130 training:  [0.055825650691986084, 0.06832313537597656, 0.01389160007238388]\n",
      "epoch:  17140 training:  [0.04700439050793648, 0.08655506372451782, 0.023053741082549095]\n",
      "epoch:  17150 training:  [0.0450495220720768, 0.0781548023223877, 0.014146817848086357]\n",
      "epoch:  17160 training:  [0.05063595250248909, 0.08143194764852524, 0.01907516084611416]\n",
      "epoch:  17170 training:  [0.05416125804185867, 0.07899957150220871, 0.027126256376504898]\n",
      "epoch:  17180 training:  [0.05954393371939659, 0.05583278834819794, 0.02590939961373806]\n",
      "epoch:  17190 training:  [0.04525286704301834, 0.06913141906261444, 0.016722261905670166]\n",
      "epoch:  17200 training:  [0.04714933782815933, 0.08855064958333969, 0.013509392738342285]\n",
      "epoch:  17210 training:  [0.05250277742743492, 0.06714280694723129, 0.025726208463311195]\n",
      "epoch:  17220 training:  [0.054436273872852325, 0.0665634348988533, 0.017692822962999344]\n",
      "epoch:  17230 training:  [0.030468782410025597, 0.25475215911865234, 0.0913320779800415]\n",
      "epoch:  17240 training:  [0.024963587522506714, 0.11469259858131409, 0.036024369299411774]\n",
      "epoch:  17250 training:  [0.038542602211236954, 0.0825900211930275, 0.019018692895770073]\n",
      "epoch:  17260 training:  [0.03936224803328514, 0.09526422619819641, 0.018916629254817963]\n",
      "epoch:  17270 training:  [0.03845041245222092, 0.14948636293411255, 0.04225657507777214]\n",
      "epoch:  17280 training:  [0.039997633546590805, 0.11174929141998291, 0.020887108519673347]\n",
      "epoch:  17290 training:  [0.047387029975652695, 0.0932396650314331, 0.019733808934688568]\n",
      "epoch:  17300 training:  [0.035696692764759064, 0.09801588952541351, 0.026652656495571136]\n",
      "epoch:  17310 training:  [0.038004979491233826, 0.09411517530679703, 0.018013691529631615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  17320 training:  [0.031131960451602936, 0.05191957950592041, 0.025376828387379646]\n",
      "epoch:  17330 training:  [0.043612681329250336, 0.05892264097929001, 0.02028524875640869]\n",
      "epoch:  17340 training:  [0.04671599715948105, 0.05712028592824936, 0.023989010602235794]\n",
      "epoch:  17350 training:  [0.04487836733460426, 0.06060806289315224, 0.03769192099571228]\n",
      "epoch:  17360 training:  [0.04848135635256767, 0.055820293724536896, 0.028824618086218834]\n",
      "epoch:  17370 training:  [0.053331080824136734, 0.05496525019407272, 0.02215355634689331]\n",
      "epoch:  17380 training:  [0.05557442456483841, 0.04874057695269585, 0.013872720301151276]\n",
      "epoch:  17390 training:  [0.0492052361369133, 0.045500706881284714, 0.030770648270845413]\n",
      "epoch:  17400 training:  [0.04558064788579941, 0.045476481318473816, 0.014075696468353271]\n",
      "epoch:  17410 training:  [0.040606673806905746, 0.0607142448425293, 0.018927760422229767]\n",
      "epoch:  17420 training:  [0.054520804435014725, 0.05661356821656227, 0.021604709327220917]\n",
      "epoch:  17430 training:  [0.049386534839868546, 0.05904800817370415, 0.03097750060260296]\n",
      "epoch:  17440 training:  [0.043494436889886856, 0.07355224341154099, 0.024192098528146744]\n",
      "epoch:  17450 training:  [0.0519053153693676, 0.054722316563129425, 0.008519796654582024]\n",
      "epoch:  17460 training:  [0.05454495549201965, 0.08563429862260818, 0.01776316948235035]\n",
      "epoch:  17470 training:  [0.03847964107990265, 0.13424813747406006, 0.02374030090868473]\n",
      "epoch:  17480 training:  [0.04610973224043846, 0.08903823047876358, 0.014801941812038422]\n",
      "epoch:  17490 training:  [0.04853058233857155, 0.07288428395986557, 0.022237226366996765]\n",
      "epoch:  17500 training:  [0.051515087485313416, 0.07286135852336884, 0.020561186596751213]\n",
      "epoch:  17510 training:  [0.05520118400454521, 0.06144705414772034, 0.027052586898207664]\n",
      "epoch:  17520 training:  [0.04487794265151024, 0.07519252598285675, 0.032755691558122635]\n",
      "epoch:  17530 training:  [0.058718785643577576, 0.05994848534464836, 0.026662753894925117]\n",
      "epoch:  17540 training:  [0.059807199984788895, 0.048234306275844574, 0.02685755304992199]\n",
      "epoch:  17550 training:  [0.047857288271188736, 0.08331076055765152, 0.02990429475903511]\n",
      "epoch:  17560 training:  [0.04695409908890724, 0.06599603593349457, 0.024277955293655396]\n",
      "epoch:  17570 training:  [0.04835813120007515, 0.05936669558286667, 0.0207762960344553]\n",
      "epoch:  17580 training:  [0.057012103497982025, 0.04797941446304321, 0.020108111202716827]\n",
      "epoch:  17590 training:  [0.051266901195049286, 0.05331891030073166, 0.02982853725552559]\n",
      "epoch:  17600 training:  [0.05434186011552811, 0.06742040067911148, -0.0029955394566059113]\n",
      "epoch:  17610 training:  [0.052832283079624176, 0.07274658977985382, 0.02363104559481144]\n",
      "epoch:  17620 training:  [0.05944732576608658, 0.0504012368619442, 0.027161190286278725]\n",
      "epoch:  17630 training:  [0.0511353462934494, 0.04018617793917656, 0.027162984013557434]\n",
      "epoch:  17640 training:  [0.0551011860370636, 0.05895634740591049, 0.019162792712450027]\n",
      "epoch:  17650 training:  [0.06200122460722923, 0.030579503625631332, 0.03453923389315605]\n",
      "epoch:  17660 training:  [0.05466926842927933, 0.042012110352516174, 0.02544751949608326]\n",
      "epoch:  17670 training:  [0.05871734395623207, 0.036384157836437225, 0.02152271941304207]\n",
      "epoch:  17680 training:  [0.05096280202269554, 0.044391930103302, 0.024946708232164383]\n",
      "epoch:  17690 training:  [0.04207293689250946, 0.042725108563899994, 0.028434645384550095]\n",
      "epoch:  17700 training:  [0.04953455552458763, 0.03647087514400482, 0.025165509432554245]\n",
      "epoch:  17710 training:  [0.059631384909152985, 0.043292127549648285, 0.010426178574562073]\n",
      "epoch:  17720 training:  [0.0683264285326004, 0.030681978911161423, 0.02084432728588581]\n",
      "epoch:  17730 training:  [0.05433420091867447, 0.04589135944843292, 0.013317706063389778]\n",
      "epoch:  17740 training:  [0.06346563994884491, 0.038826100528240204, 0.028970487415790558]\n",
      "epoch:  17750 training:  [0.050080806016922, 0.042449988424777985, 0.019549008458852768]\n",
      "epoch:  17760 training:  [0.057650480419397354, 0.053732678294181824, 0.02138293720781803]\n",
      "epoch:  17770 training:  [0.05321124196052551, 0.03079434297978878, 0.027723683044314384]\n",
      "epoch:  17780 training:  [0.061466217041015625, 0.05129486322402954, 0.0021875835955142975]\n",
      "epoch:  17790 training:  [0.05524513125419617, 0.04083097726106644, 0.0175706148147583]\n",
      "epoch:  17800 training:  [0.05506928265094757, 0.06457263976335526, 0.007556892931461334]\n",
      "epoch:  17810 training:  [0.05307100713253021, 0.04244838282465935, 0.014039216563105583]\n",
      "epoch:  17820 training:  [0.06458902359008789, 0.049697935581207275, 0.02879605069756508]\n",
      "epoch:  17830 training:  [0.06127646565437317, 0.0454518161714077, 0.016775231808423996]\n",
      "epoch:  17840 training:  [0.06355005502700806, 0.041698113083839417, 0.025146298110485077]\n",
      "epoch:  17850 training:  [0.04693325608968735, 0.054799459874629974, 0.01687643676996231]\n",
      "epoch:  17860 training:  [0.05442437529563904, 0.04731922596693039, 0.00939522497355938]\n",
      "epoch:  17870 training:  [0.04950150474905968, 0.0386991947889328, 0.02394947037100792]\n",
      "epoch:  17880 training:  [0.046943698078393936, 0.056112512946128845, 0.011754423379898071]\n",
      "epoch:  17890 training:  [0.05196277052164078, 0.04508068412542343, 0.013945985585451126]\n",
      "epoch:  17900 training:  [0.0417877621948719, 0.06822457164525986, 0.014298923313617706]\n",
      "epoch:  17910 training:  [0.05680080130696297, 0.03005225956439972, 0.017194954678416252]\n",
      "epoch:  17920 training:  [0.05486779287457466, 0.04114687442779541, 0.015547622926533222]\n",
      "epoch:  17930 training:  [0.05412094295024872, 0.04200339317321777, 0.02096547558903694]\n",
      "epoch:  17940 training:  [0.05512009561061859, 0.04057370871305466, 0.014199606142938137]\n",
      "epoch:  17950 training:  [0.04544440284371376, 0.03427344560623169, 0.013310503214597702]\n",
      "epoch:  17960 training:  [0.06661710143089294, 0.03509197756648064, 0.01799401082098484]\n",
      "epoch:  17970 training:  [0.05194739252328873, 0.04856546223163605, 0.00400630384683609]\n",
      "epoch:  17980 training:  [0.05554227903485298, 0.03673548996448517, 0.020125627517700195]\n",
      "epoch:  17990 training:  [0.08043228089809418, 0.03166873753070831, 0.015394002199172974]\n",
      "epoch:  18000 training:  [0.05069250985980034, 0.03800857812166214, 0.015124420635402203]\n",
      "epoch:  18010 training:  [0.05410054326057434, 0.044708553701639175, 0.008937172591686249]\n",
      "epoch:  18020 training:  [0.054590921849012375, 0.04198117554187775, 0.020300325006246567]\n",
      "epoch:  18030 training:  [0.06108764931559563, 0.033334799110889435, 0.018540602177381516]\n",
      "epoch:  18040 training:  [0.04531541094183922, 0.0497753769159317, 0.01404352206736803]\n",
      "epoch:  18050 training:  [0.05149927735328674, 0.03966779261827469, 0.01753396727144718]\n",
      "epoch:  18060 training:  [0.04865453392267227, 0.03869391605257988, 0.013962037861347198]\n",
      "epoch:  18070 training:  [0.056835733354091644, 0.043008364737033844, 0.016505025327205658]\n",
      "epoch:  18080 training:  [0.05334382876753807, 0.04046131297945976, 0.009308494627475739]\n",
      "epoch:  18090 training:  [0.04503973200917244, 0.048579879105091095, 0.018151167780160904]\n",
      "epoch:  18100 training:  [0.05873823165893555, 0.03685089200735092, 0.025130923837423325]\n",
      "epoch:  18110 training:  [0.048291146755218506, 0.03751353174448013, 0.0204353928565979]\n",
      "epoch:  18120 training:  [0.05590598285198212, 0.03827236220240593, 0.014648420736193657]\n",
      "epoch:  18130 training:  [0.05756952241063118, 0.031672365963459015, 0.025614652782678604]\n",
      "epoch:  18140 training:  [0.05423203110694885, 0.047024160623550415, 0.008988669142127037]\n",
      "epoch:  18150 training:  [0.04947676137089729, 0.05017952620983124, 0.01718129590153694]\n",
      "epoch:  18160 training:  [0.06189925968647003, 0.033046115189790726, 0.01379358395934105]\n",
      "epoch:  18170 training:  [0.05494728684425354, 0.04266352578997612, 0.02882825769484043]\n",
      "epoch:  18180 training:  [0.05411714315414429, 0.03671637922525406, 0.029283201321959496]\n",
      "epoch:  18190 training:  [0.05683731660246849, 0.033576589077711105, 0.026417650282382965]\n",
      "epoch:  18200 training:  [0.055831313133239746, 0.03699209913611412, 0.020095467567443848]\n",
      "epoch:  18210 training:  [0.06009122356772423, 0.043147776275873184, 0.020426291972398758]\n",
      "epoch:  18220 training:  [0.06364110857248306, 0.021758798509836197, 0.022792063653469086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  18230 training:  [0.05307544767856598, 0.02662789449095726, 0.018292594701051712]\n",
      "epoch:  18240 training:  [0.05781986936926842, 0.03712949901819229, 0.024060387164354324]\n",
      "epoch:  18250 training:  [0.07370837032794952, 0.03656449541449547, 0.016907939687371254]\n",
      "epoch:  18260 training:  [0.06343390792608261, 0.031839288771152496, 0.020905226469039917]\n",
      "epoch:  18270 training:  [0.053290169686079025, 0.03820333629846573, 0.01974428817629814]\n",
      "epoch:  18280 training:  [0.05604754388332367, 0.03882746025919914, 0.019379211589694023]\n",
      "epoch:  18290 training:  [0.06388583779335022, 0.025721659883856773, 0.017061306163668633]\n",
      "epoch:  18300 training:  [0.05861509591341019, 0.03855788707733154, 0.02465720847249031]\n",
      "epoch:  18310 training:  [0.06045969948172569, 0.017531704157590866, 0.025806263089179993]\n",
      "epoch:  18320 training:  [0.05900038778781891, 0.03574215620756149, 0.012416115030646324]\n",
      "epoch:  18330 training:  [0.05889410525560379, 0.0242107305675745, 0.041191790252923965]\n",
      "epoch:  18340 training:  [0.04283776879310608, 0.042094938457012177, 0.02487674355506897]\n",
      "epoch:  18350 training:  [0.06350673735141754, 0.0262574665248394, 0.028821133077144623]\n",
      "epoch:  18360 training:  [0.04783383384346962, 0.03745986148715019, 0.018307244405150414]\n",
      "epoch:  18370 training:  [0.04867266118526459, 0.04152071848511696, 0.01844773441553116]\n",
      "epoch:  18380 training:  [0.053815387189388275, 0.025057833641767502, 0.016395706683397293]\n",
      "epoch:  18390 training:  [0.04845733940601349, 0.03809498995542526, 0.01639728993177414]\n",
      "epoch:  18400 training:  [0.052944667637348175, 0.03712402284145355, 0.02130088582634926]\n",
      "epoch:  18410 training:  [0.054032102227211, 0.023478565737605095, 0.02092568203806877]\n",
      "epoch:  18420 training:  [0.055909331887960434, 0.02801908552646637, 0.02128911018371582]\n",
      "epoch:  18430 training:  [0.058830443769693375, 0.026617996394634247, 0.024806229397654533]\n",
      "epoch:  18440 training:  [0.05199561268091202, 0.03181788697838783, 0.02730761654675007]\n",
      "epoch:  18450 training:  [0.05983171612024307, 0.025929518043994904, 0.02315189316868782]\n",
      "epoch:  18460 training:  [0.044722963124513626, 0.028035074472427368, 0.02109789475798607]\n",
      "epoch:  18470 training:  [0.06775572896003723, 0.01976793259382248, 0.024011854082345963]\n",
      "epoch:  18480 training:  [0.054841648787260056, 0.0368620939552784, 0.021950770169496536]\n",
      "epoch:  18490 training:  [0.0681702122092247, 0.023110821843147278, 0.016140753403306007]\n",
      "epoch:  18500 training:  [0.05923972278833389, 0.029927238821983337, 0.020923197269439697]\n",
      "epoch:  18510 training:  [0.06541137397289276, 0.018817685544490814, 0.01487659476697445]\n",
      "epoch:  18520 training:  [0.04597283899784088, 0.03272882103919983, 0.02192235365509987]\n",
      "epoch:  18530 training:  [0.061230652034282684, 0.023298494517803192, 0.025185689330101013]\n",
      "epoch:  18540 training:  [0.06611764430999756, 0.01957979053258896, 0.008540925569832325]\n",
      "epoch:  18550 training:  [0.0669640600681305, 0.015836790204048157, 0.024013817310333252]\n",
      "epoch:  18560 training:  [0.05546741187572479, 0.016383182257413864, 0.024973195046186447]\n",
      "epoch:  18570 training:  [0.05856799706816673, 0.019611630588769913, 0.016585884615778923]\n",
      "epoch:  18580 training:  [0.05912843346595764, 0.019620053470134735, 0.01992219127714634]\n",
      "epoch:  18590 training:  [0.058111052960157394, 0.025547396391630173, 0.016180329024791718]\n",
      "epoch:  18600 training:  [0.05426936224102974, 0.026244379580020905, 0.019734932109713554]\n",
      "epoch:  18610 training:  [0.06517674028873444, 0.022416550666093826, 0.011989541351795197]\n",
      "epoch:  18620 training:  [0.05587080121040344, 0.023772936314344406, 0.018502606078982353]\n",
      "epoch:  18630 training:  [0.055471040308475494, 0.023798134177923203, 0.004398681223392487]\n",
      "epoch:  18640 training:  [0.05512257292866707, 0.0289120115339756, 0.014967109076678753]\n",
      "epoch:  18650 training:  [0.06309884786605835, 0.010016117244958878, 0.020561907440423965]\n",
      "epoch:  18660 training:  [0.05437947437167168, 0.018057918176054955, 0.01977689377963543]\n",
      "epoch:  18670 training:  [0.06116224825382233, 0.020715050399303436, 0.01696767471730709]\n",
      "epoch:  18680 training:  [0.05638153478503227, 0.022813808172941208, 0.023039907217025757]\n",
      "epoch:  18690 training:  [0.06251545250415802, 0.015940513461828232, 0.012511628679931164]\n",
      "epoch:  18700 training:  [0.05647875368595123, 0.018533965572714806, 0.023512180894613266]\n",
      "epoch:  18710 training:  [0.055664025247097015, 0.020211245864629745, 0.01910138875246048]\n",
      "epoch:  18720 training:  [0.05720173195004463, 0.017454486340284348, 0.02272065356373787]\n",
      "epoch:  18730 training:  [0.04999762028455734, 0.025224052369594574, 0.013617913238704205]\n",
      "epoch:  18740 training:  [0.048811402171850204, 0.039896152913570404, 0.022877413779497147]\n",
      "epoch:  18750 training:  [0.05707103759050369, 0.024736102670431137, 0.020579978823661804]\n",
      "epoch:  18760 training:  [0.05527999624609947, 0.020589493215084076, 0.031483329832553864]\n",
      "epoch:  18770 training:  [0.04573192447423935, 0.03569451719522476, 0.022296704351902008]\n",
      "epoch:  18780 training:  [0.03714379668235779, 0.028958678245544434, 0.013481494970619678]\n",
      "epoch:  18790 training:  [0.04898548126220703, 0.026360727846622467, 0.014903955161571503]\n",
      "epoch:  18800 training:  [0.04696720838546753, 0.036263927817344666, 0.010348537936806679]\n",
      "epoch:  18810 training:  [0.06051499396562576, 0.02200862392783165, 0.021893277764320374]\n",
      "epoch:  18820 training:  [0.044879961758852005, 0.017980311065912247, 0.018654413521289825]\n",
      "epoch:  18830 training:  [0.04730048030614853, 0.02521931380033493, 0.021787747740745544]\n",
      "epoch:  18840 training:  [0.04903469607234001, 0.03720870614051819, 0.017309268936514854]\n",
      "epoch:  18850 training:  [0.04999520629644394, 0.02850649505853653, 0.02063501439988613]\n",
      "epoch:  18860 training:  [0.03856068477034569, 0.032080426812171936, 0.019391458481550217]\n",
      "epoch:  18870 training:  [0.061676204204559326, 0.016853906214237213, 0.01936812326312065]\n",
      "epoch:  18880 training:  [0.05797863379120827, 0.009450562298297882, 0.025388460606336594]\n",
      "epoch:  18890 training:  [0.049925848841667175, 0.024331675842404366, 0.022159278392791748]\n",
      "epoch:  18900 training:  [0.05674212798476219, 0.023454148322343826, 0.021817337721586227]\n",
      "epoch:  18910 training:  [0.04701666906476021, 0.025717265903949738, 0.015953702852129936]\n",
      "epoch:  18920 training:  [0.07226868718862534, 0.08982350677251816, 0.08567886054515839]\n",
      "epoch:  18930 training:  [0.05750399827957153, 0.08182740211486816, 0.07494594156742096]\n",
      "epoch:  18940 training:  [0.07399222999811172, 0.02329527586698532, 0.03783847391605377]\n",
      "epoch:  18950 training:  [0.05825359746813774, 0.02233070880174637, 0.026412896811962128]\n",
      "epoch:  18960 training:  [0.050422921776771545, 0.054166171699762344, 0.015043742954730988]\n",
      "epoch:  18970 training:  [0.069105364382267, 0.022135570645332336, 0.0160197950899601]\n",
      "epoch:  18980 training:  [0.07004575431346893, 0.03012998029589653, 0.02837337926030159]\n",
      "epoch:  18990 training:  [0.10938143730163574, 0.0631464421749115, 0.04782499372959137]\n",
      "epoch:  19000 training:  [0.11649761348962784, 0.052382588386535645, 0.0496227890253067]\n",
      "epoch:  19010 training:  [0.059629544615745544, 0.3606730103492737, 0.44838595390319824]\n",
      "epoch:  19020 training:  [0.07892764359712601, 0.10428477078676224, 0.07307283580303192]\n",
      "epoch:  19030 training:  [0.0806635394692421, 0.07139696925878525, 0.06482009589672089]\n",
      "epoch:  19040 training:  [0.07732183486223221, 0.07182808220386505, 0.0525188148021698]\n",
      "epoch:  19050 training:  [0.07200206816196442, 0.07188351452350616, 0.04119831323623657]\n",
      "epoch:  19060 training:  [0.06982171535491943, 0.07234159111976624, 0.049962349236011505]\n",
      "epoch:  19070 training:  [0.07294780761003494, 0.06553402543067932, 0.05215640738606453]\n",
      "epoch:  19080 training:  [0.06731566041707993, 0.05657374486327171, 0.03974142670631409]\n",
      "epoch:  19090 training:  [0.0760725662112236, 0.045424897223711014, 0.032541342079639435]\n",
      "epoch:  19100 training:  [0.061351001262664795, 0.05809397250413895, 0.03718552365899086]\n",
      "epoch:  19110 training:  [0.059948064386844635, 0.05503302812576294, 0.03805020824074745]\n",
      "epoch:  19120 training:  [0.0652540922164917, 0.051310066133737564, 0.029549602419137955]\n",
      "epoch:  19130 training:  [0.0650874674320221, 0.045086897909641266, 0.023918472230434418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  19140 training:  [0.055627837777137756, 0.05345918983221054, 0.025074534118175507]\n",
      "epoch:  19150 training:  [0.07422847300767899, 0.032869089394807816, 0.035968273878097534]\n",
      "epoch:  19160 training:  [0.0671294778585434, 0.04063253104686737, 0.027992650866508484]\n",
      "epoch:  19170 training:  [0.05633245408535004, 0.061025671660900116, 0.05207977071404457]\n",
      "epoch:  19180 training:  [0.043544963002204895, 0.04482750967144966, 0.016855081543326378]\n",
      "epoch:  19190 training:  [0.04638803377747536, 0.04945966601371765, 0.016691338270902634]\n",
      "epoch:  19200 training:  [0.0679575502872467, 0.027004607021808624, 0.03843115642666817]\n",
      "epoch:  19210 training:  [0.0686725378036499, 0.02867666259407997, 0.021459383890032768]\n",
      "epoch:  19220 training:  [0.05946354940533638, 0.04282984137535095, 0.01717453822493553]\n",
      "epoch:  19230 training:  [0.055176664143800735, 0.034974370151758194, 0.024474019184708595]\n",
      "epoch:  19240 training:  [0.08459188044071198, 0.025400858372449875, 0.06530033051967621]\n",
      "epoch:  19250 training:  [0.06363170593976974, 0.034842025488615036, 0.024988126009702682]\n",
      "epoch:  19260 training:  [0.07119151949882507, 0.021163009107112885, 0.02960978075861931]\n",
      "epoch:  19270 training:  [0.782320499420166, 1.0547453165054321, 3.127899169921875]\n",
      "epoch:  19280 training:  [0.2939905524253845, 0.1496015191078186, 0.1340234875679016]\n",
      "epoch:  19290 training:  [0.2263394594192505, 0.08160461485385895, 0.10116473585367203]\n",
      "epoch:  19300 training:  [0.20207935571670532, 0.04803437739610672, 0.016765670850872993]\n",
      "epoch:  19310 training:  [0.18139684200286865, 0.05459430813789368, 0.026145007461309433]\n",
      "epoch:  19320 training:  [0.1828039586544037, 0.039536572992801666, 0.02355039492249489]\n",
      "epoch:  19330 training:  [0.1608162224292755, 0.04475979506969452, 0.02608945034444332]\n",
      "epoch:  19340 training:  [0.1675269454717636, 0.033372338861227036, 0.027417700737714767]\n",
      "epoch:  19350 training:  [0.15751752257347107, 0.027477476745843887, 0.02399708889424801]\n",
      "epoch:  19360 training:  [0.15039905905723572, 0.030974356457591057, 0.024404985830187798]\n",
      "epoch:  19370 training:  [0.1497393697500229, 0.029901845380663872, 0.027770359069108963]\n",
      "epoch:  19380 training:  [0.13776780664920807, 0.025894805788993835, 0.023538008332252502]\n",
      "epoch:  19390 training:  [0.13016483187675476, 0.025783570483326912, 0.037756361067295074]\n",
      "epoch:  19400 training:  [0.11131590604782104, 0.02081746608018875, 0.0178888700902462]\n",
      "epoch:  19410 training:  [0.11251608282327652, 0.016258886083960533, 0.023121315985918045]\n",
      "epoch:  19420 training:  [0.10384175926446915, 0.021683061495423317, 0.030589977279305458]\n",
      "epoch:  19430 training:  [0.1037285178899765, 0.027859482914209366, 0.025586426258087158]\n",
      "epoch:  19440 training:  [0.08098595589399338, 0.0283509511500597, 0.016393234953284264]\n",
      "epoch:  19450 training:  [0.09278764575719833, 0.022433612495660782, 0.03207322955131531]\n",
      "epoch:  19460 training:  [0.09747198224067688, 0.026715420186519623, 0.018120599910616875]\n",
      "epoch:  19470 training:  [0.07951298356056213, 0.03588432818651199, 0.025580689311027527]\n",
      "epoch:  19480 training:  [0.08563411980867386, 0.02702506259083748, 0.017182569950819016]\n",
      "epoch:  19490 training:  [0.0884769856929779, 0.02247590199112892, 0.013907143846154213]\n",
      "epoch:  19500 training:  [0.10666465014219284, 0.01527513936161995, 0.02048409730195999]\n",
      "epoch:  19510 training:  [0.08529118448495865, 0.02529286965727806, 0.01603490673005581]\n",
      "epoch:  19520 training:  [0.07129597663879395, 0.023743974044919014, 0.018486574292182922]\n",
      "epoch:  19530 training:  [0.07661467790603638, 0.018683845177292824, 0.026893526315689087]\n",
      "epoch:  19540 training:  [0.07518962025642395, 0.024564672261476517, 0.01729060709476471]\n",
      "epoch:  19550 training:  [0.08108395338058472, 0.02562432363629341, 0.02993311733007431]\n",
      "epoch:  19560 training:  [0.06245372071862221, 0.03064173460006714, 0.019344452768564224]\n",
      "epoch:  19570 training:  [0.0614437609910965, 0.021547513082623482, 0.019256817176938057]\n",
      "epoch:  19580 training:  [0.08124221116304398, 0.01493755355477333, 0.048701949417591095]\n",
      "epoch:  19590 training:  [0.0638842061161995, 0.022269606590270996, 0.0166276004165411]\n",
      "epoch:  19600 training:  [0.07684457302093506, 0.018036965280771255, 0.024263259023427963]\n",
      "epoch:  19610 training:  [0.06024201959371567, 0.02698780968785286, 0.01851578801870346]\n",
      "epoch:  19620 training:  [0.06487517058849335, 0.01918666437268257, 0.02083313651382923]\n",
      "epoch:  19630 training:  [0.08143438398838043, 0.017246168106794357, 0.025288473814725876]\n",
      "epoch:  19640 training:  [0.06742460280656815, 0.019421672448515892, 0.018990829586982727]\n",
      "epoch:  19650 training:  [0.06425776332616806, 0.026272878050804138, 0.020802117884159088]\n",
      "epoch:  19660 training:  [0.05821383371949196, 0.02310178615152836, 0.022060509771108627]\n",
      "epoch:  19670 training:  [0.0568646602332592, 0.03095298819243908, 0.020084645599126816]\n",
      "epoch:  19680 training:  [0.040886152535676956, 0.04868064820766449, 0.030108949169516563]\n",
      "epoch:  19690 training:  [0.058374203741550446, 0.029233934357762337, 0.015945635735988617]\n",
      "epoch:  19700 training:  [0.046174291521310806, 0.06955242902040482, 0.06394807249307632]\n",
      "epoch:  19710 training:  [0.05146702378988266, 0.03765014559030533, 0.020969580858945847]\n",
      "epoch:  19720 training:  [0.060321144759655, 0.02683194726705551, 0.016097914427518845]\n",
      "epoch:  19730 training:  [0.06901179254055023, 0.026686836034059525, 0.015417139045894146]\n",
      "epoch:  19740 training:  [0.06622763723134995, 0.030373012647032738, 0.013562269508838654]\n",
      "epoch:  19750 training:  [0.04331807419657707, 0.039856284856796265, 0.017156314104795456]\n",
      "epoch:  19760 training:  [0.05239184945821762, 0.034925371408462524, 0.023059580475091934]\n",
      "epoch:  19770 training:  [0.053720954805612564, 0.018460430204868317, 0.013217082247138023]\n",
      "epoch:  19780 training:  [0.058211591094732285, 0.02441883087158203, 0.01277843490242958]\n",
      "epoch:  19790 training:  [0.05277818813920021, 0.021347368136048317, 0.016358504071831703]\n",
      "epoch:  19800 training:  [0.07186642289161682, 0.021301832050085068, 0.020741386339068413]\n",
      "epoch:  19810 training:  [0.0489041842520237, 0.03641578555107117, 0.026430580765008926]\n",
      "epoch:  19820 training:  [0.058002740144729614, 0.019255295395851135, 0.01759057492017746]\n",
      "epoch:  19830 training:  [0.06689003109931946, 0.013943323865532875, 0.01966622844338417]\n",
      "epoch:  19840 training:  [0.0706300288438797, 0.01376291736960411, 0.023351673036813736]\n",
      "epoch:  19850 training:  [0.06628773361444473, 0.008317116647958755, 0.027985386550426483]\n",
      "epoch:  19860 training:  [0.07669887691736221, 0.011528294533491135, 0.03851248323917389]\n",
      "epoch:  19870 training:  [0.06529971957206726, 0.018741987645626068, 0.021165572106838226]\n",
      "epoch:  19880 training:  [0.05686314404010773, 0.01554509624838829, 0.01789500005543232]\n",
      "epoch:  19890 training:  [0.09214504808187485, 0.011128716170787811, 0.06380405277013779]\n",
      "epoch:  19900 training:  [0.05677156150341034, 0.01618029922246933, 0.022182384505867958]\n",
      "epoch:  19910 training:  [0.03639490529894829, 0.04360383376479149, 0.02873879112303257]\n",
      "epoch:  19920 training:  [0.04848596081137657, 0.028301112353801727, 0.01759263500571251]\n",
      "epoch:  19930 training:  [0.04604785889387131, 0.023972216993570328, 0.01596691459417343]\n",
      "epoch:  19940 training:  [0.04206359386444092, 0.03897640109062195, 0.022100787609815598]\n",
      "epoch:  19950 training:  [0.04839726537466049, 0.028891004621982574, 0.0047461166977882385]\n",
      "epoch:  19960 training:  [0.03168253228068352, 0.05421143397688866, 0.02484503574669361]\n",
      "epoch:  19970 training:  [0.06232810392975807, 0.014084193855524063, 0.027882808819413185]\n",
      "epoch:  19980 training:  [0.05886188894510269, 0.013300005346536636, 0.013178210705518723]\n",
      "epoch:  19990 training:  [0.0602157860994339, 0.00896955281496048, 0.021040108054876328]\n"
     ]
    }
   ],
   "source": [
    "netG,AggregateData = Train(train_data,[10,20,35],2,10,4,256,step_size=0.03,n_epochs=20000,n_critic=3,lr=0.0003,Seed=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0570, device='cuda:0', grad_fn=<SelectBackward>) tensor(0.0148, device='cuda:0', grad_fn=<SelectBackward>) tensor(0.0217, device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "setup_seed(435)\n",
    "\n",
    "AggregateData = [torch.tensor(ele,dtype=torch.float32,requires_grad = True,device=\"cuda\").t() for ele in train_data]\n",
    "\n",
    "path = netG(AggregateData[0],AggregateData[0].shape[0],35)\n",
    "\n",
    "G2 = path[:,10,:]\n",
    "G4 = path[:,20,:]\n",
    "G7 = path[:,35,:]\n",
    "\n",
    "\n",
    "####### The Sinkhorn distance between predicted and observed distribution at D2,D4 and D7\n",
    "print(a(G2,AggregateData[1]),a(G4,AggregateData[2]),a(G7,AggregateData[3]))\n",
    "\n",
    "T0 = AggregateData[0].detach().cpu().numpy()\n",
    "T2 = AggregateData[1].detach().cpu().numpy()\n",
    "T4 = AggregateData[2].detach().cpu().numpy()\n",
    "T7 = AggregateData[3].detach().cpu().numpy()\n",
    "\n",
    "G2 = G2.detach().cpu().numpy()\n",
    "G4 = G4.detach().cpu().numpy()\n",
    "G7 = G7.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
