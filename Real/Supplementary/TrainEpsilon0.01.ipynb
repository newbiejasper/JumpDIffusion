{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 4000.1233\n",
      "total residual 138.7959\n",
      "Iteration 200:\n",
      "fit residual 123.6643\n",
      "total residual 31.8464\n",
      "Iteration 400:\n",
      "fit residual 21.7894\n",
      "total residual 12.6115\n",
      "Iteration 500:\n",
      "fit residual 4.3442\n",
      "total residual 8.6686\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 2257.8358\n",
      "total residual 58.2265\n",
      "Iteration 200:\n",
      "fit residual 56.8198\n",
      "total residual 4.0447\n",
      "Iteration 400:\n",
      "fit residual 3.4777\n",
      "total residual 0.6841\n",
      "Iteration 500:\n",
      "fit residual 0.3744\n",
      "total residual 0.3185\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 2925.3664\n",
      "total residual 80.2657\n",
      "Iteration 200:\n",
      "fit residual 78.1148\n",
      "total residual 6.1559\n",
      "Iteration 400:\n",
      "fit residual 5.1661\n",
      "total residual 1.1917\n",
      "Iteration 500:\n",
      "fit residual 0.6183\n",
      "total residual 0.5917\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 3719.0401\n",
      "total residual 121.8387\n",
      "Iteration 200:\n",
      "fit residual 113.1429\n",
      "total residual 20.967\n",
      "Iteration 400:\n",
      "fit residual 15.2785\n",
      "total residual 7.12\n",
      "Iteration 500:\n",
      "fit residual 2.7305\n",
      "total residual 4.5793\n",
      "training error:  8.587173461914062  and  27.765321731567383  and  84.18779754638672\n",
      "total error:  120.54029273986816\n",
      "training error:  5.670460224151611  and  11.908557891845703  and  27.10552215576172\n",
      "total error:  44.68454027175903\n",
      "training error:  5.933959007263184  and  7.382776737213135  and  10.206132888793945\n",
      "total error:  23.522868633270264\n",
      "training error:  5.0851593017578125  and  5.191745758056641  and  5.954068183898926\n",
      "total error:  16.23097324371338\n",
      "training error:  4.116710662841797  and  3.892242908477783  and  4.3786725997924805\n",
      "total error:  12.38762617111206\n",
      "training error:  3.6861140727996826  and  3.535362482070923  and  3.3410959243774414\n",
      "total error:  10.562572479248047\n",
      "training error:  3.398488998413086  and  3.525378704071045  and  2.6051602363586426\n",
      "total error:  9.529027938842773\n",
      "training error:  3.3774290084838867  and  3.1974360942840576  and  3.068039894104004\n",
      "total error:  9.642904996871948\n",
      "training error:  3.2602148056030273  and  3.2701783180236816  and  2.2046022415161133\n",
      "total error:  8.734995365142822\n",
      "training error:  2.95127534866333  and  3.284115791320801  and  2.7375779151916504\n",
      "total error:  8.972969055175781\n",
      "training error:  3.0271623134613037  and  3.7035207748413086  and  2.4839296340942383\n",
      "total error:  9.21461272239685\n",
      "training error:  3.008803367614746  and  3.055628776550293  and  2.346850872039795\n",
      "total error:  8.411283016204834\n",
      "training error:  2.663553237915039  and  2.9421401023864746  and  2.4553022384643555\n",
      "total error:  8.06099557876587\n",
      "training error:  2.5933046340942383  and  2.800844669342041  and  2.3582491874694824\n",
      "total error:  7.752398490905762\n",
      "training error:  2.47603440284729  and  3.362069845199585  and  2.082162857055664\n",
      "total error:  7.920267105102539\n",
      "training error:  2.4638986587524414  and  3.3296010494232178  and  3.3278231620788574\n",
      "total error:  9.121322870254517\n",
      "training error:  2.485114574432373  and  2.8886756896972656  and  2.9219398498535156\n",
      "total error:  8.295730113983154\n",
      "training error:  2.438775062561035  and  2.7709646224975586  and  2.1305699348449707\n",
      "total error:  7.3403096199035645\n",
      "training error:  2.3807311058044434  and  2.9450740814208984  and  1.9670639038085938\n",
      "total error:  7.2928690910339355\n",
      "training error:  2.4321045875549316  and  2.7253355979919434  and  1.8252148628234863\n",
      "total error:  6.982655048370361\n",
      "training error:  2.3964157104492188  and  2.6958656311035156  and  1.9571919441223145\n",
      "total error:  7.049473285675049\n",
      "training error:  2.442992925643921  and  3.5027432441711426  and  1.9674310684204102\n",
      "total error:  7.913167238235474\n",
      "training error:  2.1721644401550293  and  2.263011932373047  and  1.8402434587478638\n",
      "total error:  6.27541983127594\n",
      "training error:  2.198634624481201  and  2.497605085372925  and  1.7788105010986328\n",
      "total error:  6.475050210952759\n",
      "training error:  2.0701675415039062  and  2.574648141860962  and  1.8360222578048706\n",
      "total error:  6.480837941169739\n",
      "training error:  2.1367411613464355  and  2.707289218902588  and  2.448406934738159\n",
      "total error:  7.292437314987183\n",
      "training error:  2.0921449661254883  and  3.6973204612731934  and  2.1602749824523926\n",
      "total error:  7.949740409851074\n",
      "training error:  2.0561907291412354  and  2.4915518760681152  and  1.6854826211929321\n",
      "total error:  6.233225226402283\n",
      "training error:  1.9351170063018799  and  2.3505935668945312  and  1.9820681810379028\n",
      "total error:  6.267778754234314\n",
      "training error:  2.110994577407837  and  3.2301037311553955  and  1.7278308868408203\n",
      "total error:  7.068929195404053\n",
      "training error:  2.1447038650512695  and  3.7421350479125977  and  2.0015430450439453\n",
      "total error:  7.8883819580078125\n",
      "training error:  2.005964756011963  and  2.5582175254821777  and  2.158479690551758\n",
      "total error:  6.722661972045898\n",
      "training error:  1.9934660196304321  and  2.3279261589050293  and  2.0676846504211426\n",
      "total error:  6.389076828956604\n",
      "training error:  1.898629903793335  and  2.182401180267334  and  1.703859806060791\n",
      "total error:  5.78489089012146\n",
      "training error:  1.9580620527267456  and  2.333465576171875  and  1.8222887516021729\n",
      "total error:  6.1138163805007935\n",
      "training error:  1.9747251272201538  and  2.402690887451172  and  1.965505838394165\n",
      "total error:  6.342921853065491\n",
      "training error:  1.9776585102081299  and  2.673539400100708  and  1.7039053440093994\n",
      "total error:  6.355103254318237\n",
      "training error:  2.0681092739105225  and  3.512606620788574  and  1.8154139518737793\n",
      "total error:  7.396129846572876\n",
      "training error:  1.9269399642944336  and  2.1578774452209473  and  1.6528518199920654\n",
      "total error:  5.737669229507446\n",
      "training error:  1.8166025876998901  and  2.570108413696289  and  2.156444549560547\n",
      "total error:  6.543155550956726\n",
      "training error:  1.8498815298080444  and  3.0305886268615723  and  2.4734046459198\n",
      "total error:  7.3538748025894165\n",
      "training error:  1.8649117946624756  and  2.2154510021209717  and  1.6376053094863892\n",
      "total error:  5.717968106269836\n",
      "training error:  2.0737619400024414  and  5.410475730895996  and  1.9658839702606201\n",
      "total error:  9.450121641159058\n",
      "training error:  1.8070553541183472  and  3.091993808746338  and  2.257509708404541\n",
      "total error:  7.156558871269226\n",
      "training error:  1.8830854892730713  and  2.6972532272338867  and  1.5824002027511597\n",
      "total error:  6.162738919258118\n",
      "training error:  1.9568817615509033  and  2.866048812866211  and  2.024866819381714\n",
      "total error:  6.847797393798828\n",
      "training error:  1.8845680952072144  and  3.0406241416931152  and  2.1744325160980225\n",
      "total error:  7.099624752998352\n",
      "training error:  1.969704031944275  and  3.8094050884246826  and  1.6130908727645874\n",
      "total error:  7.392199993133545\n",
      "training error:  1.6968002319335938  and  3.2782154083251953  and  2.5042316913604736\n",
      "total error:  7.479247331619263\n",
      "training error:  1.8450123071670532  and  3.451448440551758  and  1.668371319770813\n",
      "total error:  6.964832067489624\n",
      "training error:  1.7892229557037354  and  3.221364974975586  and  1.9497194290161133\n",
      "total error:  6.960307359695435\n",
      "training error:  1.8113456964492798  and  3.091668128967285  and  1.6867444515228271\n",
      "total error:  6.589758276939392\n",
      "training error:  1.7906546592712402  and  3.0653574466705322  and  2.203677177429199\n",
      "total error:  7.059689283370972\n",
      "training error:  2.076974868774414  and  3.934206962585449  and  1.6620540618896484\n",
      "total error:  7.673235893249512\n",
      "training error:  1.7748572826385498  and  3.3179399967193604  and  2.6127188205718994\n",
      "total error:  7.70551609992981\n",
      "training error:  1.9038797616958618  and  4.392122745513916  and  1.7139344215393066\n",
      "total error:  8.009936928749084\n",
      "training error:  1.7082087993621826  and  3.5562450885772705  and  2.055601119995117\n",
      "total error:  7.32005500793457\n",
      "training error:  1.8751556873321533  and  3.7482314109802246  and  1.9445945024490356\n",
      "total error:  7.567981600761414\n",
      "training error:  1.7432246208190918  and  3.193182945251465  and  2.0918076038360596\n",
      "total error:  7.028215169906616\n",
      "training error:  1.7284455299377441  and  2.6663925647735596  and  2.162280797958374\n",
      "total error:  6.557118892669678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.6754486560821533  and  2.153059959411621  and  1.6871862411499023\n",
      "total error:  5.515694856643677\n",
      "training error:  1.6624603271484375  and  3.046994209289551  and  2.043606996536255\n",
      "total error:  6.753061532974243\n",
      "training error:  1.6892871856689453  and  3.1002635955810547  and  1.899970531463623\n",
      "total error:  6.689521312713623\n",
      "training error:  1.6894574165344238  and  2.7729454040527344  and  2.2279887199401855\n",
      "total error:  6.690391540527344\n",
      "training error:  1.7647335529327393  and  2.6678004264831543  and  1.770745873451233\n",
      "total error:  6.2032798528671265\n",
      "training error:  1.6796921491622925  and  2.9960856437683105  and  1.8817791938781738\n",
      "total error:  6.557556986808777\n",
      "training error:  1.714460849761963  and  3.060393810272217  and  1.6637136936187744\n",
      "total error:  6.438568353652954\n",
      "training error:  1.6232202053070068  and  2.6153311729431152  and  1.8693288564682007\n",
      "total error:  6.107880234718323\n",
      "training error:  1.662695050239563  and  2.477398157119751  and  1.930762529373169\n",
      "total error:  6.070855736732483\n",
      "training error:  1.6331413984298706  and  2.1720173358917236  and  1.7361643314361572\n",
      "total error:  5.5413230657577515\n",
      "training error:  1.5518701076507568  and  2.391794204711914  and  1.981144905090332\n",
      "total error:  5.924809217453003\n",
      "training error:  1.7442636489868164  and  3.0402464866638184  and  1.6085689067840576\n",
      "total error:  6.393079042434692\n",
      "training error:  1.5825297832489014  and  2.742715835571289  and  2.2731244564056396\n",
      "total error:  6.59837007522583\n",
      "training error:  1.798761248588562  and  2.7749032974243164  and  2.090932846069336\n",
      "total error:  6.664597392082214\n",
      "training error:  1.5968472957611084  and  2.185849666595459  and  1.7520034313201904\n",
      "total error:  5.534700393676758\n",
      "training error:  1.5603641271591187  and  2.597590684890747  and  1.880540132522583\n",
      "total error:  6.038494944572449\n",
      "training error:  1.5982590913772583  and  3.073848009109497  and  1.5701032876968384\n",
      "total error:  6.242210388183594\n",
      "training error:  1.525631070137024  and  3.1478166580200195  and  2.1741647720336914\n",
      "total error:  6.847612500190735\n",
      "training error:  1.7431265115737915  and  4.397163391113281  and  1.6985585689544678\n",
      "total error:  7.8388484716415405\n",
      "training error:  1.567482352256775  and  3.6597559452056885  and  2.6145739555358887\n",
      "total error:  7.841812252998352\n",
      "training error:  1.7107136249542236  and  4.452519416809082  and  1.747908592224121\n",
      "total error:  7.911141633987427\n",
      "training error:  1.493664026260376  and  2.3462612628936768  and  1.6769092082977295\n",
      "total error:  5.516834497451782\n",
      "training error:  1.4888967275619507  and  2.245084524154663  and  1.6586995124816895\n",
      "total error:  5.392680764198303\n",
      "training error:  1.6911873817443848  and  4.750357627868652  and  1.7170543670654297\n",
      "total error:  8.158599376678467\n",
      "training error:  1.4776253700256348  and  3.1684844493865967  and  1.7555210590362549\n",
      "total error:  6.401630878448486\n",
      "training error:  1.5623574256896973  and  2.2912964820861816  and  1.7798054218292236\n",
      "total error:  5.6334593296051025\n",
      "training error:  1.5914013385772705  and  2.7707746028900146  and  1.9288032054901123\n",
      "total error:  6.2909791469573975\n",
      "training error:  1.511950135231018  and  2.491069793701172  and  1.767524003982544\n",
      "total error:  5.770543932914734\n",
      "training error:  1.4952750205993652  and  1.9602611064910889  and  1.54073166847229\n",
      "total error:  4.996267795562744\n",
      "training error:  1.523939609527588  and  3.263610363006592  and  1.599355697631836\n",
      "total error:  6.386905670166016\n",
      "training error:  1.471798300743103  and  2.3915486335754395  and  1.7540090084075928\n",
      "total error:  5.617355942726135\n",
      "training error:  1.4339656829833984  and  2.0672965049743652  and  1.5810976028442383\n",
      "total error:  5.082359790802002\n",
      "training error:  1.650280237197876  and  4.024691104888916  and  1.8614811897277832\n",
      "total error:  7.536452531814575\n",
      "training error:  1.492561936378479  and  2.3797810077667236  and  1.9700270891189575\n",
      "total error:  5.84237003326416\n",
      "training error:  1.4831929206848145  and  2.858707904815674  and  1.8413774967193604\n",
      "total error:  6.183278322219849\n",
      "training error:  1.4888432025909424  and  2.653040647506714  and  1.5344148874282837\n",
      "total error:  5.67629873752594\n",
      "training error:  1.457089900970459  and  2.3240582942962646  and  1.66876220703125\n",
      "total error:  5.449910402297974\n",
      "training error:  1.422785997390747  and  2.4025063514709473  and  1.5757957696914673\n",
      "total error:  5.401088118553162\n",
      "training error:  1.471550703048706  and  1.8491584062576294  and  1.7378597259521484\n",
      "total error:  5.058568835258484\n",
      "training error:  1.5927422046661377  and  3.1771528720855713  and  1.7019755840301514\n",
      "total error:  6.47187066078186\n",
      "training error:  1.3701180219650269  and  2.1474251747131348  and  1.9378950595855713\n",
      "total error:  5.455438256263733\n",
      "training error:  1.4142425060272217  and  2.0760340690612793  and  1.7811105251312256\n",
      "total error:  5.271387100219727\n",
      "training error:  1.3883774280548096  and  2.045483112335205  and  1.6153295040130615\n",
      "total error:  5.049190044403076\n",
      "training error:  1.4089038372039795  and  2.730238437652588  and  1.6239203214645386\n",
      "total error:  5.763062596321106\n",
      "training error:  1.5316344499588013  and  3.3425233364105225  and  1.5598971843719482\n",
      "total error:  6.434054970741272\n",
      "training error:  1.4240453243255615  and  2.7426743507385254  and  1.9013160467147827\n",
      "total error:  6.06803572177887\n",
      "training error:  1.4261956214904785  and  2.923325300216675  and  2.1146578788757324\n",
      "total error:  6.464178800582886\n",
      "training error:  1.4600555896759033  and  2.200049877166748  and  1.8036869764328003\n",
      "total error:  5.463792443275452\n",
      "training error:  1.4981956481933594  and  2.367133140563965  and  1.498945713043213\n",
      "total error:  5.364274501800537\n",
      "training error:  1.379263162612915  and  3.181023597717285  and  1.8451138734817505\n",
      "total error:  6.405400633811951\n",
      "training error:  1.4471323490142822  and  2.3179335594177246  and  1.5653080940246582\n",
      "total error:  5.330374002456665\n",
      "training error:  1.430317997932434  and  2.645150661468506  and  1.605633020401001\n",
      "total error:  5.681101679801941\n",
      "training error:  1.459933876991272  and  3.5501327514648438  and  2.6650643348693848\n",
      "total error:  7.6751309633255005\n",
      "training error:  1.4190006256103516  and  2.082676887512207  and  1.6008517742156982\n",
      "total error:  5.102529287338257\n",
      "training error:  1.4037909507751465  and  2.5709831714630127  and  1.5175747871398926\n",
      "total error:  5.492348909378052\n",
      "training error:  1.3676846027374268  and  2.190854072570801  and  1.603428602218628\n",
      "total error:  5.1619672775268555\n",
      "training error:  1.3938618898391724  and  2.0400702953338623  and  1.6339250802993774\n",
      "total error:  5.067857265472412\n",
      "training error:  1.5170233249664307  and  4.060763359069824  and  1.568328619003296\n",
      "total error:  7.146115303039551\n",
      "training error:  1.4239310026168823  and  2.799274444580078  and  1.7862648963928223\n",
      "total error:  6.009470343589783\n",
      "training error:  1.4927761554718018  and  3.599343776702881  and  1.7402212619781494\n",
      "total error:  6.832341194152832\n",
      "training error:  1.3491923809051514  and  1.9605369567871094  and  1.4842607975006104\n",
      "total error:  4.793990135192871\n",
      "training error:  1.3461689949035645  and  3.6715219020843506  and  1.9355683326721191\n",
      "total error:  6.953259229660034\n",
      "training error:  1.397352695465088  and  2.0195515155792236  and  1.5120068788528442\n",
      "total error:  4.928911089897156\n",
      "training error:  1.4362883567810059  and  2.555539608001709  and  1.685908555984497\n",
      "total error:  5.677736520767212\n",
      "training error:  1.4160844087600708  and  3.608567476272583  and  1.7505923509597778\n",
      "total error:  6.775244235992432\n",
      "training error:  1.3440186977386475  and  2.455109119415283  and  1.7057663202285767\n",
      "total error:  5.504894137382507\n",
      "training error:  1.2984654903411865  and  1.9359784126281738  and  1.477137565612793\n",
      "total error:  4.711581468582153\n",
      "training error:  1.3064377307891846  and  2.052769899368286  and  1.6105473041534424\n",
      "total error:  4.969754934310913\n",
      "training error:  1.2840063571929932  and  1.9194047451019287  and  1.6158595085144043\n",
      "total error:  4.819270610809326\n",
      "training error:  1.3103080987930298  and  1.9697710275650024  and  1.6946816444396973\n",
      "total error:  4.9747607707977295\n",
      "training error:  1.3149094581604004  and  1.963644027709961  and  1.4709597826004028\n",
      "total error:  4.749513268470764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.2628190517425537  and  1.9020448923110962  and  1.5356340408325195\n",
      "total error:  4.700497984886169\n",
      "training error:  1.2569851875305176  and  1.867966890335083  and  1.5361485481262207\n",
      "total error:  4.661100625991821\n",
      "training error:  1.288804292678833  and  2.185131788253784  and  1.5550363063812256\n",
      "total error:  5.028972387313843\n",
      "training error:  1.2814550399780273  and  2.1346936225891113  and  1.611405372619629\n",
      "total error:  5.027554035186768\n",
      "training error:  1.2846368551254272  and  2.3056764602661133  and  1.430795431137085\n",
      "total error:  5.0211087465286255\n",
      "training error:  1.276003122329712  and  2.1782732009887695  and  1.523726463317871\n",
      "total error:  4.9780027866363525\n",
      "training error:  1.3605105876922607  and  2.7569808959960938  and  1.4932591915130615\n",
      "total error:  5.610750675201416\n",
      "training error:  1.290886640548706  and  3.019918918609619  and  1.7065062522888184\n",
      "total error:  6.0173118114471436\n",
      "training error:  1.3054978847503662  and  2.5352983474731445  and  1.4292125701904297\n",
      "total error:  5.27000880241394\n",
      "training error:  1.2795919179916382  and  2.053018808364868  and  1.452261209487915\n",
      "total error:  4.784871935844421\n",
      "training error:  1.307677149772644  and  4.145442008972168  and  1.914603352546692\n",
      "total error:  7.367722511291504\n",
      "training error:  1.2950401306152344  and  1.860276460647583  and  1.341186761856079\n",
      "total error:  4.4965033531188965\n",
      "training error:  1.2813804149627686  and  3.412806510925293  and  1.6950085163116455\n",
      "total error:  6.389195442199707\n",
      "training error:  1.2618093490600586  and  2.4456074237823486  and  1.7900691032409668\n",
      "total error:  5.497485876083374\n",
      "training error:  1.2810368537902832  and  2.2442290782928467  and  1.6248044967651367\n",
      "total error:  5.150070428848267\n",
      "training error:  1.3933019638061523  and  3.334568500518799  and  1.4545542001724243\n",
      "total error:  6.1824246644973755\n",
      "training error:  1.2610536813735962  and  2.198246955871582  and  1.724237322807312\n",
      "total error:  5.18353796005249\n",
      "training error:  1.235595703125  and  2.321333408355713  and  1.4919419288635254\n",
      "total error:  5.048871040344238\n",
      "training error:  1.2496360540390015  and  2.088226079940796  and  1.4154551029205322\n",
      "total error:  4.75331723690033\n",
      "training error:  1.286073088645935  and  2.124924898147583  and  1.543060541152954\n",
      "total error:  4.954058527946472\n",
      "training error:  1.295975923538208  and  2.627326250076294  and  1.4283535480499268\n",
      "total error:  5.351655721664429\n",
      "training error:  1.2776163816452026  and  1.8241806030273438  and  1.4070632457733154\n",
      "total error:  4.508860230445862\n",
      "training error:  1.245089054107666  and  1.8104195594787598  and  1.367966651916504\n",
      "total error:  4.42347526550293\n",
      "training error:  1.2736694812774658  and  2.547689914703369  and  1.4370499849319458\n",
      "total error:  5.258409380912781\n",
      "training error:  1.2463955879211426  and  2.464191198348999  and  1.603430986404419\n",
      "total error:  5.3140177726745605\n",
      "training error:  1.2926380634307861  and  2.232318162918091  and  1.4803036451339722\n",
      "total error:  5.005259871482849\n",
      "training error:  1.2508224248886108  and  1.9067034721374512  and  1.3445055484771729\n",
      "total error:  4.502031445503235\n",
      "training error:  1.1943185329437256  and  2.016322374343872  and  1.3891730308532715\n",
      "total error:  4.599813938140869\n",
      "training error:  1.230652928352356  and  2.234039306640625  and  1.4293023347854614\n",
      "total error:  4.893994569778442\n",
      "training error:  1.2037957906723022  and  1.9308459758758545  and  1.4396188259124756\n",
      "total error:  4.574260592460632\n",
      "training error:  1.3006315231323242  and  3.9654488563537598  and  1.469594120979309\n",
      "total error:  6.735674500465393\n",
      "training error:  1.261460542678833  and  2.8073415756225586  and  1.7787457704544067\n",
      "total error:  5.847547888755798\n",
      "training error:  1.2385826110839844  and  2.2200262546539307  and  1.4770326614379883\n",
      "total error:  4.935641527175903\n",
      "training error:  1.2885737419128418  and  3.593127727508545  and  1.5219024419784546\n",
      "total error:  6.403603911399841\n",
      "training error:  1.2537307739257812  and  3.189687967300415  and  1.7941932678222656\n",
      "total error:  6.237612009048462\n",
      "training error:  1.2976959943771362  and  1.8114242553710938  and  1.4956514835357666\n",
      "total error:  4.604771733283997\n",
      "training error:  1.3115026950836182  and  4.389388084411621  and  1.4765697717666626\n",
      "total error:  7.177460551261902\n",
      "training error:  1.2279675006866455  and  2.073784351348877  and  1.4396076202392578\n",
      "total error:  4.74135947227478\n",
      "training error:  1.2736461162567139  and  4.101169586181641  and  1.6951388120651245\n",
      "total error:  7.069954514503479\n",
      "training error:  1.2742751836776733  and  2.2780706882476807  and  1.570745587348938\n",
      "total error:  5.123091459274292\n",
      "training error:  1.329321265220642  and  2.801720142364502  and  1.5248273611068726\n",
      "total error:  5.655868768692017\n",
      "training error:  1.3316094875335693  and  2.8191356658935547  and  1.439009189605713\n",
      "total error:  5.589754343032837\n",
      "training error:  1.3497228622436523  and  2.6786608695983887  and  1.4853664636611938\n",
      "total error:  5.513750195503235\n",
      "training error:  1.2784039974212646  and  2.4422860145568848  and  1.439488172531128\n",
      "total error:  5.160178184509277\n",
      "training error:  1.2607272863388062  and  1.8388831615447998  and  1.5184097290039062\n",
      "total error:  4.618020176887512\n",
      "training error:  1.214331865310669  and  1.8182289600372314  and  1.3610719442367554\n",
      "total error:  4.393632769584656\n",
      "training error:  1.1691070795059204  and  1.9189329147338867  and  1.3809422254562378\n",
      "total error:  4.468982219696045\n",
      "training error:  1.231497049331665  and  1.9350996017456055  and  1.4923336505889893\n",
      "total error:  4.65893030166626\n",
      "training error:  1.187319278717041  and  1.7747077941894531  and  1.4911322593688965\n",
      "total error:  4.453159332275391\n",
      "training error:  1.1930023431777954  and  1.7557940483093262  and  1.4040610790252686\n",
      "total error:  4.35285747051239\n",
      "training error:  1.183512806892395  and  1.798702597618103  and  1.3526420593261719\n",
      "total error:  4.33485746383667\n",
      "training error:  1.211816430091858  and  1.807944416999817  and  1.4770584106445312\n",
      "total error:  4.496819257736206\n",
      "training error:  1.1730953454971313  and  1.781174659729004  and  1.387842059135437\n",
      "total error:  4.342112064361572\n",
      "training error:  1.2220368385314941  and  1.986296534538269  and  1.382983922958374\n",
      "total error:  4.591317296028137\n",
      "training error:  1.1993045806884766  and  2.147179126739502  and  1.4765725135803223\n",
      "total error:  4.823056221008301\n",
      "training error:  1.2116669416427612  and  1.7793140411376953  and  1.312731385231018\n",
      "total error:  4.303712368011475\n",
      "training error:  1.2233800888061523  and  2.318748950958252  and  1.6209877729415894\n",
      "total error:  5.163116812705994\n",
      "training error:  1.280490517616272  and  4.070980072021484  and  1.5495357513427734\n",
      "total error:  6.90100634098053\n",
      "training error:  1.2484402656555176  and  4.007646560668945  and  1.837969183921814\n",
      "total error:  7.094056010246277\n",
      "training error:  1.2228467464447021  and  1.8275561332702637  and  1.3552532196044922\n",
      "total error:  4.405656099319458\n",
      "training error:  1.2783780097961426  and  2.36513614654541  and  1.4635908603668213\n",
      "total error:  5.107105016708374\n",
      "training error:  1.2206292152404785  and  1.8880817890167236  and  1.4753506183624268\n",
      "total error:  4.584061622619629\n",
      "training error:  1.2077821493148804  and  1.808936595916748  and  1.4419376850128174\n",
      "total error:  4.458656430244446\n",
      "training error:  1.1818466186523438  and  1.7001690864562988  and  1.359006643295288\n",
      "total error:  4.241022348403931\n",
      "training error:  1.2167670726776123  and  1.758500099182129  and  1.3268495798110962\n",
      "total error:  4.302116751670837\n",
      "training error:  1.175173282623291  and  1.7063102722167969  and  1.4263499975204468\n",
      "total error:  4.307833552360535\n",
      "training error:  1.1883494853973389  and  1.705711841583252  and  1.3861066102981567\n",
      "total error:  4.280167937278748\n",
      "training error:  1.1641128063201904  and  1.7166227102279663  and  1.2806668281555176\n",
      "total error:  4.161402344703674\n",
      "training error:  1.1741454601287842  and  1.7148137092590332  and  1.384230613708496\n",
      "total error:  4.2731897830963135\n",
      "training error:  1.1742281913757324  and  1.6603734493255615  and  1.3015508651733398\n",
      "total error:  4.136152505874634\n",
      "training error:  1.172506332397461  and  1.6880269050598145  and  1.336106300354004\n",
      "total error:  4.196639537811279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.1695911884307861  and  1.6925134658813477  and  1.3058351278305054\n",
      "total error:  4.167939782142639\n",
      "training error:  1.1502621173858643  and  1.7651057243347168  and  1.3070567846298218\n",
      "total error:  4.222424626350403\n",
      "training error:  1.1801626682281494  and  1.8021478652954102  and  1.3474550247192383\n",
      "total error:  4.329765558242798\n",
      "training error:  1.1965818405151367  and  1.7112059593200684  and  1.3192580938339233\n",
      "total error:  4.227045893669128\n",
      "training error:  1.164451003074646  and  1.6757787466049194  and  1.325300931930542\n",
      "total error:  4.165530681610107\n",
      "training error:  1.1668429374694824  and  1.8111616373062134  and  1.4621843099594116\n",
      "total error:  4.440188884735107\n",
      "training error:  1.1670546531677246  and  1.730797529220581  and  1.315794825553894\n",
      "total error:  4.2136470079422\n",
      "training error:  1.1618108749389648  and  1.720443844795227  and  1.3132541179656982\n",
      "total error:  4.19550883769989\n",
      "training error:  1.1579790115356445  and  1.701810359954834  and  1.2828681468963623\n",
      "total error:  4.142657518386841\n",
      "training error:  1.1735694408416748  and  1.7115468978881836  and  1.3643635511398315\n",
      "total error:  4.24947988986969\n",
      "training error:  1.1481359004974365  and  1.6750187873840332  and  1.288299560546875\n",
      "total error:  4.111454248428345\n",
      "training error:  1.1755774021148682  and  1.7291771173477173  and  1.3382492065429688\n",
      "total error:  4.243003726005554\n",
      "training error:  1.1299984455108643  and  1.6907896995544434  and  1.2922471761703491\n",
      "total error:  4.113035321235657\n",
      "training error:  1.1515545845031738  and  1.635025978088379  and  1.3148066997528076\n",
      "total error:  4.10138726234436\n",
      "training error:  1.1218364238739014  and  1.782837152481079  and  1.2619675397872925\n",
      "total error:  4.166641116142273\n",
      "training error:  1.1434690952301025  and  1.657738447189331  and  1.3088442087173462\n",
      "total error:  4.11005175113678\n",
      "training error:  1.1573665142059326  and  1.650184154510498  and  1.2913826704025269\n",
      "total error:  4.0989333391189575\n",
      "training error:  1.1471548080444336  and  1.673940896987915  and  1.3036034107208252\n",
      "total error:  4.124699115753174\n",
      "training error:  1.1272306442260742  and  1.6357965469360352  and  1.3241643905639648\n",
      "total error:  4.087191581726074\n",
      "training error:  1.122699499130249  and  1.7002638578414917  and  1.29692542552948\n",
      "total error:  4.119888782501221\n",
      "training error:  1.151585340499878  and  1.8893816471099854  and  1.330328106880188\n",
      "total error:  4.371295094490051\n",
      "training error:  1.1431620121002197  and  1.6635119915008545  and  1.3198068141937256\n",
      "total error:  4.1264808177948\n",
      "training error:  1.1315791606903076  and  1.7270338535308838  and  1.3893507719039917\n",
      "total error:  4.247963786125183\n",
      "training error:  1.1503736972808838  and  1.677990436553955  and  1.3281636238098145\n",
      "total error:  4.156527757644653\n",
      "training error:  1.1560840606689453  and  1.701655387878418  and  1.3437607288360596\n",
      "total error:  4.201500177383423\n",
      "training error:  1.1295334100723267  and  1.7004433870315552  and  1.2917160987854004\n",
      "total error:  4.121692895889282\n",
      "training error:  1.1473749876022339  and  1.626090407371521  and  1.2745730876922607\n",
      "total error:  4.048038482666016\n",
      "training error:  1.1521356105804443  and  1.6767688989639282  and  1.2971932888031006\n",
      "total error:  4.126097798347473\n",
      "training error:  1.1488890647888184  and  1.64346444606781  and  1.249018669128418\n",
      "total error:  4.041372179985046\n",
      "training error:  1.1296743154525757  and  1.7381824254989624  and  1.273199200630188\n",
      "total error:  4.141055941581726\n",
      "training error:  1.1243834495544434  and  1.7527828216552734  and  1.298034906387329\n",
      "total error:  4.175201177597046\n",
      "training error:  1.1233330965042114  and  1.654516339302063  and  1.273102045059204\n",
      "total error:  4.0509514808654785\n",
      "training error:  1.1288444995880127  and  1.6600350141525269  and  1.3483614921569824\n",
      "total error:  4.137241005897522\n",
      "training error:  1.121280312538147  and  1.6787943840026855  and  1.2945549488067627\n",
      "total error:  4.094629645347595\n",
      "training error:  1.1112525463104248  and  1.5945611000061035  and  1.2950419187545776\n",
      "total error:  4.000855565071106\n",
      "training error:  1.1485371589660645  and  1.7000900506973267  and  1.2541999816894531\n",
      "total error:  4.102827191352844\n",
      "training error:  1.099637508392334  and  1.6796772480010986  and  1.3145792484283447\n",
      "total error:  4.093894004821777\n",
      "training error:  1.1322298049926758  and  1.628299355506897  and  1.2880655527114868\n",
      "total error:  4.04859471321106\n",
      "training error:  1.1143085956573486  and  1.676915168762207  and  1.3371303081512451\n",
      "total error:  4.128354072570801\n",
      "training error:  1.1150789260864258  and  1.6338757276535034  and  1.2381521463394165\n",
      "total error:  3.9871068000793457\n",
      "training error:  1.100893259048462  and  1.7153791189193726  and  1.3061872720718384\n",
      "total error:  4.122459650039673\n",
      "training error:  1.1118736267089844  and  1.681942105293274  and  1.3071144819259644\n",
      "total error:  4.100930213928223\n",
      "training error:  1.1282625198364258  and  1.6995586156845093  and  1.3015286922454834\n",
      "total error:  4.1293498277664185\n",
      "training error:  1.148728370666504  and  1.6229209899902344  and  1.312912940979004\n",
      "total error:  4.084562301635742\n",
      "training error:  1.1365348100662231  and  1.7741745710372925  and  1.2937315702438354\n",
      "total error:  4.204440951347351\n",
      "training error:  1.1378521919250488  and  1.6758241653442383  and  1.2998621463775635\n",
      "total error:  4.113538503646851\n",
      "training error:  1.1067211627960205  and  1.6141250133514404  and  1.3083534240722656\n",
      "total error:  4.029199600219727\n",
      "training error:  1.1034409999847412  and  1.5964679718017578  and  1.2561479806900024\n",
      "total error:  3.9560569524765015\n",
      "training error:  1.1105774641036987  and  1.608344316482544  and  1.2694774866104126\n",
      "total error:  3.9883992671966553\n",
      "training error:  1.1180405616760254  and  1.6257579326629639  and  1.39212167263031\n",
      "total error:  4.135920166969299\n",
      "training error:  1.1290068626403809  and  1.6103811264038086  and  1.2417824268341064\n",
      "total error:  3.981170415878296\n",
      "training error:  1.1464061737060547  and  1.6871402263641357  and  1.2627086639404297\n",
      "total error:  4.09625506401062\n",
      "training error:  1.1286406517028809  and  1.6591582298278809  and  1.2842029333114624\n",
      "total error:  4.072001814842224\n",
      "training error:  1.1494858264923096  and  1.6631923913955688  and  1.2609561681747437\n",
      "total error:  4.073634386062622\n",
      "training error:  1.0854296684265137  and  1.6006598472595215  and  1.2300114631652832\n",
      "total error:  3.9161009788513184\n",
      "training error:  1.106644630432129  and  1.7069165706634521  and  1.347546100616455\n",
      "total error:  4.161107301712036\n",
      "training error:  1.1078464984893799  and  1.6114475727081299  and  1.2768535614013672\n",
      "total error:  3.996147632598877\n",
      "training error:  1.1004326343536377  and  1.667900562286377  and  1.3107811212539673\n",
      "total error:  4.079114317893982\n",
      "training error:  1.1325404644012451  and  1.6538527011871338  and  1.3009287118911743\n",
      "total error:  4.087321877479553\n",
      "training error:  1.104008436203003  and  1.7841147184371948  and  1.3213304281234741\n",
      "total error:  4.209453582763672\n",
      "training error:  1.1119189262390137  and  1.654793381690979  and  1.3145124912261963\n",
      "total error:  4.081224799156189\n",
      "training error:  1.0934040546417236  and  1.6030137538909912  and  1.21726655960083\n",
      "total error:  3.913684368133545\n",
      "training error:  1.1144566535949707  and  1.5974454879760742  and  1.2477264404296875\n",
      "total error:  3.9596285820007324\n",
      "training error:  1.1013685464859009  and  1.5473116636276245  and  1.2180169820785522\n",
      "total error:  3.8666971921920776\n",
      "training error:  1.1277973651885986  and  1.7456800937652588  and  1.2904739379882812\n",
      "total error:  4.163951396942139\n",
      "training error:  1.1344661712646484  and  1.8134533166885376  and  1.3810908794403076\n",
      "total error:  4.329010367393494\n",
      "training error:  1.0896263122558594  and  1.5729568004608154  and  1.2151174545288086\n",
      "total error:  3.8777005672454834\n",
      "training error:  1.0793898105621338  and  1.6566107273101807  and  1.2573423385620117\n",
      "total error:  3.993342876434326\n",
      "training error:  1.1172693967819214  and  1.6033976078033447  and  1.2837138175964355\n",
      "total error:  4.004380822181702\n",
      "training error:  1.0894806385040283  and  1.5659241676330566  and  1.2479100227355957\n",
      "total error:  3.9033148288726807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.1098450422286987  and  1.6112385988235474  and  1.3919973373413086\n",
      "total error:  4.113080978393555\n",
      "training error:  1.117152214050293  and  1.5554172992706299  and  1.220350980758667\n",
      "total error:  3.89292049407959\n",
      "training error:  1.0991710424423218  and  1.6066737174987793  and  1.2927149534225464\n",
      "total error:  3.9985597133636475\n",
      "training error:  1.086097002029419  and  1.681613564491272  and  1.2633841037750244\n",
      "total error:  4.031094670295715\n",
      "training error:  1.1161448955535889  and  1.599325180053711  and  1.2905482053756714\n",
      "total error:  4.006018280982971\n",
      "training error:  1.097308874130249  and  1.5849332809448242  and  1.257698893547058\n",
      "total error:  3.9399410486221313\n",
      "training error:  1.0971806049346924  and  1.5627423524856567  and  1.2240736484527588\n",
      "total error:  3.883996605873108\n",
      "training error:  1.0886482000350952  and  1.5613977909088135  and  1.2240285873413086\n",
      "total error:  3.8740745782852173\n",
      "training error:  1.1262099742889404  and  1.5627999305725098  and  1.246036171913147\n",
      "total error:  3.935046076774597\n",
      "training error:  1.1154996156692505  and  1.5663915872573853  and  1.2568252086639404\n",
      "total error:  3.938716411590576\n",
      "training error:  1.1025426387786865  and  1.6143994331359863  and  1.3422695398330688\n",
      "total error:  4.059211611747742\n",
      "training error:  1.099832534790039  and  1.556619644165039  and  1.2232418060302734\n",
      "total error:  3.8796939849853516\n",
      "training error:  1.1077684164047241  and  1.6555745601654053  and  1.2429728507995605\n",
      "total error:  4.00631582736969\n",
      "training error:  1.143908977508545  and  1.5925867557525635  and  1.3334643840789795\n",
      "total error:  4.069960117340088\n",
      "training error:  1.0946365594863892  and  1.5775463581085205  and  1.2173911333084106\n",
      "total error:  3.8895740509033203\n",
      "training error:  1.0858546495437622  and  1.6068161725997925  and  1.2610042095184326\n",
      "total error:  3.9536750316619873\n",
      "training error:  1.1208107471466064  and  1.549839973449707  and  1.2694261074066162\n",
      "total error:  3.9400768280029297\n",
      "training error:  1.1001063585281372  and  1.7402942180633545  and  1.2173516750335693\n",
      "total error:  4.057752251625061\n",
      "training error:  1.0985549688339233  and  1.6017627716064453  and  1.2495357990264893\n",
      "total error:  3.949853539466858\n",
      "training error:  1.0954082012176514  and  1.6658117771148682  and  1.268198847770691\n",
      "total error:  4.0294188261032104\n",
      "training error:  1.090972900390625  and  1.5753264427185059  and  1.2443585395812988\n",
      "total error:  3.9106578826904297\n",
      "training error:  1.1046453714370728  and  1.6023268699645996  and  1.2775746583938599\n",
      "total error:  3.9845468997955322\n",
      "training error:  1.0916121006011963  and  1.6463749408721924  and  1.3093825578689575\n",
      "total error:  4.047369599342346\n",
      "training error:  1.085211157798767  and  1.561286211013794  and  1.3084139823913574\n",
      "total error:  3.9549113512039185\n",
      "training error:  1.1000261306762695  and  1.5770728588104248  and  1.2335405349731445\n",
      "total error:  3.910639524459839\n",
      "training error:  1.0557974576950073  and  1.5109970569610596  and  1.2053145170211792\n",
      "total error:  3.772109031677246\n",
      "training error:  1.1075677871704102  and  1.6439933776855469  and  1.209059238433838\n",
      "total error:  3.960620403289795\n",
      "training error:  1.0910817384719849  and  1.6025962829589844  and  1.2604188919067383\n",
      "total error:  3.9540969133377075\n",
      "training error:  1.0998337268829346  and  1.5724602937698364  and  1.2323663234710693\n",
      "total error:  3.9046603441238403\n",
      "training error:  1.0769731998443604  and  1.5581661462783813  and  1.2027969360351562\n",
      "total error:  3.837936282157898\n",
      "training error:  1.0583133697509766  and  1.5202056169509888  and  1.2250486612319946\n",
      "total error:  3.80356764793396\n",
      "training error:  1.0768474340438843  and  1.5271316766738892  and  1.2163878679275513\n",
      "total error:  3.8203669786453247\n",
      "training error:  1.0777215957641602  and  1.6988660097122192  and  1.397773265838623\n",
      "total error:  4.174360871315002\n",
      "training error:  1.0743224620819092  and  1.5420030355453491  and  1.1887197494506836\n",
      "total error:  3.805045247077942\n",
      "training error:  1.0983302593231201  and  1.5351386070251465  and  1.2373408079147339\n",
      "total error:  3.8708096742630005\n",
      "training error:  1.0654349327087402  and  1.543139100074768  and  1.1930022239685059\n",
      "total error:  3.801576256752014\n",
      "training error:  1.0679352283477783  and  1.8649218082427979  and  1.245258092880249\n",
      "total error:  4.178115129470825\n",
      "training error:  1.092401146888733  and  1.6702730655670166  and  1.3132562637329102\n",
      "total error:  4.07593047618866\n",
      "training error:  1.0548882484436035  and  1.5735186338424683  and  1.2067530155181885\n",
      "total error:  3.8351598978042603\n",
      "training error:  1.0702974796295166  and  1.5214424133300781  and  1.2261195182800293\n",
      "total error:  3.817859411239624\n",
      "training error:  1.0584120750427246  and  1.579183578491211  and  1.2033874988555908\n",
      "total error:  3.8409831523895264\n",
      "training error:  1.0754377841949463  and  1.661957025527954  and  1.2733467817306519\n",
      "total error:  4.010741591453552\n",
      "training error:  1.0777709484100342  and  1.5359818935394287  and  1.232130765914917\n",
      "total error:  3.84588360786438\n",
      "training error:  1.0869331359863281  and  1.673116683959961  and  1.3360729217529297\n",
      "total error:  4.096122741699219\n",
      "training error:  1.0799990892410278  and  1.5547102689743042  and  1.1936309337615967\n",
      "total error:  3.8283402919769287\n",
      "training error:  1.0823407173156738  and  1.531388521194458  and  1.2024831771850586\n",
      "total error:  3.8162124156951904\n",
      "training error:  1.0957497358322144  and  1.4950242042541504  and  1.2161638736724854\n",
      "total error:  3.80693781375885\n",
      "training error:  1.042264699935913  and  1.5162930488586426  and  1.2503453493118286\n",
      "total error:  3.8089030981063843\n",
      "training error:  1.0688283443450928  and  1.5300731658935547  and  1.2279160022735596\n",
      "total error:  3.826817512512207\n",
      "training error:  1.0815033912658691  and  1.4802236557006836  and  1.2236320972442627\n",
      "total error:  3.7853591442108154\n",
      "training error:  1.0597468614578247  and  1.5283350944519043  and  1.2589974403381348\n",
      "total error:  3.8470793962478638\n",
      "training error:  1.0871639251708984  and  1.5043950080871582  and  1.2117905616760254\n",
      "total error:  3.803349494934082\n",
      "training error:  1.1013127565383911  and  1.6832693815231323  and  1.2939374446868896\n",
      "total error:  4.078519582748413\n",
      "training error:  1.0298831462860107  and  1.583033561706543  and  1.2064006328582764\n",
      "total error:  3.81931734085083\n",
      "training error:  1.0648622512817383  and  1.5195670127868652  and  1.2961294651031494\n",
      "total error:  3.880558729171753\n",
      "training error:  1.0467190742492676  and  1.5002009868621826  and  1.2445828914642334\n",
      "total error:  3.7915029525756836\n",
      "training error:  1.06904935836792  and  1.4979698657989502  and  1.2248402833938599\n",
      "total error:  3.79185950756073\n",
      "training error:  1.0582032203674316  and  1.5130959749221802  and  1.173984408378601\n",
      "total error:  3.745283603668213\n",
      "training error:  1.0621434450149536  and  1.8056385517120361  and  1.364400863647461\n",
      "total error:  4.232182860374451\n",
      "training error:  1.1083545684814453  and  1.8708412647247314  and  1.5049736499786377\n",
      "total error:  4.4841694831848145\n",
      "training error:  1.0622222423553467  and  1.56785249710083  and  1.2987182140350342\n",
      "total error:  3.928792953491211\n",
      "training error:  1.1026313304901123  and  1.5731191635131836  and  1.2085590362548828\n",
      "total error:  3.8843095302581787\n",
      "training error:  1.0477654933929443  and  1.549013376235962  and  1.2313604354858398\n",
      "total error:  3.828139305114746\n",
      "training error:  1.0480496883392334  and  1.4595537185668945  and  1.2334444522857666\n",
      "total error:  3.7410478591918945\n",
      "training error:  1.0927923917770386  and  1.5209770202636719  and  1.1904771327972412\n",
      "total error:  3.8042465448379517\n",
      "training error:  1.0526771545410156  and  1.4808824062347412  and  1.2084687948226929\n",
      "total error:  3.7420283555984497\n",
      "training error:  1.0503524541854858  and  1.5401268005371094  and  1.173210859298706\n",
      "total error:  3.7636901140213013\n",
      "training error:  1.0670082569122314  and  1.4871385097503662  and  1.178482174873352\n",
      "total error:  3.7326289415359497\n",
      "training error:  1.0648881196975708  and  1.4869709014892578  and  1.2134373188018799\n",
      "total error:  3.7652963399887085\n",
      "training error:  1.0742135047912598  and  1.5070492029190063  and  1.2572834491729736\n",
      "total error:  3.8385461568832397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.1012204885482788  and  1.5812909603118896  and  1.2585015296936035\n",
      "total error:  3.941012978553772\n",
      "training error:  1.0666767358779907  and  1.498076319694519  and  1.1922807693481445\n",
      "total error:  3.7570338249206543\n",
      "training error:  1.0409541130065918  and  1.474427580833435  and  1.1929811239242554\n",
      "total error:  3.7083628177642822\n",
      "training error:  1.0317583084106445  and  1.529158353805542  and  1.2066552639007568\n",
      "total error:  3.7675719261169434\n",
      "training error:  1.0818530321121216  and  1.4647696018218994  and  1.2327828407287598\n",
      "total error:  3.7794054746627808\n",
      "training error:  1.0441814661026  and  1.492915391921997  and  1.2076282501220703\n",
      "total error:  3.7447251081466675\n",
      "training error:  1.0595569610595703  and  1.502698302268982  and  1.2690715789794922\n",
      "total error:  3.8313268423080444\n",
      "training error:  1.0525836944580078  and  1.4895503520965576  and  1.2186743021011353\n",
      "total error:  3.7608083486557007\n",
      "training error:  1.0428192615509033  and  1.5114426612854004  and  1.2235784530639648\n",
      "total error:  3.7778403759002686\n",
      "training error:  1.0172592401504517  and  1.503597617149353  and  1.1830859184265137\n",
      "total error:  3.7039427757263184\n",
      "training error:  1.0507811307907104  and  1.4782943725585938  and  1.1694780588150024\n",
      "total error:  3.6985535621643066\n",
      "training error:  1.0359634160995483  and  1.5812755823135376  and  1.2876386642456055\n",
      "total error:  3.9048776626586914\n",
      "training error:  1.0629477500915527  and  1.5874089002609253  and  1.2023593187332153\n",
      "total error:  3.8527159690856934\n",
      "training error:  1.050133466720581  and  1.5983229875564575  and  1.2183516025543213\n",
      "total error:  3.86680805683136\n",
      "training error:  1.0251518487930298  and  1.5162607431411743  and  1.1810293197631836\n",
      "total error:  3.7224419116973877\n",
      "training error:  1.0275068283081055  and  1.6087409257888794  and  1.2115256786346436\n",
      "total error:  3.8477734327316284\n",
      "training error:  1.0325214862823486  and  1.5011229515075684  and  1.2241877317428589\n",
      "total error:  3.757832169532776\n",
      "training error:  1.0275461673736572  and  1.563306212425232  and  1.1800493001937866\n",
      "total error:  3.770901679992676\n",
      "training error:  1.0412850379943848  and  1.5272374153137207  and  1.2103197574615479\n",
      "total error:  3.7788422107696533\n",
      "training error:  1.0449315309524536  and  1.549369215965271  and  1.3707716464996338\n",
      "total error:  3.9650723934173584\n",
      "training error:  1.0523641109466553  and  1.529982089996338  and  1.266638994216919\n",
      "total error:  3.848985195159912\n",
      "training error:  1.0335209369659424  and  1.5086073875427246  and  1.1581518650054932\n",
      "total error:  3.70028018951416\n",
      "training error:  1.0188064575195312  and  1.4721362590789795  and  1.2223408222198486\n",
      "total error:  3.7132835388183594\n",
      "training error:  1.0678250789642334  and  1.5554392337799072  and  1.2444779872894287\n",
      "total error:  3.8677423000335693\n",
      "training error:  1.048000693321228  and  1.4976145029067993  and  1.200258493423462\n",
      "total error:  3.7458736896514893\n",
      "training error:  1.0347647666931152  and  1.5884674787521362  and  1.2272536754608154\n",
      "total error:  3.850485920906067\n",
      "training error:  1.068127155303955  and  1.4855254888534546  and  1.1728432178497314\n",
      "total error:  3.726495862007141\n",
      "training error:  1.0357776880264282  and  1.519662618637085  and  1.182421326637268\n",
      "total error:  3.7378616333007812\n",
      "training error:  1.0542118549346924  and  1.6422474384307861  and  1.1766507625579834\n",
      "total error:  3.873110055923462\n",
      "training error:  1.0462632179260254  and  1.4955358505249023  and  1.2162851095199585\n",
      "total error:  3.7580841779708862\n",
      "training error:  1.0440376996994019  and  1.5430011749267578  and  1.223063349723816\n",
      "total error:  3.8101022243499756\n",
      "training error:  1.027996301651001  and  1.5036218166351318  and  1.2151236534118652\n",
      "total error:  3.746741771697998\n",
      "training error:  1.0415124893188477  and  1.5447174310684204  and  1.2254812717437744\n",
      "total error:  3.8117111921310425\n",
      "training error:  1.0512797832489014  and  1.4844609498977661  and  1.1829895973205566\n",
      "total error:  3.718730330467224\n",
      "training error:  1.043796181678772  and  1.7462552785873413  and  1.2312767505645752\n",
      "total error:  4.0213282108306885\n",
      "training error:  1.054447889328003  and  1.5297534465789795  and  1.300106406211853\n",
      "total error:  3.8843077421188354\n",
      "training error:  1.0687804222106934  and  1.5400221347808838  and  1.231607437133789\n",
      "total error:  3.840409994125366\n",
      "training error:  1.0397188663482666  and  1.488950252532959  and  1.1838099956512451\n",
      "total error:  3.7124791145324707\n",
      "training error:  1.0180504322052002  and  1.5774328708648682  and  1.2234981060028076\n",
      "total error:  3.818981409072876\n",
      "training error:  1.031123399734497  and  1.4994585514068604  and  1.1965525150299072\n",
      "total error:  3.7271344661712646\n",
      "training error:  1.0387401580810547  and  1.5392240285873413  and  1.1731460094451904\n",
      "total error:  3.7511101961135864\n",
      "training error:  1.0489161014556885  and  1.495955228805542  and  1.3229684829711914\n",
      "total error:  3.867839813232422\n",
      "training error:  1.0278520584106445  and  1.5110244750976562  and  1.2369799613952637\n",
      "total error:  3.7758564949035645\n",
      "training error:  1.031364917755127  and  1.565037488937378  and  1.2161095142364502\n",
      "total error:  3.812511920928955\n",
      "training error:  1.0258126258850098  and  1.4499015808105469  and  1.190403699874878\n",
      "total error:  3.6661179065704346\n",
      "training error:  1.0211100578308105  and  1.4905052185058594  and  1.1966309547424316\n",
      "total error:  3.7082462310791016\n",
      "training error:  1.040293574333191  and  1.4705902338027954  and  1.1730051040649414\n",
      "total error:  3.6838889122009277\n",
      "training error:  0.9971200227737427  and  1.5084658861160278  and  1.1807736158370972\n",
      "total error:  3.6863595247268677\n",
      "training error:  1.041487216949463  and  1.6144582033157349  and  1.206244707107544\n",
      "total error:  3.8621901273727417\n",
      "training error:  1.0222034454345703  and  1.479203462600708  and  1.1746151447296143\n",
      "total error:  3.6760220527648926\n",
      "training error:  1.0343658924102783  and  1.480088233947754  and  1.1877515316009521\n",
      "total error:  3.7022056579589844\n",
      "training error:  1.0256099700927734  and  1.4491201639175415  and  1.1793986558914185\n",
      "total error:  3.6541287899017334\n",
      "training error:  1.0215091705322266  and  1.5069034099578857  and  1.1425631046295166\n",
      "total error:  3.670975685119629\n",
      "training error:  1.008070945739746  and  1.5131514072418213  and  1.2065004110336304\n",
      "total error:  3.7277227640151978\n",
      "training error:  1.0178046226501465  and  1.6019868850708008  and  1.1768858432769775\n",
      "total error:  3.796677350997925\n",
      "training error:  1.0268542766571045  and  1.4687073230743408  and  1.1541138887405396\n",
      "total error:  3.649675488471985\n",
      "training error:  1.004767656326294  and  1.446685791015625  and  1.1928150653839111\n",
      "total error:  3.64426851272583\n",
      "training error:  1.022282600402832  and  1.5824687480926514  and  1.3200855255126953\n",
      "total error:  3.9248368740081787\n",
      "training error:  1.0154790878295898  and  1.5061979293823242  and  1.171425700187683\n",
      "total error:  3.693102717399597\n",
      "training error:  1.014922857284546  and  1.4576140642166138  and  1.1501851081848145\n",
      "total error:  3.622722029685974\n",
      "training error:  1.028590202331543  and  1.4958252906799316  and  1.193819522857666\n",
      "total error:  3.7182350158691406\n",
      "training error:  1.00575852394104  and  1.4998440742492676  and  1.2178850173950195\n",
      "total error:  3.723487615585327\n",
      "training error:  1.029170274734497  and  1.5763046741485596  and  1.2275738716125488\n",
      "total error:  3.8330488204956055\n",
      "training error:  1.0114136934280396  and  1.531687617301941  and  1.1932706832885742\n",
      "total error:  3.7363719940185547\n",
      "training error:  1.0290789604187012  and  1.4740256071090698  and  1.2199745178222656\n",
      "total error:  3.7230790853500366\n",
      "training error:  1.0343000888824463  and  1.4671626091003418  and  1.1716175079345703\n",
      "total error:  3.6730802059173584\n",
      "training error:  1.0220012664794922  and  1.44559907913208  and  1.1669106483459473\n",
      "total error:  3.6345109939575195\n",
      "training error:  1.0270090103149414  and  1.5122575759887695  and  1.1534898281097412\n",
      "total error:  3.692756414413452\n",
      "training error:  1.0044690370559692  and  1.4803414344787598  and  1.200141429901123\n",
      "total error:  3.684951901435852\n",
      "training error:  1.0210318565368652  and  1.4350676536560059  and  1.2111815214157104\n",
      "total error:  3.6672810316085815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.9940990209579468  and  1.5220742225646973  and  1.2380967140197754\n",
      "total error:  3.7542699575424194\n",
      "training error:  1.0140527486801147  and  1.5845811367034912  and  1.1654689311981201\n",
      "total error:  3.764102816581726\n",
      "training error:  0.9973326921463013  and  1.5080043077468872  and  1.1570860147476196\n",
      "total error:  3.662423014640808\n",
      "training error:  1.0390820503234863  and  1.5296127796173096  and  1.1962090730667114\n",
      "total error:  3.7649039030075073\n",
      "training error:  1.0032600164413452  and  1.540157437324524  and  1.1593739986419678\n",
      "total error:  3.702791452407837\n",
      "training error:  0.9898293614387512  and  1.4783127307891846  and  1.158970594406128\n",
      "total error:  3.6271126866340637\n",
      "training error:  1.0196278095245361  and  1.4398040771484375  and  1.1783955097198486\n",
      "total error:  3.6378273963928223\n",
      "training error:  0.9965514540672302  and  1.4242584705352783  and  1.1475634574890137\n",
      "total error:  3.568373382091522\n",
      "training error:  1.012423038482666  and  1.545911431312561  and  1.1712331771850586\n",
      "total error:  3.7295676469802856\n",
      "training error:  1.0499944686889648  and  1.4762178659439087  and  1.2077507972717285\n",
      "total error:  3.733963131904602\n",
      "training error:  1.0190446376800537  and  1.4756414890289307  and  1.1587257385253906\n",
      "total error:  3.653411865234375\n",
      "training error:  1.0203649997711182  and  1.4403362274169922  and  1.1635788679122925\n",
      "total error:  3.624280095100403\n",
      "training error:  0.9982055425643921  and  1.448686122894287  and  1.1677610874176025\n",
      "total error:  3.6146527528762817\n",
      "training error:  1.008543610572815  and  1.4598991870880127  and  1.1569575071334839\n",
      "total error:  3.6254003047943115\n",
      "training error:  1.016136646270752  and  1.4820120334625244  and  1.2145582437515259\n",
      "total error:  3.7127069234848022\n",
      "training error:  0.9988791942596436  and  1.4348708391189575  and  1.1825520992279053\n",
      "total error:  3.6163021326065063\n",
      "training error:  1.0201876163482666  and  1.517742395401001  and  1.1264169216156006\n",
      "total error:  3.664346933364868\n",
      "training error:  0.9996992945671082  and  1.493873119354248  and  1.225050449371338\n",
      "total error:  3.718622863292694\n",
      "training error:  1.011375069618225  and  1.5337785482406616  and  1.206254482269287\n",
      "total error:  3.751408100128174\n",
      "training error:  1.0320250988006592  and  1.4558305740356445  and  1.2036051750183105\n",
      "total error:  3.6914608478546143\n",
      "training error:  1.0007685422897339  and  1.4470187425613403  and  1.189908504486084\n",
      "total error:  3.637695789337158\n",
      "training error:  0.9946898221969604  and  1.434736967086792  and  1.1759798526763916\n",
      "total error:  3.605406641960144\n",
      "training error:  0.9876656532287598  and  1.4447317123413086  and  1.147870421409607\n",
      "total error:  3.5802677869796753\n",
      "training error:  0.9979116320610046  and  1.4914584159851074  and  1.1721631288528442\n",
      "total error:  3.6615331768989563\n",
      "training error:  1.0059428215026855  and  1.4550930261611938  and  1.1991722583770752\n",
      "total error:  3.6602081060409546\n",
      "training error:  1.0176002979278564  and  1.4360517263412476  and  1.1575521230697632\n",
      "total error:  3.611204147338867\n",
      "training error:  1.0206449031829834  and  1.689906358718872  and  1.202033281326294\n",
      "total error:  3.9125845432281494\n",
      "training error:  1.013735294342041  and  1.4771525859832764  and  1.2545595169067383\n",
      "total error:  3.7454473972320557\n",
      "training error:  1.0147572755813599  and  1.5752958059310913  and  1.1806378364562988\n",
      "total error:  3.77069091796875\n",
      "training error:  0.9953717589378357  and  1.4081488847732544  and  1.1886805295944214\n",
      "total error:  3.5922011733055115\n",
      "training error:  0.9982500076293945  and  1.4078638553619385  and  1.1578996181488037\n",
      "total error:  3.5640134811401367\n",
      "training error:  0.981838583946228  and  1.9512605667114258  and  1.170973300933838\n",
      "total error:  4.104072451591492\n",
      "training error:  1.0015093088150024  and  1.455742359161377  and  1.186460018157959\n",
      "total error:  3.6437116861343384\n",
      "training error:  1.0150835514068604  and  1.7912571430206299  and  1.2530920505523682\n",
      "total error:  4.059432744979858\n",
      "training error:  0.9931745529174805  and  1.490647315979004  and  1.1470952033996582\n",
      "total error:  3.6309170722961426\n",
      "training error:  0.9952051043510437  and  1.4457499980926514  and  1.1928706169128418\n",
      "total error:  3.633825719356537\n",
      "training error:  0.9995722770690918  and  1.4249893426895142  and  1.170850396156311\n",
      "total error:  3.595412015914917\n",
      "training error:  0.9675388336181641  and  1.8450852632522583  and  1.231546401977539\n",
      "total error:  4.044170498847961\n",
      "training error:  1.0050334930419922  and  1.6848444938659668  and  1.1808652877807617\n",
      "total error:  3.8707432746887207\n",
      "training error:  1.013465404510498  and  1.5337885618209839  and  1.157073736190796\n",
      "total error:  3.704327702522278\n",
      "training error:  0.9948269724845886  and  1.4739015102386475  and  1.117997646331787\n",
      "total error:  3.586726129055023\n",
      "training error:  0.9580278992652893  and  1.4447364807128906  and  1.2007615566253662\n",
      "total error:  3.603525936603546\n",
      "training error:  0.9826403856277466  and  1.4907203912734985  and  1.16120183467865\n",
      "total error:  3.634562611579895\n",
      "training error:  1.018147349357605  and  1.4673285484313965  and  1.2360671758651733\n",
      "total error:  3.721543073654175\n",
      "training error:  0.994292140007019  and  1.4536168575286865  and  1.207714557647705\n",
      "total error:  3.6556235551834106\n",
      "training error:  0.9880874156951904  and  1.6447303295135498  and  1.2502537965774536\n",
      "total error:  3.883071541786194\n",
      "training error:  0.9832278490066528  and  1.5358107089996338  and  1.1488783359527588\n",
      "total error:  3.6679168939590454\n",
      "training error:  0.9894930720329285  and  1.476942777633667  and  1.142191767692566\n",
      "total error:  3.6086276173591614\n",
      "training error:  0.9764724373817444  and  1.4192912578582764  and  1.1690549850463867\n",
      "total error:  3.5648186802864075\n",
      "training error:  0.9854716062545776  and  1.4915279150009155  and  1.1292214393615723\n",
      "total error:  3.6062209606170654\n",
      "training error:  0.994104266166687  and  1.4654028415679932  and  1.139667272567749\n",
      "total error:  3.599174380302429\n",
      "training error:  0.9830834269523621  and  1.6075654029846191  and  1.2109509706497192\n",
      "total error:  3.8015998005867004\n",
      "training error:  0.9673412442207336  and  1.520582914352417  and  1.1477856636047363\n",
      "total error:  3.635709822177887\n",
      "training error:  0.9749690890312195  and  1.3907768726348877  and  1.1642944812774658\n",
      "total error:  3.530040442943573\n",
      "training error:  0.9962450265884399  and  1.5110864639282227  and  1.252638578414917\n",
      "total error:  3.7599700689315796\n",
      "training error:  1.0171295404434204  and  1.4398446083068848  and  1.1962286233901978\n",
      "total error:  3.653202772140503\n",
      "training error:  0.9787321090698242  and  1.4003167152404785  and  1.1278424263000488\n",
      "total error:  3.5068912506103516\n",
      "training error:  0.9587218165397644  and  1.473268985748291  and  1.2289729118347168\n",
      "total error:  3.660963714122772\n",
      "training error:  0.9938997626304626  and  1.4258164167404175  and  1.1590244770050049\n",
      "total error:  3.578740656375885\n",
      "training error:  1.0021723508834839  and  1.5400820970535278  and  1.1735196113586426\n",
      "total error:  3.7157740592956543\n",
      "training error:  0.9689408540725708  and  1.4750392436981201  and  1.2666761875152588\n",
      "total error:  3.7106562852859497\n",
      "training error:  0.9859637022018433  and  1.3884177207946777  and  1.138677716255188\n",
      "total error:  3.513059139251709\n",
      "training error:  0.9870333671569824  and  1.5181519985198975  and  1.111857295036316\n",
      "total error:  3.617042660713196\n",
      "training error:  0.9687486290931702  and  1.39853835105896  and  1.1605198383331299\n",
      "total error:  3.52780681848526\n",
      "training error:  1.0018730163574219  and  1.4799548387527466  and  1.2591922283172607\n",
      "total error:  3.741020083427429\n",
      "training error:  1.0037659406661987  and  1.4540082216262817  and  1.133382797241211\n",
      "total error:  3.5911569595336914\n",
      "training error:  0.9674009680747986  and  1.389174461364746  and  1.1724116802215576\n",
      "total error:  3.5289871096611023\n",
      "training error:  0.9660462737083435  and  1.441248893737793  and  1.1269251108169556\n",
      "total error:  3.534220278263092\n",
      "training error:  0.9562638998031616  and  1.4132492542266846  and  1.1624335050582886\n",
      "total error:  3.5319466590881348\n",
      "training error:  0.9780135750770569  and  1.357027530670166  and  1.177062749862671\n",
      "total error:  3.512103855609894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.9889802932739258  and  1.5185110569000244  and  1.1432311534881592\n",
      "total error:  3.6507225036621094\n",
      "training error:  0.9621830582618713  and  1.5369091033935547  and  1.1978285312652588\n",
      "total error:  3.696920692920685\n",
      "training error:  0.9627043008804321  and  1.4554544687271118  and  1.2123239040374756\n",
      "total error:  3.6304826736450195\n",
      "training error:  1.0054206848144531  and  1.466135859489441  and  1.191497802734375\n",
      "total error:  3.663054347038269\n",
      "training error:  0.9487791061401367  and  1.5065281391143799  and  1.1137068271636963\n",
      "total error:  3.569014072418213\n",
      "training error:  1.0198001861572266  and  1.435736894607544  and  1.1593708992004395\n",
      "total error:  3.61490797996521\n",
      "training error:  0.9663941860198975  and  1.4452048540115356  and  1.1367502212524414\n",
      "total error:  3.5483492612838745\n",
      "training error:  0.9761618375778198  and  1.4381873607635498  and  1.2015520334243774\n",
      "total error:  3.615901231765747\n",
      "training error:  0.9867277145385742  and  1.4407832622528076  and  1.1784989833831787\n",
      "total error:  3.6060099601745605\n",
      "training error:  0.9699469208717346  and  1.4620046615600586  and  1.1275311708450317\n",
      "total error:  3.559482753276825\n",
      "training error:  0.9740501642227173  and  1.3920427560806274  and  1.1972451210021973\n",
      "total error:  3.563338041305542\n",
      "training error:  0.9787993431091309  and  1.5100798606872559  and  1.2447941303253174\n",
      "total error:  3.733673334121704\n",
      "training error:  0.980139434337616  and  1.4089868068695068  and  1.1383490562438965\n",
      "total error:  3.5274752974510193\n",
      "training error:  0.9700689315795898  and  1.4763123989105225  and  1.2208927869796753\n",
      "total error:  3.6672741174697876\n",
      "training error:  0.9763466119766235  and  1.3982808589935303  and  1.1364388465881348\n",
      "total error:  3.5110663175582886\n",
      "training error:  0.9834490418434143  and  1.4005106687545776  and  1.1457332372665405\n",
      "total error:  3.5296929478645325\n",
      "training error:  0.9957740306854248  and  1.4429384469985962  and  1.2101068496704102\n",
      "total error:  3.648819327354431\n",
      "training error:  0.9680412411689758  and  1.4425528049468994  and  1.1906800270080566\n",
      "total error:  3.601274073123932\n",
      "training error:  0.9769575595855713  and  1.5487186908721924  and  1.1611242294311523\n",
      "total error:  3.686800479888916\n",
      "training error:  0.9733274579048157  and  1.7331132888793945  and  1.2322368621826172\n",
      "total error:  3.9386776089668274\n",
      "training error:  0.954658031463623  and  1.4475187063217163  and  1.1606061458587646\n",
      "total error:  3.562782883644104\n",
      "training error:  0.9650686979293823  and  1.4203118085861206  and  1.2515077590942383\n",
      "total error:  3.636888265609741\n",
      "training error:  0.9784877300262451  and  1.3932013511657715  and  1.1718792915344238\n",
      "total error:  3.5435683727264404\n",
      "training error:  0.9821746349334717  and  1.3827121257781982  and  1.1311360597610474\n",
      "total error:  3.4960228204727173\n",
      "training error:  0.957348108291626  and  1.5929489135742188  and  1.1832647323608398\n",
      "total error:  3.7335617542266846\n",
      "training error:  0.9506078958511353  and  1.4472187757492065  and  1.188406229019165\n",
      "total error:  3.586232900619507\n",
      "training error:  0.964614748954773  and  1.4379587173461914  and  1.1567347049713135\n",
      "total error:  3.559308171272278\n",
      "training error:  0.9937901496887207  and  1.5431122779846191  and  1.1498687267303467\n",
      "total error:  3.6867711544036865\n",
      "training error:  0.9725725650787354  and  1.4497900009155273  and  1.1319026947021484\n",
      "total error:  3.554265260696411\n",
      "training error:  0.9548975825309753  and  1.454322099685669  and  1.1404266357421875\n",
      "total error:  3.549646317958832\n",
      "training error:  0.9601607322692871  and  1.4750170707702637  and  1.2636449337005615\n",
      "total error:  3.6988227367401123\n",
      "training error:  0.9854229092597961  and  1.4360191822052002  and  1.2756645679473877\n",
      "total error:  3.697106659412384\n",
      "training error:  0.9679415822029114  and  1.4071619510650635  and  1.174142599105835\n",
      "total error:  3.54924613237381\n",
      "training error:  0.9691453576087952  and  1.3919355869293213  and  1.1035747528076172\n",
      "total error:  3.4646556973457336\n",
      "training error:  0.9483150243759155  and  1.4188157320022583  and  1.165919303894043\n",
      "total error:  3.533050060272217\n",
      "training error:  0.9869890809059143  and  1.4299194812774658  and  1.1469321250915527\n",
      "total error:  3.563840687274933\n",
      "training error:  0.9607592225074768  and  1.5617759227752686  and  1.118422269821167\n",
      "total error:  3.6409574151039124\n",
      "training error:  0.9626852869987488  and  1.4422757625579834  and  1.1526594161987305\n",
      "total error:  3.5576204657554626\n",
      "training error:  0.9427173137664795  and  1.535200834274292  and  1.1215384006500244\n",
      "total error:  3.599456548690796\n",
      "training error:  0.9690923690795898  and  1.4217681884765625  and  1.194757103919983\n",
      "total error:  3.5856176614761353\n",
      "training error:  0.9429681897163391  and  1.3894755840301514  and  1.135941743850708\n",
      "total error:  3.4683855175971985\n",
      "training error:  0.9852164387702942  and  1.5716124773025513  and  1.2328298091888428\n",
      "total error:  3.7896587252616882\n",
      "training error:  0.95311039686203  and  1.3823018074035645  and  1.0880842208862305\n",
      "total error:  3.423496425151825\n",
      "training error:  0.9484202861785889  and  1.580461025238037  and  1.1848280429840088\n",
      "total error:  3.7137093544006348\n",
      "training error:  0.9632672071456909  and  1.708478569984436  and  1.1066101789474487\n",
      "total error:  3.7783559560775757\n",
      "training error:  0.9750394225120544  and  1.3651928901672363  and  1.1580891609191895\n",
      "total error:  3.4983214735984802\n",
      "training error:  0.9303333759307861  and  1.3948346376419067  and  1.1382726430892944\n",
      "total error:  3.4634406566619873\n",
      "training error:  0.9451097249984741  and  1.4789059162139893  and  1.1963839530944824\n",
      "total error:  3.620399594306946\n",
      "training error:  0.9581522941589355  and  1.431522250175476  and  1.122262716293335\n",
      "total error:  3.5119372606277466\n",
      "training error:  0.954325795173645  and  1.6168572902679443  and  1.1070541143417358\n",
      "total error:  3.678237199783325\n",
      "training error:  0.9498807191848755  and  1.629500150680542  and  1.2161695957183838\n",
      "total error:  3.7955504655838013\n",
      "training error:  0.9437233805656433  and  1.4347505569458008  and  1.1711087226867676\n",
      "total error:  3.5495826601982117\n",
      "training error:  0.9457141757011414  and  1.3839256763458252  and  1.1523107290267944\n",
      "total error:  3.481950581073761\n",
      "training error:  0.968741774559021  and  1.4028840065002441  and  1.1297030448913574\n",
      "total error:  3.5013288259506226\n",
      "training error:  0.9310508966445923  and  1.6062284708023071  and  1.1136014461517334\n",
      "total error:  3.650880813598633\n",
      "training error:  0.9555287957191467  and  1.5530900955200195  and  1.1433221101760864\n",
      "total error:  3.6519410014152527\n",
      "training error:  0.9600579738616943  and  1.5700500011444092  and  1.2948546409606934\n",
      "total error:  3.824962615966797\n",
      "training error:  0.9288313388824463  and  1.3877415657043457  and  1.1423026323318481\n",
      "total error:  3.45887553691864\n",
      "training error:  0.9410763382911682  and  1.3892338275909424  and  1.151033878326416\n",
      "total error:  3.4813440442085266\n",
      "training error:  0.9372767210006714  and  1.4346015453338623  and  1.0958445072174072\n",
      "total error:  3.467722773551941\n",
      "training error:  0.9599877595901489  and  1.4097166061401367  and  1.1309497356414795\n",
      "total error:  3.500654101371765\n",
      "training error:  0.9676171541213989  and  1.5468136072158813  and  1.1562190055847168\n",
      "total error:  3.670649766921997\n",
      "training error:  0.9565762877464294  and  1.5133512020111084  and  1.1985024213790894\n",
      "total error:  3.668429911136627\n",
      "training error:  0.9566304683685303  and  1.5990691184997559  and  1.1506683826446533\n",
      "total error:  3.7063679695129395\n",
      "training error:  0.9488022327423096  and  1.4530608654022217  and  1.1898902654647827\n",
      "total error:  3.591753363609314\n",
      "training error:  0.9566145539283752  and  1.3865861892700195  and  1.1359694004058838\n",
      "total error:  3.4791701436042786\n",
      "training error:  0.9381409287452698  and  1.4601881504058838  and  1.1540913581848145\n",
      "total error:  3.552420437335968\n",
      "training error:  0.9456539154052734  and  1.4574589729309082  and  1.1747127771377563\n",
      "total error:  3.577825665473938\n",
      "training error:  0.9253242015838623  and  1.386264443397522  and  1.1582151651382446\n",
      "total error:  3.469803810119629\n",
      "training error:  0.955990195274353  and  1.4054710865020752  and  1.2161720991134644\n",
      "total error:  3.5776333808898926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.9404668807983398  and  1.424098253250122  and  1.3067858219146729\n",
      "total error:  3.6713509559631348\n",
      "training error:  0.950316309928894  and  1.5022259950637817  and  1.1462339162826538\n",
      "total error:  3.5987762212753296\n",
      "training error:  0.9535213112831116  and  1.4670641422271729  and  1.1150217056274414\n",
      "total error:  3.535607159137726\n",
      "training error:  0.9312984943389893  and  1.3967548608779907  and  1.1282455921173096\n",
      "total error:  3.4562989473342896\n",
      "training error:  0.9446348547935486  and  1.427411675453186  and  1.206246256828308\n",
      "total error:  3.5782927870750427\n",
      "training error:  0.925500750541687  and  1.4642219543457031  and  1.116389274597168\n",
      "total error:  3.506111979484558\n",
      "training error:  0.9299601912498474  and  1.5186667442321777  and  1.1450270414352417\n",
      "total error:  3.593653976917267\n",
      "training error:  0.9543787240982056  and  1.3788928985595703  and  1.085832118988037\n",
      "total error:  3.419103741645813\n",
      "training error:  0.939847469329834  and  1.3711957931518555  and  1.14766526222229\n",
      "total error:  3.4587085247039795\n",
      "training error:  0.9290751814842224  and  1.3735076189041138  and  1.1246132850646973\n",
      "total error:  3.4271960854530334\n",
      "training error:  0.9183954000473022  and  1.3872830867767334  and  1.1177730560302734\n",
      "total error:  3.423451542854309\n",
      "training error:  0.89815354347229  and  1.3807930946350098  and  1.1121845245361328\n",
      "total error:  3.3911311626434326\n",
      "training error:  0.9173252582550049  and  1.4153281450271606  and  1.1213669776916504\n",
      "total error:  3.454020380973816\n",
      "training error:  0.9515737295150757  and  1.5729496479034424  and  1.1474833488464355\n",
      "total error:  3.6720067262649536\n",
      "training error:  0.9395714998245239  and  1.4243637323379517  and  1.1428289413452148\n",
      "total error:  3.5067641735076904\n",
      "training error:  0.9403359889984131  and  1.3867820501327515  and  1.1968258619308472\n",
      "total error:  3.5239439010620117\n",
      "training error:  0.9292840957641602  and  1.3815289735794067  and  1.2647902965545654\n",
      "total error:  3.5756033658981323\n",
      "training error:  0.9068543910980225  and  1.3661547899246216  and  1.109480381011963\n",
      "total error:  3.382489562034607\n",
      "training error:  0.9571454524993896  and  1.4149672985076904  and  1.1981754302978516\n",
      "total error:  3.5702881813049316\n",
      "training error:  0.9119439125061035  and  1.3686518669128418  and  1.1353390216827393\n",
      "total error:  3.4159348011016846\n",
      "training error:  0.9368022084236145  and  1.592237949371338  and  1.1739518642425537\n",
      "total error:  3.702992022037506\n",
      "training error:  0.9602051973342896  and  1.5637316703796387  and  1.1253573894500732\n",
      "total error:  3.6492942571640015\n",
      "training error:  0.953475296497345  and  1.4222944974899292  and  1.128570556640625\n",
      "total error:  3.504340350627899\n",
      "training error:  0.9098305702209473  and  1.3488500118255615  and  1.10563063621521\n",
      "total error:  3.3643112182617188\n",
      "training error:  0.9278824329376221  and  1.4991114139556885  and  1.1598927974700928\n",
      "total error:  3.5868866443634033\n",
      "training error:  0.9674539566040039  and  1.4202898740768433  and  1.2439755201339722\n",
      "total error:  3.6317193508148193\n",
      "training error:  0.9421016573905945  and  1.4252737760543823  and  1.132188081741333\n",
      "total error:  3.49956351518631\n",
      "training error:  0.8997201919555664  and  1.3697242736816406  and  1.1397653818130493\n",
      "total error:  3.4092098474502563\n",
      "training error:  0.9452296495437622  and  1.4438282251358032  and  1.0999881029129028\n",
      "total error:  3.4890459775924683\n",
      "training error:  0.929190456867218  and  1.3841705322265625  and  1.1832826137542725\n",
      "total error:  3.496643602848053\n",
      "training error:  0.9187132716178894  and  1.548639178276062  and  1.1617023944854736\n",
      "total error:  3.629054844379425\n",
      "training error:  0.9297467470169067  and  1.363701343536377  and  1.1095397472381592\n",
      "total error:  3.402987837791443\n",
      "training error:  0.9342098832130432  and  1.444331407546997  and  1.1175131797790527\n",
      "total error:  3.496054470539093\n",
      "training error:  0.9316875338554382  and  1.5765459537506104  and  1.1263842582702637\n",
      "total error:  3.6346177458763123\n",
      "training error:  0.9033393859863281  and  1.391028642654419  and  1.1441466808319092\n",
      "total error:  3.4385147094726562\n",
      "training error:  0.9024764895439148  and  1.3829116821289062  and  1.0926743745803833\n",
      "total error:  3.3780625462532043\n",
      "training error:  0.8905845880508423  and  1.402679204940796  and  1.1483030319213867\n",
      "total error:  3.441566824913025\n",
      "training error:  0.9201914072036743  and  1.3826178312301636  and  1.1351089477539062\n",
      "total error:  3.437918186187744\n",
      "training error:  0.9462684988975525  and  1.5018649101257324  and  1.1268389225006104\n",
      "total error:  3.5749723315238953\n",
      "training error:  0.9050989151000977  and  1.3955812454223633  and  1.1297495365142822\n",
      "total error:  3.430429697036743\n",
      "training error:  0.942471444606781  and  1.5600941181182861  and  1.1895676851272583\n",
      "total error:  3.6921332478523254\n",
      "training error:  0.9507325887680054  and  1.4506871700286865  and  1.2524895668029785\n",
      "total error:  3.6539093255996704\n",
      "training error:  0.9132036566734314  and  1.4816770553588867  and  1.1514372825622559\n",
      "total error:  3.546317994594574\n",
      "training error:  0.9376652240753174  and  1.4130918979644775  and  1.1353870630264282\n",
      "total error:  3.486144185066223\n",
      "training error:  0.929114580154419  and  1.686352252960205  and  1.1397591829299927\n",
      "total error:  3.7552260160446167\n",
      "training error:  0.9012783765792847  and  1.4164178371429443  and  1.0910861492156982\n",
      "total error:  3.4087823629379272\n",
      "training error:  0.9328158497810364  and  1.5640254020690918  and  1.1352330446243286\n",
      "total error:  3.632074296474457\n",
      "training error:  0.9131383895874023  and  1.3778321743011475  and  1.079344630241394\n",
      "total error:  3.370315194129944\n",
      "training error:  0.9144983291625977  and  1.6054682731628418  and  1.2191452980041504\n",
      "total error:  3.73911190032959\n",
      "training error:  0.9328733086585999  and  1.3982278108596802  and  1.1637369394302368\n",
      "total error:  3.494838058948517\n",
      "training error:  0.9348893165588379  and  1.486045002937317  and  1.0941611528396606\n",
      "total error:  3.5150954723358154\n",
      "training error:  0.9060574173927307  and  1.3843731880187988  and  1.1276419162750244\n",
      "total error:  3.418072521686554\n",
      "training error:  0.8986145853996277  and  1.3769402503967285  and  1.1464200019836426\n",
      "total error:  3.421974837779999\n",
      "training error:  0.9030213952064514  and  1.3642737865447998  and  1.1673252582550049\n",
      "total error:  3.434620440006256\n",
      "training error:  0.9016284942626953  and  1.384412169456482  and  1.132901668548584\n",
      "total error:  3.4189423322677612\n",
      "training error:  0.9033067226409912  and  1.3742222785949707  and  1.1993083953857422\n",
      "total error:  3.476837396621704\n",
      "training error:  0.92323899269104  and  1.4388359785079956  and  1.1029762029647827\n",
      "total error:  3.4650511741638184\n",
      "training error:  0.9332854151725769  and  1.361142635345459  and  1.0882012844085693\n",
      "total error:  3.3826293349266052\n",
      "training error:  0.9279525279998779  and  1.3799504041671753  and  1.1417450904846191\n",
      "total error:  3.4496480226516724\n",
      "training error:  0.8992418050765991  and  1.3872301578521729  and  1.113306999206543\n",
      "total error:  3.399778962135315\n",
      "training error:  0.9065860509872437  and  1.4118194580078125  and  1.098685383796692\n",
      "total error:  3.417090892791748\n",
      "training error:  0.9106330275535583  and  1.3235962390899658  and  1.1085686683654785\n",
      "total error:  3.3427979350090027\n",
      "training error:  0.9037207961082458  and  1.6924419403076172  and  1.1831971406936646\n",
      "total error:  3.7793598771095276\n",
      "training error:  0.9069982171058655  and  1.5362825393676758  and  1.0790717601776123\n",
      "total error:  3.5223525166511536\n",
      "training error:  0.8914810419082642  and  1.5881829261779785  and  1.2345646619796753\n",
      "total error:  3.714228630065918\n",
      "training error:  0.9460972547531128  and  1.395435094833374  and  1.112393856048584\n",
      "total error:  3.453926205635071\n",
      "training error:  0.8933855295181274  and  1.3639150857925415  and  1.0817244052886963\n",
      "total error:  3.3390250205993652\n",
      "training error:  0.9290263056755066  and  1.4003887176513672  and  1.092636227607727\n",
      "total error:  3.422051250934601\n",
      "training error:  0.9346715211868286  and  1.4792815446853638  and  1.238346815109253\n",
      "total error:  3.6522998809814453\n",
      "training error:  0.876519501209259  and  1.51448655128479  and  1.1301718950271606\n",
      "total error:  3.5211779475212097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.9181543588638306  and  1.3664069175720215  and  1.1387379169464111\n",
      "total error:  3.423299193382263\n",
      "training error:  0.9145143628120422  and  1.4305052757263184  and  1.1377630233764648\n",
      "total error:  3.4827826619148254\n",
      "training error:  0.9053983092308044  and  1.4149689674377441  and  1.1045186519622803\n",
      "total error:  3.424885928630829\n",
      "training error:  0.9138343334197998  and  1.4044920206069946  and  1.1554741859436035\n",
      "total error:  3.473800539970398\n",
      "training error:  0.887093186378479  and  1.4112629890441895  and  1.1863155364990234\n",
      "total error:  3.484671711921692\n",
      "training error:  0.9048730134963989  and  1.3664066791534424  and  1.1271274089813232\n",
      "total error:  3.3984071016311646\n",
      "training error:  0.8935115337371826  and  1.4126609563827515  and  1.1307246685028076\n",
      "total error:  3.4368971586227417\n",
      "training error:  0.9275690317153931  and  1.361552119255066  and  1.1338067054748535\n",
      "total error:  3.4229278564453125\n",
      "training error:  0.8925497531890869  and  1.4372965097427368  and  1.1250512599945068\n",
      "total error:  3.4548975229263306\n",
      "training error:  0.9062488079071045  and  1.5384433269500732  and  1.1983466148376465\n",
      "total error:  3.643038749694824\n",
      "training error:  0.9312512874603271  and  1.4002299308776855  and  1.1312508583068848\n",
      "total error:  3.4627320766448975\n",
      "training error:  0.911300003528595  and  1.4252030849456787  and  1.2706379890441895\n",
      "total error:  3.607141077518463\n",
      "training error:  0.8999758958816528  and  1.8402801752090454  and  1.2205768823623657\n",
      "total error:  3.960832953453064\n",
      "training error:  0.8931019902229309  and  1.4193826913833618  and  1.0981979370117188\n",
      "total error:  3.4106826186180115\n",
      "training error:  0.9074320197105408  and  1.34357750415802  and  1.1059457063674927\n",
      "total error:  3.3569552302360535\n",
      "training error:  0.9189476370811462  and  1.5112413167953491  and  1.1482243537902832\n",
      "total error:  3.5784133076667786\n",
      "training error:  0.9117112755775452  and  1.3863657712936401  and  1.097290277481079\n",
      "total error:  3.3953673243522644\n",
      "training error:  0.9148147106170654  and  1.435552954673767  and  1.1360552310943604\n",
      "total error:  3.486422896385193\n",
      "training error:  0.9236294627189636  and  1.5566039085388184  and  1.1664835214614868\n",
      "total error:  3.646716892719269\n",
      "training error:  0.9147146940231323  and  1.343882441520691  and  1.12344229221344\n",
      "total error:  3.382039427757263\n",
      "training error:  0.8964234590530396  and  1.350440502166748  and  1.0924789905548096\n",
      "total error:  3.339342951774597\n",
      "training error:  0.8925579190254211  and  1.3540223836898804  and  1.0986367464065552\n",
      "total error:  3.3452170491218567\n",
      "training error:  0.8940215110778809  and  1.384429693222046  and  1.1344878673553467\n",
      "total error:  3.4129390716552734\n",
      "training error:  0.9158114194869995  and  1.3757257461547852  and  1.1205105781555176\n",
      "total error:  3.4120477437973022\n",
      "training error:  0.8972223997116089  and  1.4021821022033691  and  1.132449746131897\n",
      "total error:  3.431854248046875\n",
      "training error:  0.8997073769569397  and  1.3556149005889893  and  1.1235325336456299\n",
      "total error:  3.378854811191559\n",
      "training error:  0.8820725679397583  and  1.3741488456726074  and  1.1291611194610596\n",
      "total error:  3.3853825330734253\n",
      "training error:  0.9042055606842041  and  1.410501480102539  and  1.1200623512268066\n",
      "total error:  3.43476939201355\n",
      "training error:  0.9341899752616882  and  1.3749322891235352  and  1.1419203281402588\n",
      "total error:  3.451042592525482\n",
      "training error:  0.9371809959411621  and  1.6696505546569824  and  1.115461826324463\n",
      "total error:  3.7222933769226074\n",
      "training error:  0.9018248319625854  and  1.408341407775879  and  1.1598379611968994\n",
      "total error:  3.4700042009353638\n",
      "training error:  0.9210600256919861  and  1.5719208717346191  and  1.1454205513000488\n",
      "total error:  3.638401448726654\n",
      "training error:  0.8934247493743896  and  1.573472023010254  and  1.1265740394592285\n",
      "total error:  3.593470811843872\n",
      "training error:  0.8778448700904846  and  1.4285929203033447  and  1.0628756284713745\n",
      "total error:  3.369313418865204\n",
      "training error:  0.9222327470779419  and  1.3256925344467163  and  1.084470272064209\n",
      "total error:  3.332395553588867\n",
      "training error:  0.8836531043052673  and  1.4046021699905396  and  1.159258484840393\n",
      "total error:  3.4475137591362\n",
      "training error:  0.8833618760108948  and  1.3427512645721436  and  1.0965244770050049\n",
      "total error:  3.322637617588043\n",
      "training error:  0.878400444984436  and  1.4891191720962524  and  1.2565869092941284\n",
      "total error:  3.624106526374817\n",
      "training error:  0.8834115862846375  and  1.4425041675567627  and  1.0996383428573608\n",
      "total error:  3.425554096698761\n",
      "training error:  0.922942042350769  and  1.3809677362442017  and  1.1219947338104248\n",
      "total error:  3.4259045124053955\n",
      "training error:  0.8637993335723877  and  1.3143651485443115  and  1.1205735206604004\n",
      "total error:  3.2987380027770996\n",
      "training error:  0.9041601419448853  and  1.3683749437332153  and  1.0749626159667969\n",
      "total error:  3.3474977016448975\n",
      "training error:  0.8850418925285339  and  1.3791332244873047  and  1.0950038433074951\n",
      "total error:  3.3591789603233337\n",
      "training error:  0.8945553302764893  and  1.3920685052871704  and  1.1205729246139526\n",
      "total error:  3.4071967601776123\n",
      "training error:  0.9286313652992249  and  1.426680088043213  and  1.1259760856628418\n",
      "total error:  3.4812875390052795\n",
      "training error:  0.8887788653373718  and  1.317674994468689  and  1.0665161609649658\n",
      "total error:  3.2729700207710266\n",
      "training error:  0.8965655565261841  and  1.4314275979995728  and  1.087674856185913\n",
      "total error:  3.41566801071167\n",
      "training error:  0.9356242418289185  and  1.4679701328277588  and  1.1080121994018555\n",
      "total error:  3.5116065740585327\n",
      "training error:  0.8996623754501343  and  1.5055325031280518  and  1.2722845077514648\n",
      "total error:  3.677479386329651\n",
      "training error:  0.9460927248001099  and  1.5001577138900757  and  1.1066890954971313\n",
      "total error:  3.552939534187317\n",
      "training error:  0.8952997326850891  and  1.3640563488006592  and  1.096222162246704\n",
      "total error:  3.3555782437324524\n",
      "training error:  0.9021537899971008  and  1.3682477474212646  and  1.1123175621032715\n",
      "total error:  3.382719099521637\n",
      "training error:  0.8564069271087646  and  1.4336345195770264  and  1.090388536453247\n",
      "total error:  3.380429983139038\n",
      "training error:  0.9189596772193909  and  1.597883939743042  and  1.158196210861206\n",
      "total error:  3.675039827823639\n",
      "training error:  0.8801616430282593  and  1.396975040435791  and  1.1045130491256714\n",
      "total error:  3.3816497325897217\n",
      "training error:  0.8826239705085754  and  1.3308185338974  and  1.089869499206543\n",
      "total error:  3.3033120036125183\n",
      "training error:  0.9096986651420593  and  1.3940622806549072  and  1.1003942489624023\n",
      "total error:  3.404155194759369\n",
      "training error:  0.8785820603370667  and  1.3367329835891724  and  1.100406289100647\n",
      "total error:  3.315721333026886\n",
      "training error:  0.8595893383026123  and  1.3685569763183594  and  1.095689058303833\n",
      "total error:  3.3238353729248047\n",
      "training error:  0.8744918704032898  and  1.3447303771972656  and  1.0751333236694336\n",
      "total error:  3.294355571269989\n",
      "training error:  0.8879560232162476  and  1.3663231134414673  and  1.1476013660430908\n",
      "total error:  3.4018805027008057\n",
      "training error:  0.8843678832054138  and  1.4401674270629883  and  1.0718932151794434\n",
      "total error:  3.3964285254478455\n",
      "training error:  0.8646152019500732  and  1.424006700515747  and  1.1269758939743042\n",
      "total error:  3.4155977964401245\n",
      "training error:  0.8775957226753235  and  1.4290218353271484  and  1.1009513139724731\n",
      "total error:  3.407568871974945\n",
      "training error:  0.8663123846054077  and  1.3087892532348633  and  1.1420339345932007\n",
      "total error:  3.3171355724334717\n",
      "training error:  0.8952077031135559  and  1.3647124767303467  and  1.0575777292251587\n",
      "total error:  3.3174979090690613\n",
      "training error:  0.8705263733863831  and  1.3647663593292236  and  1.271409273147583\n",
      "total error:  3.5067020058631897\n",
      "training error:  0.8821054100990295  and  1.3889408111572266  and  1.1074986457824707\n",
      "total error:  3.378544867038727\n",
      "training error:  0.8448212742805481  and  1.3251177072525024  and  1.1727936267852783\n",
      "total error:  3.342732608318329\n",
      "training error:  0.9085855484008789  and  1.61040461063385  and  1.1566306352615356\n",
      "total error:  3.6756207942962646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.8733423352241516  and  1.3460475206375122  and  1.047450065612793\n",
      "total error:  3.266839921474457\n",
      "training error:  0.8736662268638611  and  1.412517786026001  and  1.1072731018066406\n",
      "total error:  3.3934571146965027\n",
      "training error:  0.8704147934913635  and  1.528760552406311  and  1.0854637622833252\n",
      "total error:  3.4846391081809998\n",
      "training error:  0.8604089021682739  and  1.314079999923706  and  1.1149547100067139\n",
      "total error:  3.289443612098694\n",
      "training error:  0.8715293407440186  and  1.352165699005127  and  1.0673675537109375\n",
      "total error:  3.291062593460083\n",
      "training error:  0.854900062084198  and  1.3262965679168701  and  1.1466400623321533\n",
      "total error:  3.3278366923332214\n",
      "training error:  0.8601369857788086  and  1.3169162273406982  and  1.0601744651794434\n",
      "total error:  3.23722767829895\n",
      "training error:  0.8648403882980347  and  1.6251816749572754  and  1.1672385931015015\n",
      "total error:  3.6572606563568115\n",
      "training error:  0.8837770819664001  and  1.3427128791809082  and  1.069930076599121\n",
      "total error:  3.2964200377464294\n",
      "training error:  0.918632447719574  and  1.520439624786377  and  1.1989173889160156\n",
      "total error:  3.6379894614219666\n",
      "training error:  0.9390698075294495  and  1.8451952934265137  and  1.1265869140625\n",
      "total error:  3.910852015018463\n",
      "training error:  0.8791415691375732  and  1.3797249794006348  and  1.0817408561706543\n",
      "total error:  3.3406074047088623\n",
      "training error:  0.9036803245544434  and  1.339100956916809  and  1.0973560810089111\n",
      "total error:  3.3401373624801636\n",
      "training error:  0.8686804175376892  and  1.3573081493377686  and  1.1897928714752197\n",
      "total error:  3.4157814383506775\n",
      "training error:  0.8988328576087952  and  1.289062261581421  and  1.074294924736023\n",
      "total error:  3.262190043926239\n",
      "training error:  0.8924096822738647  and  1.3929710388183594  and  1.0651745796203613\n",
      "total error:  3.3505553007125854\n",
      "training error:  0.8803476095199585  and  1.372603416442871  and  1.084426999092102\n",
      "total error:  3.3373780250549316\n",
      "training error:  0.8876554369926453  and  1.508624792098999  and  1.1389111280441284\n",
      "total error:  3.5351913571357727\n",
      "training error:  0.8881807923316956  and  1.3355284929275513  and  1.0764329433441162\n",
      "total error:  3.300142228603363\n",
      "training error:  0.8649564981460571  and  1.3487063646316528  and  1.093045711517334\n",
      "total error:  3.306708574295044\n",
      "training error:  0.8524719476699829  and  1.3240559101104736  and  1.1926735639572144\n",
      "total error:  3.369201421737671\n",
      "training error:  0.8743579387664795  and  1.454518437385559  and  1.1540992259979248\n",
      "total error:  3.4829756021499634\n",
      "training error:  0.8494975566864014  and  1.342301845550537  and  1.1094260215759277\n",
      "total error:  3.301225423812866\n",
      "training error:  0.8594010472297668  and  1.3725214004516602  and  1.0846586227416992\n",
      "total error:  3.316581070423126\n",
      "training error:  0.8428760766983032  and  1.31376051902771  and  1.071800947189331\n",
      "total error:  3.2284375429153442\n",
      "training error:  0.8781390190124512  and  1.3223118782043457  and  1.0742888450622559\n",
      "total error:  3.2747397422790527\n",
      "training error:  0.8590914607048035  and  1.3309214115142822  and  1.042988657951355\n",
      "total error:  3.2330015301704407\n",
      "training error:  0.86351078748703  and  1.299197793006897  and  1.058887004852295\n",
      "total error:  3.221595585346222\n",
      "training error:  0.8530899286270142  and  1.351325511932373  and  1.1166929006576538\n",
      "total error:  3.321108341217041\n",
      "training error:  0.8482521772384644  and  1.3022745847702026  and  1.053429126739502\n",
      "total error:  3.203955888748169\n",
      "training error:  0.878872275352478  and  1.3512732982635498  and  1.0909686088562012\n",
      "total error:  3.321114182472229\n",
      "training error:  0.8494529724121094  and  1.322153091430664  and  1.0695561170578003\n",
      "total error:  3.2411621809005737\n",
      "training error:  0.8566863536834717  and  1.3814882040023804  and  1.0891703367233276\n",
      "total error:  3.3273448944091797\n",
      "training error:  0.8480442762374878  and  1.3144536018371582  and  1.1641643047332764\n",
      "total error:  3.3266621828079224\n",
      "training error:  0.8363907337188721  and  1.3269872665405273  and  1.0412343740463257\n",
      "total error:  3.204612374305725\n",
      "training error:  0.8694416284561157  and  1.3686885833740234  and  1.0831654071807861\n",
      "total error:  3.3212956190109253\n",
      "training error:  0.8581126928329468  and  1.3080050945281982  and  1.0742977857589722\n",
      "total error:  3.240415573120117\n",
      "training error:  0.8569152355194092  and  1.3273086547851562  and  1.0887196063995361\n",
      "total error:  3.2729434967041016\n",
      "training error:  0.8405567407608032  and  1.3384809494018555  and  1.0493369102478027\n",
      "total error:  3.2283746004104614\n",
      "training error:  0.8420025110244751  and  1.4649991989135742  and  1.1217105388641357\n",
      "total error:  3.428712248802185\n",
      "training error:  0.8239457607269287  and  1.35301673412323  and  1.0926419496536255\n",
      "total error:  3.269604444503784\n",
      "training error:  0.871146559715271  and  1.4840724468231201  and  1.100278615951538\n",
      "total error:  3.455497622489929\n",
      "training error:  0.8696950674057007  and  1.3265109062194824  and  1.0352392196655273\n",
      "total error:  3.2314451932907104\n",
      "training error:  0.8498940467834473  and  1.3335613012313843  and  1.0784938335418701\n",
      "total error:  3.2619491815567017\n",
      "training error:  0.8391011357307434  and  1.3696932792663574  and  1.1127218008041382\n",
      "total error:  3.321516215801239\n",
      "training error:  0.8575159311294556  and  1.3398065567016602  and  1.1084436178207397\n",
      "total error:  3.3057661056518555\n",
      "training error:  0.8684699535369873  and  1.4533882141113281  and  1.18259596824646\n",
      "total error:  3.5044541358947754\n",
      "training error:  0.9253843426704407  and  1.3874655961990356  and  1.1614402532577515\n",
      "total error:  3.474290192127228\n",
      "training error:  0.854080319404602  and  1.4075160026550293  and  1.1509042978286743\n",
      "total error:  3.4125006198883057\n",
      "training error:  0.8706926107406616  and  1.3430633544921875  and  1.0802003145217896\n",
      "total error:  3.2939562797546387\n",
      "training error:  0.866482138633728  and  1.378454566001892  and  1.1493895053863525\n",
      "total error:  3.3943262100219727\n",
      "training error:  0.8405094742774963  and  1.3632301092147827  and  1.126873254776001\n",
      "total error:  3.33061283826828\n",
      "training error:  0.8692282438278198  and  1.3541457653045654  and  1.1148335933685303\n",
      "total error:  3.3382076025009155\n",
      "training error:  0.8556931614875793  and  1.6658689975738525  and  1.0926077365875244\n",
      "total error:  3.6141698956489563\n",
      "training error:  0.8677753210067749  and  1.3303437232971191  and  1.0679872035980225\n",
      "total error:  3.2661062479019165\n",
      "training error:  0.8482364416122437  and  1.4480576515197754  and  1.1787575483322144\n",
      "total error:  3.4750516414642334\n",
      "training error:  0.8809951543807983  and  1.4747304916381836  and  1.0499181747436523\n",
      "total error:  3.4056438207626343\n",
      "training error:  0.8527153730392456  and  1.4719159603118896  and  1.091284990310669\n",
      "total error:  3.415916323661804\n",
      "training error:  0.8550065755844116  and  1.2745678424835205  and  1.0715175867080688\n",
      "total error:  3.201092004776001\n",
      "training error:  0.8476152420043945  and  1.331968069076538  and  1.0745301246643066\n",
      "total error:  3.2541134357452393\n",
      "training error:  0.8197628259658813  and  1.3217296600341797  and  1.1070756912231445\n",
      "total error:  3.2485681772232056\n",
      "training error:  0.8695513010025024  and  1.4532431364059448  and  1.236100196838379\n",
      "total error:  3.558894634246826\n",
      "training error:  0.8523434400558472  and  1.425835132598877  and  1.170886754989624\n",
      "total error:  3.449065327644348\n",
      "training error:  0.8452949523925781  and  1.3599764108657837  and  1.0884978771209717\n",
      "total error:  3.2937692403793335\n",
      "training error:  0.8447799682617188  and  1.3036460876464844  and  1.162449598312378\n",
      "total error:  3.310875654220581\n",
      "training error:  0.8223316669464111  and  1.360403060913086  and  1.052008867263794\n",
      "total error:  3.234743595123291\n",
      "training error:  0.8700039982795715  and  1.399810552597046  and  1.2294354438781738\n",
      "total error:  3.4992499947547913\n",
      "training error:  0.8448849320411682  and  1.286767601966858  and  1.1009211540222168\n",
      "total error:  3.232573688030243\n",
      "training error:  0.8608357906341553  and  1.8514378070831299  and  1.221030592918396\n",
      "total error:  3.933304190635681\n",
      "training error:  0.8399492502212524  and  1.3072502613067627  and  1.070661187171936\n",
      "total error:  3.217860698699951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.8887952566146851  and  1.4616400003433228  and  1.1018723249435425\n",
      "total error:  3.4523075819015503\n",
      "training error:  0.8542017340660095  and  1.3466122150421143  and  1.0942734479904175\n",
      "total error:  3.2950873970985413\n",
      "training error:  0.8272950649261475  and  1.3497035503387451  and  1.2198951244354248\n",
      "total error:  3.3968937397003174\n",
      "training error:  0.827559769153595  and  1.3239802122116089  and  1.1022276878356934\n",
      "total error:  3.253767669200897\n",
      "training error:  0.8226279020309448  and  1.4035546779632568  and  1.2616151571273804\n",
      "total error:  3.487797737121582\n",
      "training error:  0.8174234628677368  and  1.3652420043945312  and  1.0715175867080688\n",
      "total error:  3.254183053970337\n",
      "training error:  0.8606151342391968  and  1.3173766136169434  and  1.0706169605255127\n",
      "total error:  3.248608708381653\n",
      "training error:  0.8379865884780884  and  1.2641453742980957  and  1.1207870244979858\n",
      "total error:  3.22291898727417\n",
      "training error:  0.8389513492584229  and  1.4057395458221436  and  1.084745168685913\n",
      "total error:  3.3294360637664795\n",
      "training error:  0.8232600688934326  and  1.3031127452850342  and  1.0580248832702637\n",
      "total error:  3.1843976974487305\n",
      "training error:  0.8434596657752991  and  1.319928526878357  and  1.0307788848876953\n",
      "total error:  3.1941670775413513\n",
      "training error:  0.8340427875518799  and  1.3451635837554932  and  1.0474205017089844\n",
      "total error:  3.2266268730163574\n",
      "training error:  0.8377593755722046  and  1.3278512954711914  and  1.0849223136901855\n",
      "total error:  3.2505329847335815\n",
      "training error:  0.8276599049568176  and  1.3007838726043701  and  1.086742877960205\n",
      "total error:  3.215186655521393\n",
      "training error:  0.8463859558105469  and  1.3568156957626343  and  1.089786171913147\n",
      "total error:  3.292987823486328\n",
      "training error:  0.8443471789360046  and  1.3074884414672852  and  1.1584279537200928\n",
      "total error:  3.3102635741233826\n",
      "training error:  0.8421989679336548  and  1.3203606605529785  and  1.0816283226013184\n",
      "total error:  3.2441879510879517\n",
      "training error:  0.8302907943725586  and  1.4251669645309448  and  1.0388604402542114\n",
      "total error:  3.294318199157715\n",
      "training error:  0.8318477272987366  and  1.326819896697998  and  1.0452649593353271\n",
      "total error:  3.2039325833320618\n",
      "training error:  0.830633282661438  and  1.3928546905517578  and  1.091760516166687\n",
      "total error:  3.315248489379883\n",
      "training error:  0.8341760635375977  and  1.305253028869629  and  1.0578713417053223\n",
      "total error:  3.197300434112549\n",
      "training error:  0.8372564315795898  and  1.2940795421600342  and  1.05568265914917\n",
      "total error:  3.187018632888794\n",
      "training error:  0.8673999309539795  and  1.4546012878417969  and  1.1391501426696777\n",
      "total error:  3.461151361465454\n",
      "training error:  0.832201361656189  and  1.3364975452423096  and  1.1621180772781372\n",
      "total error:  3.3308169841766357\n",
      "training error:  0.824347972869873  and  1.3943679332733154  and  1.1076297760009766\n",
      "total error:  3.326345682144165\n",
      "training error:  0.836881697177887  and  1.336987018585205  and  1.124014139175415\n",
      "total error:  3.297882854938507\n",
      "training error:  0.834293782711029  and  1.471116065979004  and  1.0718363523483276\n",
      "total error:  3.3772462010383606\n",
      "training error:  0.8409179449081421  and  1.3154629468917847  and  1.2002308368682861\n",
      "total error:  3.356611728668213\n",
      "training error:  0.8398953080177307  and  1.3426012992858887  and  1.0514249801635742\n",
      "total error:  3.2339215874671936\n",
      "training error:  0.887519121170044  and  1.4280266761779785  and  1.1426042318344116\n",
      "total error:  3.458150029182434\n",
      "training error:  0.8368345499038696  and  1.3041071891784668  and  1.0567710399627686\n",
      "total error:  3.197712779045105\n",
      "training error:  0.8333519697189331  and  1.348545789718628  and  1.092777967453003\n",
      "total error:  3.274675726890564\n",
      "training error:  0.8234039545059204  and  1.3154220581054688  and  1.0435450077056885\n",
      "total error:  3.1823710203170776\n",
      "training error:  0.8322312831878662  and  1.282362699508667  and  1.1173624992370605\n",
      "total error:  3.2319564819335938\n",
      "training error:  0.8422881960868835  and  1.5355745553970337  and  1.0643587112426758\n",
      "total error:  3.442221462726593\n",
      "training error:  0.8159195184707642  and  1.3207659721374512  and  1.0633412599563599\n",
      "total error:  3.200026750564575\n",
      "training error:  0.8051321506500244  and  1.3637498617172241  and  1.0459909439086914\n",
      "total error:  3.21487295627594\n",
      "training error:  0.8014611601829529  and  1.36784029006958  and  1.1238731145858765\n",
      "total error:  3.2931745648384094\n",
      "training error:  0.8218821287155151  and  1.3148362636566162  and  1.0941224098205566\n",
      "total error:  3.230840802192688\n",
      "training error:  0.8678137063980103  and  1.3146696090698242  and  1.0942058563232422\n",
      "total error:  3.2766891717910767\n",
      "training error:  0.818927526473999  and  1.3518517017364502  and  1.1058664321899414\n",
      "total error:  3.2766456604003906\n",
      "training error:  0.8235887289047241  and  1.2796070575714111  and  1.0340933799743652\n",
      "total error:  3.1372891664505005\n",
      "training error:  0.8000867366790771  and  1.3150546550750732  and  1.0400300025939941\n",
      "total error:  3.1551713943481445\n",
      "training error:  0.8175957202911377  and  1.2754600048065186  and  1.1049997806549072\n",
      "total error:  3.1980555057525635\n",
      "training error:  0.8179512619972229  and  1.4034404754638672  and  1.0481786727905273\n",
      "total error:  3.2695704102516174\n",
      "training error:  0.8213430047035217  and  1.275179147720337  and  1.074798822402954\n",
      "total error:  3.1713209748268127\n",
      "training error:  0.8358248472213745  and  1.3306028842926025  and  1.0850149393081665\n",
      "total error:  3.2514426708221436\n",
      "training error:  0.8039891719818115  and  1.2922064065933228  and  1.0405259132385254\n",
      "total error:  3.1367214918136597\n",
      "training error:  0.8321292400360107  and  1.2972813844680786  and  1.0402233600616455\n",
      "total error:  3.169633984565735\n",
      "training error:  0.8159999251365662  and  1.3074883222579956  and  1.0501182079315186\n",
      "total error:  3.1736064553260803\n",
      "training error:  0.8257563710212708  and  1.3586134910583496  and  1.057615876197815\n",
      "total error:  3.2419857382774353\n",
      "training error:  0.8017871379852295  and  1.2985018491744995  and  1.1096487045288086\n",
      "total error:  3.2099376916885376\n",
      "training error:  0.8380260467529297  and  1.4173134565353394  and  1.049443006515503\n",
      "total error:  3.304782509803772\n",
      "training error:  0.8298994302749634  and  1.3286628723144531  and  1.063331127166748\n",
      "total error:  3.2218934297561646\n",
      "training error:  0.8347569108009338  and  1.3149175643920898  and  1.0548386573791504\n",
      "total error:  3.204513132572174\n",
      "training error:  0.8559247255325317  and  1.3693867921829224  and  1.051391363143921\n",
      "total error:  3.276702880859375\n",
      "training error:  0.8072670698165894  and  1.2861213684082031  and  1.0636053085327148\n",
      "total error:  3.1569937467575073\n",
      "training error:  0.8595291376113892  and  1.531752109527588  and  1.143341302871704\n",
      "total error:  3.534622550010681\n",
      "training error:  0.8384044766426086  and  1.3308212757110596  and  1.2204005718231201\n",
      "total error:  3.3896263241767883\n",
      "training error:  0.811475396156311  and  1.299094796180725  and  1.0730178356170654\n",
      "total error:  3.1835880279541016\n",
      "training error:  0.8283330202102661  and  1.2899127006530762  and  1.0806806087493896\n",
      "total error:  3.198926329612732\n",
      "training error:  0.8208405375480652  and  1.2721784114837646  and  1.0633689165115356\n",
      "total error:  3.1563878655433655\n",
      "training error:  0.8189793825149536  and  1.2919378280639648  and  1.057748556137085\n",
      "total error:  3.1686657667160034\n",
      "training error:  0.8249757289886475  and  1.288848876953125  and  1.0101789236068726\n",
      "total error:  3.124003529548645\n",
      "training error:  0.833935022354126  and  1.4106935262680054  and  1.114915132522583\n",
      "total error:  3.3595436811447144\n",
      "training error:  0.8047432899475098  and  1.282850742340088  and  1.0556468963623047\n",
      "total error:  3.1432409286499023\n",
      "training error:  0.8344662189483643  and  1.3278639316558838  and  1.1403486728668213\n",
      "total error:  3.3026788234710693\n",
      "training error:  0.8501518964767456  and  1.3681752681732178  and  1.0598275661468506\n",
      "total error:  3.278154730796814\n",
      "training error:  0.8431328535079956  and  1.3203076124191284  and  1.1014832258224487\n",
      "total error:  3.2649236917495728\n",
      "training error:  0.813999593257904  and  1.3024746179580688  and  1.081531286239624\n",
      "total error:  3.198005497455597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.8320251107215881  and  1.2982938289642334  and  1.0503575801849365\n",
      "total error:  3.180676519870758\n",
      "training error:  0.7962765693664551  and  1.3224716186523438  and  1.046181559562683\n",
      "total error:  3.164929747581482\n",
      "training error:  0.830367922782898  and  1.2858538627624512  and  1.0505887269973755\n",
      "total error:  3.1668105125427246\n",
      "training error:  0.8045377731323242  and  1.3201203346252441  and  1.1112630367279053\n",
      "total error:  3.2359211444854736\n",
      "training error:  0.8032628893852234  and  1.282059669494629  and  1.0389354228973389\n",
      "total error:  3.124257981777191\n",
      "training error:  0.8475581407546997  and  1.342742681503296  and  1.20307195186615\n",
      "total error:  3.3933727741241455\n",
      "training error:  0.8247844576835632  and  1.4558155536651611  and  1.141068935394287\n",
      "total error:  3.4216689467430115\n",
      "training error:  0.8122880458831787  and  1.275554895401001  and  1.0571002960205078\n",
      "total error:  3.1449432373046875\n",
      "training error:  0.799368679523468  and  1.3322196006774902  and  1.127602458000183\n",
      "total error:  3.2591907382011414\n",
      "training error:  0.8261730670928955  and  1.5267292261123657  and  1.0691828727722168\n",
      "total error:  3.422085165977478\n",
      "training error:  0.8154624700546265  and  1.2768443822860718  and  1.06888747215271\n",
      "total error:  3.161194324493408\n",
      "training error:  0.8293250203132629  and  1.4316359758377075  and  1.0899710655212402\n",
      "total error:  3.3509320616722107\n",
      "training error:  0.8402819037437439  and  1.3495069742202759  and  1.0676876306533813\n",
      "total error:  3.257476508617401\n",
      "training error:  0.7983080148696899  and  1.3269329071044922  and  1.1138474941253662\n",
      "total error:  3.2390884160995483\n",
      "training error:  0.8221207857131958  and  1.33437180519104  and  1.0823750495910645\n",
      "total error:  3.2388676404953003\n",
      "training error:  0.8139008283615112  and  1.2769615650177002  and  1.0464625358581543\n",
      "total error:  3.1373249292373657\n",
      "training error:  0.8144681453704834  and  1.309697151184082  and  1.11244535446167\n",
      "total error:  3.2366106510162354\n",
      "training error:  0.7944270968437195  and  1.2777290344238281  and  1.0695469379425049\n",
      "total error:  3.1417030692100525\n",
      "training error:  0.8017244338989258  and  1.3551368713378906  and  1.0561479330062866\n",
      "total error:  3.213009238243103\n",
      "training error:  0.825771689414978  and  1.700217366218567  and  1.4488568305969238\n",
      "total error:  3.9748458862304688\n",
      "training error:  0.8133056163787842  and  1.2820429801940918  and  1.048545002937317\n",
      "total error:  3.143893599510193\n",
      "training error:  0.8258247375488281  and  1.3063294887542725  and  1.067537546157837\n",
      "total error:  3.1996917724609375\n",
      "training error:  0.8467810750007629  and  1.3227839469909668  and  1.0589406490325928\n",
      "total error:  3.2285056710243225\n",
      "training error:  0.8096156120300293  and  1.3553130626678467  and  1.0929747819900513\n",
      "total error:  3.2579034566879272\n",
      "training error:  0.8238166570663452  and  1.3748526573181152  and  1.0800514221191406\n",
      "total error:  3.278720736503601\n",
      "training error:  0.8007032871246338  and  1.30800461769104  and  1.0842251777648926\n",
      "total error:  3.1929330825805664\n",
      "training error:  0.8400453329086304  and  1.3226640224456787  and  1.0797100067138672\n",
      "total error:  3.2424193620681763\n",
      "training error:  0.792156457901001  and  1.3007533550262451  and  1.092931866645813\n",
      "total error:  3.185841679573059\n",
      "training error:  0.8158060312271118  and  1.3760910034179688  and  1.0519163608551025\n",
      "total error:  3.243813395500183\n",
      "training error:  0.7973599433898926  and  1.281719446182251  and  1.001417636871338\n",
      "total error:  3.0804970264434814\n",
      "training error:  0.8044619560241699  and  1.2688449621200562  and  1.052872896194458\n",
      "total error:  3.126179814338684\n",
      "training error:  0.7875852584838867  and  1.2854137420654297  and  1.0923994779586792\n",
      "total error:  3.1653984785079956\n",
      "training error:  0.7930288314819336  and  1.3118866682052612  and  1.06905996799469\n",
      "total error:  3.1739754676818848\n",
      "training error:  0.797754168510437  and  1.5972373485565186  and  1.2120822668075562\n",
      "total error:  3.6070737838745117\n",
      "training error:  0.7934873104095459  and  1.4505119323730469  and  1.0535273551940918\n",
      "total error:  3.2975265979766846\n",
      "training error:  0.7848105430603027  and  1.291325330734253  and  1.1957027912139893\n",
      "total error:  3.271838665008545\n",
      "training error:  0.8064740896224976  and  1.3018203973770142  and  1.1012846231460571\n",
      "total error:  3.209579110145569\n",
      "training error:  0.8068426251411438  and  1.2689270973205566  and  1.040245771408081\n",
      "total error:  3.1160154938697815\n",
      "training error:  0.7829494476318359  and  1.277552843093872  and  1.033165454864502\n",
      "total error:  3.09366774559021\n",
      "training error:  0.8091106414794922  and  1.279625415802002  and  1.1026922464370728\n",
      "total error:  3.191428303718567\n",
      "training error:  0.8321035504341125  and  1.4298231601715088  and  1.1048598289489746\n",
      "total error:  3.366786539554596\n",
      "training error:  0.7945200204849243  and  1.2569963932037354  and  1.012566328048706\n",
      "total error:  3.0640827417373657\n",
      "training error:  0.7897758483886719  and  1.3084285259246826  and  1.1310701370239258\n",
      "total error:  3.2292745113372803\n",
      "training error:  0.7944359183311462  and  1.473301887512207  and  1.1298699378967285\n",
      "total error:  3.397607743740082\n",
      "training error:  0.8380407691001892  and  1.3119407892227173  and  1.0584359169006348\n",
      "total error:  3.2084174752235413\n",
      "training error:  0.8162226676940918  and  1.399604320526123  and  1.1411640644073486\n",
      "total error:  3.3569910526275635\n",
      "training error:  0.8077633380889893  and  1.350484848022461  and  1.0747814178466797\n",
      "total error:  3.23302960395813\n",
      "training error:  0.8132925033569336  and  1.3014311790466309  and  1.0288617610931396\n",
      "total error:  3.143585443496704\n",
      "training error:  0.7877403497695923  and  1.3053953647613525  and  1.1768195629119873\n",
      "total error:  3.269955277442932\n",
      "training error:  0.7995645403862  and  1.2723541259765625  and  1.147188663482666\n",
      "total error:  3.2191073298454285\n",
      "training error:  0.7946397066116333  and  1.239691972732544  and  1.0244985818862915\n",
      "total error:  3.0588302612304688\n",
      "training error:  0.8115811347961426  and  1.396045207977295  and  1.1283451318740845\n",
      "total error:  3.335971474647522\n",
      "training error:  0.8029602766036987  and  1.2584913969039917  and  1.06087064743042\n",
      "total error:  3.1223223209381104\n",
      "training error:  0.7780313491821289  and  1.5515179634094238  and  1.0530561208724976\n",
      "total error:  3.3826054334640503\n",
      "training error:  0.8193156719207764  and  1.4690076112747192  and  1.0381851196289062\n",
      "total error:  3.326508402824402\n",
      "training error:  0.8024342060089111  and  1.315366268157959  and  1.148242712020874\n",
      "total error:  3.266043186187744\n",
      "training error:  0.7923453450202942  and  1.265791654586792  and  1.024346113204956\n",
      "total error:  3.0824831128120422\n",
      "training error:  0.7983362674713135  and  1.3878179788589478  and  1.0321978330612183\n",
      "total error:  3.2183520793914795\n",
      "training error:  0.8145099878311157  and  1.2886509895324707  and  1.1279200315475464\n",
      "total error:  3.231081008911133\n",
      "training error:  0.8075989484786987  and  1.3475816249847412  and  1.1001158952713013\n",
      "total error:  3.255296468734741\n",
      "training error:  0.795727014541626  and  1.2580794095993042  and  1.0220720767974854\n",
      "total error:  3.0758785009384155\n",
      "training error:  0.8255462050437927  and  1.7858920097351074  and  1.1042320728302002\n",
      "total error:  3.7156702876091003\n",
      "training error:  0.7963739633560181  and  1.264981746673584  and  1.0576775074005127\n",
      "total error:  3.1190332174301147\n",
      "training error:  0.7906781435012817  and  1.3378297090530396  and  1.0244746208190918\n",
      "total error:  3.152982473373413\n",
      "training error:  0.8199808597564697  and  1.3837406635284424  and  1.0921587944030762\n",
      "total error:  3.2958803176879883\n",
      "training error:  0.7836668491363525  and  1.288707971572876  and  1.068732738494873\n",
      "total error:  3.1411075592041016\n",
      "training error:  0.800412654876709  and  1.331453800201416  and  1.0899345874786377\n",
      "total error:  3.2218010425567627\n",
      "training error:  0.8067260980606079  and  1.284670352935791  and  1.1341466903686523\n",
      "total error:  3.2255431413650513\n",
      "training error:  0.7927649021148682  and  1.2654802799224854  and  1.0652143955230713\n",
      "total error:  3.123459577560425\n",
      "training error:  0.8149039149284363  and  1.283540964126587  and  1.0474658012390137\n",
      "total error:  3.145910680294037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.8328872919082642  and  1.4739325046539307  and  1.2992701530456543\n",
      "total error:  3.606089949607849\n",
      "training error:  0.7959039211273193  and  1.271348476409912  and  1.0765241384506226\n",
      "total error:  3.143776535987854\n",
      "training error:  0.8091904520988464  and  1.345489263534546  and  1.1189628839492798\n",
      "total error:  3.273642599582672\n",
      "training error:  0.8437836766242981  and  1.3231785297393799  and  1.102539300918579\n",
      "total error:  3.269501507282257\n",
      "training error:  0.8123223185539246  and  1.3558140993118286  and  1.0569349527359009\n",
      "total error:  3.225071370601654\n",
      "training error:  0.8081828355789185  and  1.3036737442016602  and  1.0146287679672241\n",
      "total error:  3.1264853477478027\n",
      "training error:  0.7894676327705383  and  1.3399189710617065  and  1.1554875373840332\n",
      "total error:  3.284874141216278\n",
      "training error:  0.7794038653373718  and  1.3236442804336548  and  1.0555336475372314\n",
      "total error:  3.158581793308258\n",
      "training error:  0.7844398617744446  and  1.2535954713821411  and  1.0189971923828125\n",
      "total error:  3.057032525539398\n",
      "training error:  0.7929489016532898  and  1.3742055892944336  and  1.2637617588043213\n",
      "total error:  3.4309162497520447\n",
      "training error:  0.7615512609481812  and  1.256525993347168  and  1.0225136280059814\n",
      "total error:  3.0405908823013306\n",
      "training error:  0.7680051326751709  and  1.3044655323028564  and  1.1682777404785156\n",
      "total error:  3.240748405456543\n",
      "training error:  0.79081791639328  and  1.2965161800384521  and  1.0442394018173218\n",
      "total error:  3.131573498249054\n",
      "training error:  0.7676239013671875  and  1.2647231817245483  and  1.0309745073318481\n",
      "total error:  3.063321590423584\n",
      "training error:  0.7991662621498108  and  1.2949182987213135  and  1.0498929023742676\n",
      "total error:  3.143977463245392\n",
      "training error:  0.7985314130783081  and  1.3829996585845947  and  1.0200530290603638\n",
      "total error:  3.2015841007232666\n",
      "training error:  0.7598446607589722  and  1.3233659267425537  and  1.0314302444458008\n",
      "total error:  3.1146408319473267\n",
      "training error:  0.7636643648147583  and  1.2843995094299316  and  1.0237406492233276\n",
      "total error:  3.0718045234680176\n",
      "training error:  0.8049384355545044  and  1.3160518407821655  and  1.0743327140808105\n",
      "total error:  3.1953229904174805\n",
      "training error:  0.7617893218994141  and  1.2764374017715454  and  1.0026829242706299\n",
      "total error:  3.0409096479415894\n",
      "training error:  0.7912260293960571  and  1.306764841079712  and  1.0771353244781494\n",
      "total error:  3.1751261949539185\n",
      "training error:  0.783793032169342  and  1.3080880641937256  and  1.0251076221466064\n",
      "total error:  3.116988718509674\n",
      "training error:  0.8060692548751831  and  1.3555184602737427  and  1.0578478574752808\n",
      "total error:  3.2194355726242065\n",
      "training error:  0.794856071472168  and  1.2857542037963867  and  1.0395724773406982\n",
      "total error:  3.120182752609253\n",
      "training error:  0.7744629979133606  and  1.370992660522461  and  1.0722492933273315\n",
      "total error:  3.217704951763153\n",
      "training error:  0.7794142365455627  and  1.3934097290039062  and  1.0246996879577637\n",
      "total error:  3.1975236535072327\n",
      "training error:  0.7690122723579407  and  1.2837556600570679  and  1.0311579704284668\n",
      "total error:  3.0839259028434753\n",
      "training error:  0.7558270692825317  and  1.4236679077148438  and  1.047008991241455\n",
      "total error:  3.2265039682388306\n",
      "training error:  0.8049468398094177  and  1.333740234375  and  1.1031553745269775\n",
      "total error:  3.2418424487113953\n",
      "training error:  0.7577667236328125  and  1.4041614532470703  and  1.073179006576538\n",
      "total error:  3.235107183456421\n",
      "training error:  0.7815083861351013  and  1.223939061164856  and  1.0372838973999023\n",
      "total error:  3.0427313446998596\n",
      "training error:  0.7687546610832214  and  1.248365879058838  and  1.0291856527328491\n",
      "total error:  3.0463061928749084\n",
      "training error:  0.8222960829734802  and  1.8854650259017944  and  1.2015278339385986\n",
      "total error:  3.9092889428138733\n",
      "training error:  0.7683488130569458  and  1.3205947875976562  and  1.0131902694702148\n",
      "total error:  3.102133870124817\n",
      "training error:  0.7493653297424316  and  1.2889585494995117  and  1.0773024559020996\n",
      "total error:  3.115626335144043\n",
      "training error:  0.7772911787033081  and  1.3067762851715088  and  1.0366251468658447\n",
      "total error:  3.1206926107406616\n",
      "training error:  0.7673901319503784  and  1.2111098766326904  and  1.0015555620193481\n",
      "total error:  2.980055570602417\n",
      "training error:  0.7688561677932739  and  1.2694392204284668  and  1.0964410305023193\n",
      "total error:  3.13473641872406\n",
      "training error:  0.8033326864242554  and  1.4540040493011475  and  1.101802110671997\n",
      "total error:  3.3591388463974\n",
      "training error:  0.8124275207519531  and  1.3316928148269653  and  1.0654215812683105\n",
      "total error:  3.209541916847229\n",
      "training error:  0.7665403485298157  and  1.4023447036743164  and  1.1540627479553223\n",
      "total error:  3.3229478001594543\n",
      "training error:  0.7704117298126221  and  1.2797108888626099  and  1.0498521327972412\n",
      "total error:  3.099974751472473\n",
      "training error:  0.7702001929283142  and  1.25282883644104  and  1.0362414121627808\n",
      "total error:  3.059270441532135\n",
      "training error:  0.7663585543632507  and  1.4359066486358643  and  1.0527632236480713\n",
      "total error:  3.2550284266471863\n",
      "training error:  0.7542721033096313  and  1.2661359310150146  and  1.0578473806381226\n",
      "total error:  3.0782554149627686\n",
      "training error:  0.7819214463233948  and  1.3119533061981201  and  1.0338623523712158\n",
      "total error:  3.1277371048927307\n",
      "training error:  0.7876348495483398  and  1.4106431007385254  and  1.1833717823028564\n",
      "total error:  3.3816497325897217\n",
      "training error:  0.7671475410461426  and  1.290704607963562  and  1.0916390419006348\n",
      "total error:  3.1494911909103394\n",
      "training error:  0.7872210144996643  and  1.3102567195892334  and  1.0356944799423218\n",
      "total error:  3.1331722140312195\n",
      "training error:  0.7742897272109985  and  1.3190762996673584  and  1.0971574783325195\n",
      "total error:  3.1905235052108765\n",
      "training error:  0.7811751365661621  and  1.3769199848175049  and  1.2177796363830566\n",
      "total error:  3.3758747577667236\n",
      "training error:  0.7757275104522705  and  1.5158538818359375  and  1.098313570022583\n",
      "total error:  3.389894962310791\n",
      "training error:  0.7864738702774048  and  1.2861835956573486  and  1.0012246370315552\n",
      "total error:  3.0738821029663086\n",
      "training error:  0.7832374572753906  and  1.3742291927337646  and  1.0749274492263794\n",
      "total error:  3.2323940992355347\n",
      "training error:  0.770201563835144  and  1.2681300640106201  and  1.011537790298462\n",
      "total error:  3.049869418144226\n",
      "training error:  0.7730722427368164  and  1.2911220788955688  and  1.0390527248382568\n",
      "total error:  3.103247046470642\n",
      "training error:  0.7664847373962402  and  1.2634459733963013  and  1.0168004035949707\n",
      "total error:  3.046731114387512\n",
      "training error:  0.7642336487770081  and  1.3030351400375366  and  1.0784549713134766\n",
      "total error:  3.1457237601280212\n",
      "training error:  0.7762560248374939  and  1.2787325382232666  and  1.026221513748169\n",
      "total error:  3.0812100768089294\n",
      "training error:  0.744172215461731  and  1.249367356300354  and  1.003192663192749\n",
      "total error:  2.996732234954834\n",
      "training error:  0.7524487376213074  and  1.259528398513794  and  1.034945011138916\n",
      "total error:  3.0469221472740173\n",
      "training error:  0.7973566055297852  and  1.3635292053222656  and  1.1544208526611328\n",
      "total error:  3.3153066635131836\n",
      "training error:  0.7757625579833984  and  1.343458890914917  and  1.0527093410491943\n",
      "total error:  3.1719307899475098\n",
      "training error:  0.7707940936088562  and  1.2562716007232666  and  1.0157923698425293\n",
      "total error:  3.042858064174652\n",
      "training error:  0.7859315872192383  and  1.2770370244979858  and  1.018564224243164\n",
      "total error:  3.081532835960388\n",
      "training error:  0.741701602935791  and  1.2517447471618652  and  1.0106521844863892\n",
      "total error:  3.0040985345840454\n",
      "training error:  0.7837249636650085  and  1.253342628479004  and  1.0454437732696533\n",
      "total error:  3.0825113654136658\n",
      "training error:  0.7381467819213867  and  1.2647424936294556  and  1.039534568786621\n",
      "total error:  3.0424238443374634\n",
      "training error:  0.7445465326309204  and  1.2906198501586914  and  1.0363154411315918\n",
      "total error:  3.0714818239212036\n",
      "training error:  0.7594343423843384  and  1.2571474313735962  and  1.168076753616333\n",
      "total error:  3.1846585273742676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7435681223869324  and  1.245274305343628  and  1.0003864765167236\n",
      "total error:  2.989228904247284\n",
      "training error:  0.7498725652694702  and  1.2932714223861694  and  1.0567829608917236\n",
      "total error:  3.0999269485473633\n",
      "training error:  0.7924160957336426  and  1.2645986080169678  and  1.0653386116027832\n",
      "total error:  3.1223533153533936\n",
      "training error:  0.7511671781539917  and  1.326938271522522  and  0.9964550137519836\n",
      "total error:  3.0745604634284973\n",
      "training error:  0.7536033391952515  and  1.250058889389038  and  1.0413758754730225\n",
      "total error:  3.045038104057312\n",
      "training error:  0.7621943950653076  and  1.2310644388198853  and  1.055194616317749\n",
      "total error:  3.048453450202942\n",
      "training error:  0.7730115652084351  and  1.2368273735046387  and  1.015634298324585\n",
      "total error:  3.0254732370376587\n",
      "training error:  0.747424840927124  and  1.310813307762146  and  1.0249176025390625\n",
      "total error:  3.0831557512283325\n",
      "training error:  0.7375009059906006  and  1.249959945678711  and  1.0278615951538086\n",
      "total error:  3.01532244682312\n",
      "training error:  0.7550384998321533  and  1.2272467613220215  and  1.0220750570297241\n",
      "total error:  3.004360318183899\n",
      "training error:  0.755145251750946  and  1.3680174350738525  and  1.057377815246582\n",
      "total error:  3.1805405020713806\n",
      "training error:  0.7615137696266174  and  1.301269292831421  and  1.0331847667694092\n",
      "total error:  3.0959678292274475\n",
      "training error:  0.7619606256484985  and  1.2071211338043213  and  1.0731160640716553\n",
      "total error:  3.042197823524475\n",
      "training error:  0.7686981558799744  and  1.2728004455566406  and  1.0601160526275635\n",
      "total error:  3.1016146540641785\n",
      "training error:  0.7712095975875854  and  1.238082766532898  and  1.0232096910476685\n",
      "total error:  3.032502055168152\n",
      "training error:  0.7599725127220154  and  1.3068211078643799  and  1.022059440612793\n",
      "total error:  3.0888530611991882\n",
      "training error:  0.7709318995475769  and  1.294978380203247  and  0.9816543459892273\n",
      "total error:  3.0475646257400513\n",
      "training error:  0.7441725134849548  and  1.2191715240478516  and  0.9856078028678894\n",
      "total error:  2.948951840400696\n",
      "training error:  0.7884445786476135  and  1.553497314453125  and  1.031545877456665\n",
      "total error:  3.3734877705574036\n",
      "training error:  0.7571901082992554  and  1.24856698513031  and  1.0703442096710205\n",
      "total error:  3.076101303100586\n",
      "training error:  0.7776432037353516  and  1.2427799701690674  and  1.028350591659546\n",
      "total error:  3.048773765563965\n",
      "training error:  0.7662743330001831  and  1.2381619215011597  and  1.062450647354126\n",
      "total error:  3.0668869018554688\n",
      "training error:  0.7638633251190186  and  1.2598257064819336  and  1.0835036039352417\n",
      "total error:  3.107192635536194\n",
      "training error:  0.7565665245056152  and  1.2603302001953125  and  1.0852549076080322\n",
      "total error:  3.10215163230896\n",
      "training error:  0.7718044519424438  and  1.722454309463501  and  1.199730634689331\n",
      "total error:  3.693989396095276\n",
      "training error:  0.7837032675743103  and  1.3025351762771606  and  1.0736689567565918\n",
      "total error:  3.1599074006080627\n",
      "training error:  0.7479596138000488  and  1.2379391193389893  and  1.0312113761901855\n",
      "total error:  3.0171101093292236\n",
      "training error:  0.7655008435249329  and  1.2763631343841553  and  1.1061465740203857\n",
      "total error:  3.148010551929474\n",
      "training error:  0.7649348378181458  and  1.242103099822998  and  1.0106744766235352\n",
      "total error:  3.017712414264679\n",
      "training error:  0.7757647037506104  and  1.3501278162002563  and  1.0966475009918213\n",
      "total error:  3.222540020942688\n",
      "training error:  0.749596357345581  and  1.3363248109817505  and  1.1154613494873047\n",
      "total error:  3.2013825178146362\n",
      "training error:  0.7624505162239075  and  1.2835911512374878  and  1.0247743129730225\n",
      "total error:  3.0708159804344177\n",
      "training error:  0.7448765635490417  and  1.344996452331543  and  1.0333977937698364\n",
      "total error:  3.123270809650421\n",
      "training error:  0.7563109397888184  and  1.295998215675354  and  1.0647553205490112\n",
      "total error:  3.1170644760131836\n",
      "training error:  0.7721906900405884  and  1.2541497945785522  and  1.028632640838623\n",
      "total error:  3.0549731254577637\n",
      "training error:  0.80560302734375  and  1.290627121925354  and  1.110683560371399\n",
      "total error:  3.206913709640503\n",
      "training error:  0.7788069248199463  and  1.4953910112380981  and  1.1353838443756104\n",
      "total error:  3.409581780433655\n",
      "training error:  0.7666141390800476  and  1.2645319700241089  and  1.0017318725585938\n",
      "total error:  3.0328779816627502\n",
      "training error:  0.774101734161377  and  1.3202264308929443  and  1.2681217193603516\n",
      "total error:  3.362449884414673\n",
      "training error:  0.7354077100753784  and  1.2523901462554932  and  1.0309287309646606\n",
      "total error:  3.0187265872955322\n",
      "training error:  0.7465519905090332  and  1.2473580837249756  and  1.0322118997573853\n",
      "total error:  3.026121973991394\n",
      "training error:  0.764072835445404  and  1.3070546388626099  and  1.1742470264434814\n",
      "total error:  3.2453745007514954\n",
      "training error:  0.7532610297203064  and  1.4648140668869019  and  1.0950977802276611\n",
      "total error:  3.3131728768348694\n",
      "training error:  0.7430496215820312  and  1.2999690771102905  and  1.0770224332809448\n",
      "total error:  3.1200411319732666\n",
      "training error:  0.7724906206130981  and  1.3531630039215088  and  0.9986705183982849\n",
      "total error:  3.124324142932892\n",
      "training error:  0.7745367884635925  and  1.2579268217086792  and  0.9959392547607422\n",
      "total error:  3.028402864933014\n",
      "training error:  0.73250412940979  and  1.2441686391830444  and  1.0068162679672241\n",
      "total error:  2.9834890365600586\n",
      "training error:  0.7435835003852844  and  1.2695152759552002  and  0.9927052855491638\n",
      "total error:  3.0058040618896484\n",
      "training error:  0.7635884284973145  and  1.2705579996109009  and  1.0807914733886719\n",
      "total error:  3.114937901496887\n",
      "training error:  0.7509455680847168  and  1.410679578781128  and  1.041796088218689\n",
      "total error:  3.2034212350845337\n",
      "training error:  0.7479203939437866  and  1.2484301328659058  and  1.004431962966919\n",
      "total error:  3.0007824897766113\n",
      "training error:  0.7540469765663147  and  1.2416512966156006  and  1.0229389667510986\n",
      "total error:  3.018637239933014\n",
      "training error:  0.7786177396774292  and  1.2432630062103271  and  1.0102256536483765\n",
      "total error:  3.032106399536133\n",
      "training error:  0.7296266555786133  and  1.2299288511276245  and  1.0030620098114014\n",
      "total error:  2.962617516517639\n",
      "training error:  0.7606639862060547  and  1.2572605609893799  and  1.1586003303527832\n",
      "total error:  3.1765248775482178\n",
      "training error:  0.7652450203895569  and  1.2688742876052856  and  0.9891448020935059\n",
      "total error:  3.0232641100883484\n",
      "training error:  0.7647107839584351  and  1.2309536933898926  and  1.0876262187957764\n",
      "total error:  3.083290696144104\n",
      "training error:  0.7414724826812744  and  1.2808763980865479  and  1.0356824398040771\n",
      "total error:  3.0580313205718994\n",
      "training error:  0.7781016826629639  and  1.5136529207229614  and  1.2149782180786133\n",
      "total error:  3.5067328214645386\n",
      "training error:  0.76020747423172  and  1.2891318798065186  and  1.006672978401184\n",
      "total error:  3.0560123324394226\n",
      "training error:  0.7761859893798828  and  1.2638548612594604  and  1.0139003992080688\n",
      "total error:  3.053941249847412\n",
      "training error:  0.7340118288993835  and  1.210320234298706  and  1.0397191047668457\n",
      "total error:  2.9840511679649353\n",
      "training error:  0.7593268156051636  and  1.2510125637054443  and  1.0689852237701416\n",
      "total error:  3.0793246030807495\n",
      "training error:  0.7500727772712708  and  1.3091386556625366  and  0.9989622831344604\n",
      "total error:  3.058173716068268\n",
      "training error:  0.7601537704467773  and  1.3050589561462402  and  1.0409560203552246\n",
      "total error:  3.106168746948242\n",
      "training error:  0.7279564738273621  and  1.2077187299728394  and  1.0299869775772095\n",
      "total error:  2.965662181377411\n",
      "training error:  0.7325788736343384  and  1.2406566143035889  and  1.0678561925888062\n",
      "total error:  3.0410916805267334\n",
      "training error:  0.7322678565979004  and  1.4612118005752563  and  1.056771993637085\n",
      "total error:  3.2502516508102417\n",
      "training error:  0.7335482835769653  and  1.292585849761963  and  0.9971032738685608\n",
      "total error:  3.023237407207489\n",
      "training error:  0.7258763313293457  and  1.2460076808929443  and  1.0186338424682617\n",
      "total error:  2.9905178546905518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7317082285881042  and  1.2773699760437012  and  0.9925781488418579\n",
      "total error:  3.0016563534736633\n",
      "training error:  0.7605822682380676  and  1.257725477218628  and  1.0208534002304077\n",
      "total error:  3.0391611456871033\n",
      "training error:  0.7321813106536865  and  1.188671588897705  and  0.9800540208816528\n",
      "total error:  2.9009069204330444\n",
      "training error:  0.7710613012313843  and  1.2818603515625  and  1.0698264837265015\n",
      "total error:  3.1227481365203857\n",
      "training error:  0.7514139413833618  and  1.3366175889968872  and  1.1016550064086914\n",
      "total error:  3.1896865367889404\n",
      "training error:  0.7483114004135132  and  1.250396966934204  and  1.0316638946533203\n",
      "total error:  3.0303722620010376\n",
      "training error:  0.7348713278770447  and  1.2530486583709717  and  1.1150333881378174\n",
      "total error:  3.1029533743858337\n",
      "training error:  0.7712981700897217  and  1.2892502546310425  and  1.0409979820251465\n",
      "total error:  3.1015464067459106\n",
      "training error:  0.7309648990631104  and  1.2391555309295654  and  1.015472173690796\n",
      "total error:  2.9855926036834717\n",
      "training error:  0.730456531047821  and  1.2491031885147095  and  0.9881318807601929\n",
      "total error:  2.9676916003227234\n",
      "training error:  0.7336188554763794  and  1.2166821956634521  and  1.0102323293685913\n",
      "total error:  2.960533380508423\n",
      "training error:  0.7587140202522278  and  1.2245981693267822  and  1.0464364290237427\n",
      "total error:  3.0297486186027527\n",
      "training error:  0.7525864839553833  and  1.3651814460754395  and  1.1146299839019775\n",
      "total error:  3.2323979139328003\n",
      "training error:  0.739450216293335  and  1.2211947441101074  and  1.0171164274215698\n",
      "total error:  2.977761387825012\n",
      "training error:  0.7213945984840393  and  1.2343519926071167  and  1.020728588104248\n",
      "total error:  2.976475179195404\n",
      "training error:  0.7218561172485352  and  1.3114537000656128  and  1.007958173751831\n",
      "total error:  3.041267991065979\n",
      "training error:  0.7531948089599609  and  1.271249771118164  and  1.0313706398010254\n",
      "total error:  3.0558152198791504\n",
      "training error:  0.747086226940155  and  1.2563035488128662  and  1.000760793685913\n",
      "total error:  3.0041505694389343\n",
      "training error:  0.7765299081802368  and  1.2666547298431396  and  1.0098826885223389\n",
      "total error:  3.0530673265457153\n",
      "training error:  0.758355975151062  and  1.360978603363037  and  1.0585436820983887\n",
      "total error:  3.177878260612488\n",
      "training error:  0.7346961498260498  and  1.342599630355835  and  1.0665003061294556\n",
      "total error:  3.1437960863113403\n",
      "training error:  0.7411707043647766  and  1.4346081018447876  and  1.0098015069961548\n",
      "total error:  3.185580313205719\n",
      "training error:  0.7389547228813171  and  1.4241666793823242  and  1.0776728391647339\n",
      "total error:  3.2407942414283752\n",
      "training error:  0.7222225666046143  and  1.3031196594238281  and  1.018337368965149\n",
      "total error:  3.0436795949935913\n",
      "training error:  0.7310376167297363  and  1.2644246816635132  and  1.0908024311065674\n",
      "total error:  3.086264729499817\n",
      "training error:  0.7526537179946899  and  1.192192792892456  and  1.0328609943389893\n",
      "total error:  2.9777075052261353\n",
      "training error:  0.7547309398651123  and  1.2568035125732422  and  1.0169544219970703\n",
      "total error:  3.028488874435425\n",
      "training error:  0.7152958512306213  and  1.2279841899871826  and  0.974023699760437\n",
      "total error:  2.917303740978241\n",
      "training error:  0.7511637210845947  and  1.2562270164489746  and  1.0315688848495483\n",
      "total error:  3.0389596223831177\n",
      "training error:  0.7019441723823547  and  1.3149827718734741  and  1.022087574005127\n",
      "total error:  3.039014518260956\n",
      "training error:  0.7072793245315552  and  1.2718735933303833  and  1.012093424797058\n",
      "total error:  2.9912463426589966\n",
      "training error:  0.7130367159843445  and  1.2035585641860962  and  1.0179989337921143\n",
      "total error:  2.934594213962555\n",
      "training error:  0.7270042896270752  and  1.194895625114441  and  1.0033024549484253\n",
      "total error:  2.9252023696899414\n",
      "training error:  0.7246026992797852  and  1.2539106607437134  and  1.044389009475708\n",
      "total error:  3.0229023694992065\n",
      "training error:  0.7698020339012146  and  1.3748178482055664  and  1.0742180347442627\n",
      "total error:  3.2188379168510437\n",
      "training error:  0.7282240390777588  and  1.3568824529647827  and  0.9948431253433228\n",
      "total error:  3.0799496173858643\n",
      "training error:  0.7566744089126587  and  1.2441352605819702  and  1.09751296043396\n",
      "total error:  3.098322629928589\n",
      "training error:  0.7244358658790588  and  1.2455288171768188  and  1.0266200304031372\n",
      "total error:  2.996584713459015\n",
      "training error:  0.7221020460128784  and  1.2527797222137451  and  1.039283275604248\n",
      "total error:  3.0141650438308716\n",
      "training error:  0.7236647009849548  and  1.2246017456054688  and  0.9778693914413452\n",
      "total error:  2.926135838031769\n",
      "training error:  0.7214643955230713  and  1.207214117050171  and  0.987468957901001\n",
      "total error:  2.916147470474243\n",
      "training error:  0.729174792766571  and  1.237931728363037  and  1.0118165016174316\n",
      "total error:  2.97892302274704\n",
      "training error:  0.7326974272727966  and  1.2622487545013428  and  0.9879027605056763\n",
      "total error:  2.9828489422798157\n",
      "training error:  0.7554421424865723  and  1.2619004249572754  and  1.1016921997070312\n",
      "total error:  3.119034767150879\n",
      "training error:  0.7403851747512817  and  1.4028937816619873  and  1.0528150796890259\n",
      "total error:  3.196094036102295\n",
      "training error:  0.7331738471984863  and  1.2179701328277588  and  1.038059949874878\n",
      "total error:  2.989203929901123\n",
      "training error:  0.739527702331543  and  1.2690098285675049  and  0.9965196251869202\n",
      "total error:  3.005057156085968\n",
      "training error:  0.7141842842102051  and  1.2496254444122314  and  1.0169638395309448\n",
      "total error:  2.9807735681533813\n",
      "training error:  0.7318223714828491  and  1.291783332824707  and  1.053248405456543\n",
      "total error:  3.076854109764099\n",
      "training error:  0.7191041707992554  and  1.231184482574463  and  1.0158088207244873\n",
      "total error:  2.9660974740982056\n",
      "training error:  0.7583119869232178  and  1.6025416851043701  and  1.0963003635406494\n",
      "total error:  3.4571540355682373\n",
      "training error:  0.7192009687423706  and  1.2546970844268799  and  1.0113394260406494\n",
      "total error:  2.9852374792099\n",
      "training error:  0.7435837984085083  and  1.3792431354522705  and  1.0403497219085693\n",
      "total error:  3.163176655769348\n",
      "training error:  0.7384932041168213  and  1.265913724899292  and  1.074379801750183\n",
      "total error:  3.0787867307662964\n",
      "training error:  0.7243520617485046  and  1.2290761470794678  and  1.0551186800003052\n",
      "total error:  3.0085468888282776\n",
      "training error:  0.7778346538543701  and  1.2734426259994507  and  0.9944943189620972\n",
      "total error:  3.045771598815918\n",
      "training error:  0.7143750786781311  and  1.1748453378677368  and  0.9929195046424866\n",
      "total error:  2.8821399211883545\n",
      "training error:  0.7207663655281067  and  1.2245750427246094  and  1.0264158248901367\n",
      "total error:  2.971757233142853\n",
      "training error:  0.7100878953933716  and  1.1728492975234985  and  0.9992452263832092\n",
      "total error:  2.8821824193000793\n",
      "training error:  0.7199444770812988  and  1.1980161666870117  and  1.0062525272369385\n",
      "total error:  2.924213171005249\n",
      "training error:  0.7309602499008179  and  1.2130277156829834  and  1.0331404209136963\n",
      "total error:  2.9771283864974976\n",
      "training error:  0.7085205316543579  and  1.1902592182159424  and  0.9737732410430908\n",
      "total error:  2.872552990913391\n",
      "training error:  0.7419604063034058  and  1.2322986125946045  and  1.028409719467163\n",
      "total error:  3.0026687383651733\n",
      "training error:  0.7283148765563965  and  1.2877511978149414  and  1.2512845993041992\n",
      "total error:  3.267350673675537\n",
      "training error:  0.7422723770141602  and  1.255082368850708  and  1.046098232269287\n",
      "total error:  3.0434529781341553\n",
      "training error:  0.7093976140022278  and  1.224212884902954  and  0.9776929020881653\n",
      "total error:  2.911303400993347\n",
      "training error:  0.7552661895751953  and  1.3421962261199951  and  1.102917194366455\n",
      "total error:  3.2003796100616455\n",
      "training error:  0.7640088796615601  and  1.270688533782959  and  1.0518684387207031\n",
      "total error:  3.086565852165222\n",
      "training error:  0.6966954469680786  and  1.2648420333862305  and  0.976289689540863\n",
      "total error:  2.937827169895172\n",
      "training error:  0.7574683427810669  and  1.2468376159667969  and  1.06745445728302\n",
      "total error:  3.071760416030884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7109464406967163  and  1.2099096775054932  and  0.9761776924133301\n",
      "total error:  2.8970338106155396\n",
      "training error:  0.7111570239067078  and  1.2485308647155762  and  1.0653986930847168\n",
      "total error:  3.0250865817070007\n",
      "training error:  0.71346116065979  and  1.2826321125030518  and  1.0549317598342896\n",
      "total error:  3.0510250329971313\n",
      "training error:  0.7441014647483826  and  1.2087255716323853  and  1.0247317552566528\n",
      "total error:  2.9775587916374207\n",
      "training error:  0.7323365211486816  and  1.3205502033233643  and  1.069115400314331\n",
      "total error:  3.122002124786377\n",
      "training error:  0.7186568379402161  and  1.1800098419189453  and  1.0096418857574463\n",
      "total error:  2.9083085656166077\n",
      "training error:  0.7331581711769104  and  1.244660496711731  and  1.0774431228637695\n",
      "total error:  3.055261790752411\n",
      "training error:  0.7311124801635742  and  1.2959415912628174  and  1.0871460437774658\n",
      "total error:  3.1142001152038574\n",
      "training error:  0.712791383266449  and  1.2632827758789062  and  1.1006016731262207\n",
      "total error:  3.076675832271576\n",
      "training error:  0.7108350396156311  and  1.1718761920928955  and  1.0430930852890015\n",
      "total error:  2.925804316997528\n",
      "training error:  0.7445486187934875  and  1.4705840349197388  and  1.100975513458252\n",
      "total error:  3.3161081671714783\n",
      "training error:  0.7256799340248108  and  1.2241249084472656  and  1.119217872619629\n",
      "total error:  3.0690227150917053\n",
      "training error:  0.7395361661911011  and  1.2499938011169434  and  1.1385434865951538\n",
      "total error:  3.1280734539031982\n",
      "training error:  0.7453200817108154  and  1.3586831092834473  and  1.0477248430252075\n",
      "total error:  3.15172803401947\n",
      "training error:  0.7101722955703735  and  1.2419579029083252  and  0.9886693954467773\n",
      "total error:  2.940799593925476\n",
      "training error:  0.7149267196655273  and  1.1945457458496094  and  1.0044363737106323\n",
      "total error:  2.913908839225769\n",
      "training error:  0.7284021973609924  and  1.2346932888031006  and  1.017009973526001\n",
      "total error:  2.980105459690094\n",
      "training error:  0.70389723777771  and  1.3160316944122314  and  1.0553139448165894\n",
      "total error:  3.0752428770065308\n",
      "training error:  0.7013744115829468  and  1.1726014614105225  and  1.016472339630127\n",
      "total error:  2.890448212623596\n",
      "training error:  0.7141966819763184  and  1.2487449645996094  and  1.0843396186828613\n",
      "total error:  3.047281265258789\n",
      "training error:  0.7495818138122559  and  1.2115285396575928  and  0.998891294002533\n",
      "total error:  2.9600016474723816\n",
      "training error:  0.74294513463974  and  1.298094630241394  and  1.045710802078247\n",
      "total error:  3.086750566959381\n",
      "training error:  0.7049358487129211  and  1.2076603174209595  and  1.0236015319824219\n",
      "total error:  2.9361976981163025\n",
      "training error:  0.7001373171806335  and  1.1803297996520996  and  1.0137484073638916\n",
      "total error:  2.8942155241966248\n",
      "training error:  0.7088199853897095  and  1.1825642585754395  and  0.9992039799690247\n",
      "total error:  2.8905882239341736\n",
      "training error:  0.6998956203460693  and  1.2108067274093628  and  1.0103131532669067\n",
      "total error:  2.921015501022339\n",
      "training error:  0.7143090963363647  and  1.2348029613494873  and  1.0552211999893188\n",
      "total error:  3.004333257675171\n",
      "training error:  0.6926116347312927  and  1.2363533973693848  and  0.9705437421798706\n",
      "total error:  2.899508774280548\n",
      "training error:  0.7275882959365845  and  1.1890976428985596  and  0.9808177351951599\n",
      "total error:  2.897503674030304\n",
      "training error:  0.7069623470306396  and  1.2090375423431396  and  1.074127197265625\n",
      "total error:  2.9901270866394043\n",
      "training error:  0.7165611982345581  and  1.239203691482544  and  1.0139693021774292\n",
      "total error:  2.9697341918945312\n",
      "training error:  0.7182500958442688  and  1.370537519454956  and  1.0872724056243896\n",
      "total error:  3.1760600209236145\n",
      "training error:  0.7269788980484009  and  1.2127217054367065  and  1.0192979574203491\n",
      "total error:  2.9589985609054565\n",
      "training error:  0.708089292049408  and  1.1959607601165771  and  1.0243327617645264\n",
      "total error:  2.9283828139305115\n",
      "training error:  0.7122376561164856  and  1.2013643980026245  and  0.9739260077476501\n",
      "total error:  2.8875280618667603\n",
      "training error:  0.7063403129577637  and  1.2233788967132568  and  1.0647361278533936\n",
      "total error:  2.994455337524414\n",
      "training error:  0.6871401071548462  and  1.2512940168380737  and  1.000370740890503\n",
      "total error:  2.938804864883423\n",
      "training error:  0.7107288837432861  and  1.2477290630340576  and  0.9972902536392212\n",
      "total error:  2.955748200416565\n",
      "training error:  0.7248742580413818  and  1.3496119976043701  and  1.03289794921875\n",
      "total error:  3.107384204864502\n",
      "training error:  0.716489315032959  and  1.2096296548843384  and  1.0132198333740234\n",
      "total error:  2.939338803291321\n",
      "training error:  0.7145848274230957  and  1.2009499073028564  and  1.0899734497070312\n",
      "total error:  3.0055081844329834\n",
      "training error:  0.70479416847229  and  1.2188150882720947  and  1.0018329620361328\n",
      "total error:  2.9254422187805176\n",
      "training error:  0.6966283917427063  and  1.2540206909179688  and  0.9912374019622803\n",
      "total error:  2.9418864846229553\n",
      "training error:  0.7570624351501465  and  1.9030735492706299  and  1.1851892471313477\n",
      "total error:  3.845325231552124\n",
      "training error:  0.7228254079818726  and  1.1909751892089844  and  1.1020463705062866\n",
      "total error:  3.0158469676971436\n",
      "training error:  0.7159836888313293  and  1.205783486366272  and  1.0347098112106323\n",
      "total error:  2.9564769864082336\n",
      "training error:  0.7191648483276367  and  1.24627685546875  and  0.9543224573135376\n",
      "total error:  2.9197641611099243\n",
      "training error:  0.7347829341888428  and  1.3888747692108154  and  1.0515655279159546\n",
      "total error:  3.175223231315613\n",
      "training error:  0.7166218757629395  and  1.3370331525802612  and  1.0408344268798828\n",
      "total error:  3.0944894552230835\n",
      "training error:  0.7150248885154724  and  1.1665775775909424  and  0.941508412361145\n",
      "total error:  2.82311087846756\n",
      "training error:  0.6914621591567993  and  1.2937664985656738  and  0.9706274271011353\n",
      "total error:  2.9558560848236084\n",
      "training error:  0.7026599645614624  and  1.1529396772384644  and  0.9290037155151367\n",
      "total error:  2.7846033573150635\n",
      "training error:  0.7259343266487122  and  1.3177070617675781  and  1.0900673866271973\n",
      "total error:  3.1337087750434875\n",
      "training error:  0.7188130617141724  and  1.426051378250122  and  1.0889167785644531\n",
      "total error:  3.2337812185287476\n",
      "training error:  0.7037118673324585  and  1.184098720550537  and  1.0174951553344727\n",
      "total error:  2.9053057432174683\n",
      "training error:  0.7311391830444336  and  1.2571154832839966  and  0.9749822020530701\n",
      "total error:  2.9632368683815002\n",
      "training error:  0.6951406002044678  and  1.2825071811676025  and  1.0228487253189087\n",
      "total error:  3.000496506690979\n",
      "training error:  0.6855887174606323  and  1.2433865070343018  and  0.9830363988876343\n",
      "total error:  2.9120116233825684\n",
      "training error:  0.7374317646026611  and  1.268612265586853  and  1.124384880065918\n",
      "total error:  3.130428910255432\n",
      "training error:  0.6950280070304871  and  1.2080893516540527  and  0.9679430723190308\n",
      "total error:  2.8710604310035706\n",
      "training error:  0.7023544311523438  and  1.1981784105300903  and  0.9630310535430908\n",
      "total error:  2.863563895225525\n",
      "training error:  0.7488812804222107  and  1.4228922128677368  and  1.024664282798767\n",
      "total error:  3.1964377760887146\n",
      "training error:  0.7006135582923889  and  1.181014895439148  and  1.0406688451766968\n",
      "total error:  2.9222972989082336\n",
      "training error:  0.6845389008522034  and  1.1999890804290771  and  0.9747892618179321\n",
      "total error:  2.8593172430992126\n",
      "training error:  0.7064350843429565  and  1.208734393119812  and  1.001739263534546\n",
      "total error:  2.9169087409973145\n",
      "training error:  0.7205272912979126  and  1.1854331493377686  and  0.9491111040115356\n",
      "total error:  2.855071544647217\n",
      "training error:  0.7229486703872681  and  1.2002798318862915  and  0.9539172053337097\n",
      "total error:  2.8771457076072693\n",
      "training error:  0.7033357620239258  and  1.2684378623962402  and  1.037717342376709\n",
      "total error:  3.009490966796875\n",
      "training error:  0.700012743473053  and  1.25882887840271  and  1.1252199411392212\n",
      "total error:  3.084061563014984\n",
      "training error:  0.6979825496673584  and  1.2406091690063477  and  1.0242829322814941\n",
      "total error:  2.9628746509552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7053011655807495  and  1.2533881664276123  and  1.1384609937667847\n",
      "total error:  3.0971503257751465\n",
      "training error:  0.7125871181488037  and  1.263728141784668  and  1.0011309385299683\n",
      "total error:  2.97744619846344\n",
      "training error:  0.7035617828369141  and  1.1913702487945557  and  1.0382367372512817\n",
      "total error:  2.9331687688827515\n",
      "training error:  0.7091390490531921  and  1.2188767194747925  and  1.0015970468521118\n",
      "total error:  2.9296128153800964\n",
      "training error:  0.6892918348312378  and  1.2348823547363281  and  0.9791200160980225\n",
      "total error:  2.9032942056655884\n",
      "training error:  0.7374359369277954  and  1.2651658058166504  and  0.996547281742096\n",
      "total error:  2.9991490244865417\n",
      "training error:  0.7016081213951111  and  1.307435393333435  and  0.951009213924408\n",
      "total error:  2.960052728652954\n",
      "training error:  0.7015217542648315  and  1.22553288936615  and  0.9857065677642822\n",
      "total error:  2.9127612113952637\n",
      "training error:  0.7226661443710327  and  1.241952657699585  and  0.9918743968009949\n",
      "total error:  2.9564931988716125\n",
      "training error:  0.7033700942993164  and  1.22846257686615  and  1.0214903354644775\n",
      "total error:  2.953323006629944\n",
      "training error:  0.6842536926269531  and  1.3465094566345215  and  1.1276873350143433\n",
      "total error:  3.158450484275818\n",
      "training error:  0.679566502571106  and  1.203125238418579  and  1.084729790687561\n",
      "total error:  2.967421531677246\n",
      "training error:  0.7436223030090332  and  1.328598141670227  and  1.0088332891464233\n",
      "total error:  3.0810537338256836\n",
      "training error:  0.6878361701965332  and  1.2605564594268799  and  1.0545042753219604\n",
      "total error:  3.0028969049453735\n",
      "training error:  0.6967223882675171  and  1.2158302068710327  and  1.0359265804290771\n",
      "total error:  2.948479175567627\n",
      "training error:  0.6867679357528687  and  1.334185242652893  and  1.0579094886779785\n",
      "total error:  3.0788626670837402\n",
      "training error:  0.730016827583313  and  1.232898473739624  and  1.075265645980835\n",
      "total error:  3.038180947303772\n",
      "training error:  0.6829372644424438  and  1.2297227382659912  and  1.0123183727264404\n",
      "total error:  2.9249783754348755\n",
      "training error:  0.7006285190582275  and  1.3362071514129639  and  0.9837414026260376\n",
      "total error:  3.020577073097229\n",
      "training error:  0.7042444944381714  and  1.1953006982803345  and  0.9631109237670898\n",
      "total error:  2.8626561164855957\n",
      "training error:  0.7070366144180298  and  1.1903719902038574  and  0.9843616485595703\n",
      "total error:  2.8817702531814575\n",
      "training error:  0.6826877593994141  and  1.2270146608352661  and  0.9847682118415833\n",
      "total error:  2.8944706320762634\n",
      "training error:  0.7202396392822266  and  1.2184817790985107  and  1.054581880569458\n",
      "total error:  2.9933032989501953\n",
      "training error:  0.7225011587142944  and  1.4002442359924316  and  1.086542010307312\n",
      "total error:  3.209287405014038\n",
      "training error:  0.6842631697654724  and  1.2970073223114014  and  1.0555269718170166\n",
      "total error:  3.0367974638938904\n",
      "training error:  0.7009021043777466  and  1.1830496788024902  and  0.9525171518325806\n",
      "total error:  2.8364689350128174\n",
      "training error:  0.7222849726676941  and  1.26643967628479  and  1.0830671787261963\n",
      "total error:  3.0717918276786804\n",
      "training error:  0.692690372467041  and  1.2234320640563965  and  1.0024945735931396\n",
      "total error:  2.918617010116577\n",
      "training error:  0.720043957233429  and  1.22767972946167  and  0.9632967710494995\n",
      "total error:  2.9110204577445984\n",
      "training error:  0.7241297364234924  and  1.1642570495605469  and  0.9496244192123413\n",
      "total error:  2.8380112051963806\n",
      "training error:  0.7001250982284546  and  1.2828534841537476  and  0.9958230257034302\n",
      "total error:  2.9788016080856323\n",
      "training error:  0.7414125204086304  and  1.6911617517471313  and  1.19728684425354\n",
      "total error:  3.6298611164093018\n",
      "training error:  0.7037160396575928  and  1.224646806716919  and  1.0412795543670654\n",
      "total error:  2.969642400741577\n",
      "training error:  0.7056982517242432  and  1.224176287651062  and  0.9966729879379272\n",
      "total error:  2.9265475273132324\n",
      "training error:  0.6894181966781616  and  1.168368935585022  and  0.9913754463195801\n",
      "total error:  2.8491625785827637\n",
      "training error:  0.6886402368545532  and  1.2377097606658936  and  0.9431453943252563\n",
      "total error:  2.869495391845703\n",
      "training error:  0.6701744794845581  and  1.2327895164489746  and  1.0663708448410034\n",
      "total error:  2.969334840774536\n",
      "training error:  0.6880619525909424  and  1.2778359651565552  and  1.2411020994186401\n",
      "total error:  3.2070000171661377\n",
      "training error:  0.7364761829376221  and  1.240576148033142  and  1.0149633884429932\n",
      "total error:  2.9920157194137573\n",
      "training error:  0.7306255102157593  and  1.2876648902893066  and  1.0720593929290771\n",
      "total error:  3.090349793434143\n",
      "training error:  0.6899423003196716  and  1.2890105247497559  and  1.041440486907959\n",
      "total error:  3.0203933119773865\n",
      "training error:  0.6783225536346436  and  1.246556043624878  and  1.1960783004760742\n",
      "total error:  3.1209568977355957\n",
      "training error:  0.6792102456092834  and  1.190777063369751  and  0.9812068939208984\n",
      "total error:  2.851194202899933\n",
      "training error:  0.6934073567390442  and  1.4276325702667236  and  1.0519729852676392\n",
      "total error:  3.173012912273407\n",
      "training error:  0.6858236789703369  and  1.2542057037353516  and  1.0852406024932861\n",
      "total error:  3.0252699851989746\n",
      "training error:  0.67735755443573  and  1.2011560201644897  and  0.9754223227500916\n",
      "total error:  2.8539358973503113\n",
      "training error:  0.708863377571106  and  1.2162320613861084  and  1.1067854166030884\n",
      "total error:  3.0318808555603027\n",
      "training error:  0.7074229717254639  and  1.1820330619812012  and  0.9930002689361572\n",
      "total error:  2.8824563026428223\n",
      "training error:  0.6961392164230347  and  1.1702382564544678  and  0.9446775317192078\n",
      "total error:  2.81105500459671\n",
      "training error:  0.6754903197288513  and  1.2000200748443604  and  1.0264791250228882\n",
      "total error:  2.9019895195961\n",
      "training error:  0.7502040863037109  and  1.2751970291137695  and  0.9969050884246826\n",
      "total error:  3.022306203842163\n",
      "training error:  0.6753154397010803  and  1.2325326204299927  and  1.0275877714157104\n",
      "total error:  2.9354358315467834\n",
      "training error:  0.6723813414573669  and  1.1577297449111938  and  1.0426932573318481\n",
      "total error:  2.872804343700409\n",
      "training error:  0.6913631558418274  and  1.17849862575531  and  1.0172333717346191\n",
      "total error:  2.8870951533317566\n",
      "training error:  0.696215033531189  and  1.2148405313491821  and  1.0211007595062256\n",
      "total error:  2.9321563243865967\n",
      "training error:  0.6714099049568176  and  1.378258228302002  and  1.0660765171051025\n",
      "total error:  3.115744650363922\n",
      "training error:  0.772089958190918  and  1.4536455869674683  and  1.183321475982666\n",
      "total error:  3.4090570211410522\n",
      "training error:  0.6955930590629578  and  1.2437900304794312  and  1.093340277671814\n",
      "total error:  3.032723367214203\n",
      "training error:  0.6941795349121094  and  1.2067670822143555  and  1.037511944770813\n",
      "total error:  2.938458561897278\n",
      "training error:  0.665727972984314  and  1.1232025623321533  and  0.9869195222854614\n",
      "total error:  2.7758500576019287\n",
      "training error:  0.6951892971992493  and  1.1676661968231201  and  0.9743945002555847\n",
      "total error:  2.837249994277954\n",
      "training error:  0.7091885805130005  and  1.2995729446411133  and  0.9934613704681396\n",
      "total error:  3.0022228956222534\n",
      "training error:  0.6978849172592163  and  1.2078598737716675  and  0.9843870997428894\n",
      "total error:  2.890131890773773\n",
      "training error:  0.7086944580078125  and  1.2256126403808594  and  1.0027875900268555\n",
      "total error:  2.9370946884155273\n",
      "training error:  0.6771773099899292  and  1.1968430280685425  and  0.98774653673172\n",
      "total error:  2.8617668747901917\n",
      "training error:  0.6668593883514404  and  1.2448217868804932  and  0.9729914665222168\n",
      "total error:  2.8846726417541504\n",
      "training error:  0.6928638815879822  and  1.1781582832336426  and  1.0268337726593018\n",
      "total error:  2.8978559374809265\n",
      "training error:  0.6886904835700989  and  1.2429683208465576  and  1.068965196609497\n",
      "total error:  3.0006240010261536\n",
      "training error:  0.6917821168899536  and  1.2351922988891602  and  0.9264921545982361\n",
      "total error:  2.85346657037735\n",
      "training error:  0.7056163549423218  and  1.3534724712371826  and  1.1221550703048706\n",
      "total error:  3.181243896484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6895118951797485  and  1.2155015468597412  and  0.9877712726593018\n",
      "total error:  2.8927847146987915\n",
      "training error:  0.6646513938903809  and  1.1906267404556274  and  1.0009398460388184\n",
      "total error:  2.8562179803848267\n",
      "training error:  0.7136592864990234  and  1.2661919593811035  and  0.9628415107727051\n",
      "total error:  2.942692756652832\n",
      "training error:  0.6680553555488586  and  1.149858832359314  and  0.9718100428581238\n",
      "total error:  2.7897242307662964\n",
      "training error:  0.6598175168037415  and  1.1658356189727783  and  1.0010600090026855\n",
      "total error:  2.8267131447792053\n",
      "training error:  0.6613394618034363  and  1.1353416442871094  and  0.9955861568450928\n",
      "total error:  2.7922672629356384\n",
      "training error:  0.6794639825820923  and  1.2055861949920654  and  0.9596872329711914\n",
      "total error:  2.844737410545349\n",
      "training error:  0.6843738555908203  and  1.2449755668640137  and  1.0141891241073608\n",
      "total error:  2.943538546562195\n",
      "training error:  0.6718795299530029  and  1.150019884109497  and  0.9975524544715881\n",
      "total error:  2.819451868534088\n",
      "training error:  0.7399656772613525  and  1.260857343673706  and  0.9539244771003723\n",
      "total error:  2.954747498035431\n",
      "training error:  0.6931014657020569  and  1.187560796737671  and  0.94199538230896\n",
      "total error:  2.8226576447486877\n",
      "training error:  0.668279767036438  and  1.1648836135864258  and  1.1071505546569824\n",
      "total error:  2.940313935279846\n",
      "training error:  0.70986008644104  and  1.228888750076294  and  0.9406603574752808\n",
      "total error:  2.8794091939926147\n",
      "training error:  0.6678258180618286  and  1.2047146558761597  and  0.9657520055770874\n",
      "total error:  2.8382924795150757\n",
      "training error:  0.6815620064735413  and  1.2194839715957642  and  0.9356100559234619\n",
      "total error:  2.8366560339927673\n",
      "training error:  0.6516214609146118  and  1.1756007671356201  and  1.0001955032348633\n",
      "total error:  2.827417731285095\n",
      "training error:  0.6813390254974365  and  1.2735340595245361  and  0.9954979419708252\n",
      "total error:  2.950371026992798\n",
      "training error:  0.694034218788147  and  1.2265684604644775  and  1.0226620435714722\n",
      "total error:  2.9432647228240967\n",
      "training error:  0.679133951663971  and  1.1807243824005127  and  1.0160129070281982\n",
      "total error:  2.875871241092682\n",
      "training error:  0.7030493021011353  and  1.2465434074401855  and  0.9482870101928711\n",
      "total error:  2.897879719734192\n",
      "training error:  0.7820191979408264  and  1.2951292991638184  and  1.067528486251831\n",
      "total error:  3.144676983356476\n",
      "training error:  0.6980087757110596  and  1.196500539779663  and  0.973071813583374\n",
      "total error:  2.8675811290740967\n",
      "training error:  0.67690509557724  and  1.2216172218322754  and  0.9621497392654419\n",
      "total error:  2.8606720566749573\n",
      "training error:  0.6802734136581421  and  1.264278769493103  and  1.0097951889038086\n",
      "total error:  2.9543473720550537\n",
      "training error:  0.6706827878952026  and  1.1435551643371582  and  1.0433225631713867\n",
      "total error:  2.8575605154037476\n",
      "training error:  0.6778765916824341  and  1.2393906116485596  and  0.9624822735786438\n",
      "total error:  2.8797494769096375\n",
      "training error:  0.7035008668899536  and  1.3211736679077148  and  1.0411354303359985\n",
      "total error:  3.065809965133667\n",
      "training error:  0.6818533539772034  and  1.1614972352981567  and  1.0161476135253906\n",
      "total error:  2.8594982028007507\n",
      "training error:  0.6606709957122803  and  1.2926883697509766  and  0.9156956076622009\n",
      "total error:  2.8690549731254578\n",
      "training error:  0.682553768157959  and  1.2964599132537842  and  1.1519269943237305\n",
      "total error:  3.1309406757354736\n",
      "training error:  0.6804397106170654  and  1.2079594135284424  and  1.0649092197418213\n",
      "total error:  2.953308343887329\n",
      "training error:  0.7043299078941345  and  1.2468514442443848  and  0.9709798097610474\n",
      "total error:  2.9221611618995667\n",
      "training error:  0.6766403317451477  and  1.2774319648742676  and  1.0280357599258423\n",
      "total error:  2.9821080565452576\n",
      "training error:  0.7494433522224426  and  1.5832605361938477  and  1.1738542318344116\n",
      "total error:  3.506558120250702\n",
      "training error:  0.6936353445053101  and  1.1755355596542358  and  0.9827777743339539\n",
      "total error:  2.8519486784934998\n",
      "training error:  0.6594805717468262  and  1.1701164245605469  and  0.9633248448371887\n",
      "total error:  2.7929218411445618\n",
      "training error:  0.7432190179824829  and  1.2408734560012817  and  1.1704556941986084\n",
      "total error:  3.154548168182373\n",
      "training error:  0.6835486888885498  and  1.2331215143203735  and  0.9589220285415649\n",
      "total error:  2.8755922317504883\n",
      "training error:  0.675686240196228  and  1.14591646194458  and  0.9664002060890198\n",
      "total error:  2.788002908229828\n",
      "training error:  0.6734055280685425  and  1.284042239189148  and  0.9763164520263672\n",
      "total error:  2.9337642192840576\n",
      "training error:  0.6637598276138306  and  1.1997020244598389  and  1.0064340829849243\n",
      "total error:  2.8698959350585938\n",
      "training error:  0.6500212550163269  and  1.2110044956207275  and  1.1019375324249268\n",
      "total error:  2.962963283061981\n",
      "training error:  0.6649471521377563  and  1.1787045001983643  and  0.9576427340507507\n",
      "total error:  2.8012943863868713\n",
      "training error:  0.6885791420936584  and  1.3119699954986572  and  0.9704955816268921\n",
      "total error:  2.9710447192192078\n",
      "training error:  0.685377836227417  and  1.160187005996704  and  0.9689976572990417\n",
      "total error:  2.814562499523163\n",
      "training error:  0.7010351419448853  and  1.1814846992492676  and  0.9810792207717896\n",
      "total error:  2.8635990619659424\n",
      "training error:  0.6578754186630249  and  1.256962537765503  and  1.056640386581421\n",
      "total error:  2.9714783430099487\n",
      "training error:  0.6697448492050171  and  1.2205547094345093  and  1.0227402448654175\n",
      "total error:  2.913039803504944\n",
      "training error:  0.6953655481338501  and  1.247718334197998  and  0.9970088005065918\n",
      "total error:  2.94009268283844\n",
      "training error:  0.6886193752288818  and  1.2714142799377441  and  0.9701038599014282\n",
      "total error:  2.930137515068054\n",
      "training error:  0.6613245606422424  and  1.165158987045288  and  1.1064083576202393\n",
      "total error:  2.9328919053077698\n",
      "training error:  0.6591622829437256  and  1.1878340244293213  and  1.143923282623291\n",
      "total error:  2.990919589996338\n",
      "training error:  0.6893925666809082  and  1.1836684942245483  and  1.021984338760376\n",
      "total error:  2.8950453996658325\n",
      "training error:  0.6476998329162598  and  1.1885085105895996  and  0.9721993207931519\n",
      "total error:  2.8084076642990112\n",
      "training error:  0.6689534783363342  and  1.1417425870895386  and  0.9774709343910217\n",
      "total error:  2.7881669998168945\n",
      "training error:  0.6624107360839844  and  1.1723767518997192  and  1.0592288970947266\n",
      "total error:  2.89401638507843\n",
      "training error:  0.6573706865310669  and  1.3280833959579468  and  1.008406400680542\n",
      "total error:  2.9938604831695557\n",
      "training error:  0.6887599229812622  and  1.2255887985229492  and  0.9256333112716675\n",
      "total error:  2.839982032775879\n",
      "training error:  0.6665225028991699  and  1.2010397911071777  and  1.070270299911499\n",
      "total error:  2.9378325939178467\n",
      "training error:  0.6666807532310486  and  1.2322626113891602  and  0.9826675057411194\n",
      "total error:  2.881610870361328\n",
      "training error:  0.6872323155403137  and  1.2535550594329834  and  0.9644957780838013\n",
      "total error:  2.9052831530570984\n",
      "training error:  0.6846228837966919  and  1.1597199440002441  and  1.0389986038208008\n",
      "total error:  2.883341431617737\n",
      "training error:  0.6960285305976868  and  1.3795965909957886  and  1.1115131378173828\n",
      "total error:  3.187138259410858\n",
      "training error:  0.6960012912750244  and  1.2021186351776123  and  0.9560543298721313\n",
      "total error:  2.854174256324768\n",
      "training error:  0.6678472757339478  and  1.2104183435440063  and  1.0324435234069824\n",
      "total error:  2.9107091426849365\n",
      "training error:  0.6879216432571411  and  1.2495286464691162  and  0.9727858304977417\n",
      "total error:  2.910236120223999\n",
      "training error:  0.6768856644630432  and  1.2525179386138916  and  1.035274624824524\n",
      "total error:  2.9646782279014587\n",
      "training error:  0.6659862995147705  and  1.1421737670898438  and  0.9801506996154785\n",
      "total error:  2.7883107662200928\n",
      "training error:  0.6667085289955139  and  1.1332316398620605  and  0.9354164004325867\n",
      "total error:  2.735356569290161\n",
      "training error:  0.7029354572296143  and  1.1736865043640137  and  0.9284194707870483\n",
      "total error:  2.8050414323806763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7097716927528381  and  1.2030763626098633  and  0.970677375793457\n",
      "total error:  2.8835254311561584\n",
      "training error:  0.6559072732925415  and  1.3528039455413818  and  1.006397008895874\n",
      "total error:  3.0151082277297974\n",
      "training error:  0.6749445199966431  and  1.2911169528961182  and  0.9928915500640869\n",
      "total error:  2.958953022956848\n",
      "training error:  0.665045976638794  and  1.1995317935943604  and  1.0260416269302368\n",
      "total error:  2.890619397163391\n",
      "training error:  0.6402813196182251  and  1.1333142518997192  and  0.9105044007301331\n",
      "total error:  2.6840999722480774\n",
      "training error:  0.6481939554214478  and  1.1992313861846924  and  0.9412566423416138\n",
      "total error:  2.788681983947754\n",
      "training error:  0.6971312165260315  and  1.2477171421051025  and  0.9856301546096802\n",
      "total error:  2.930478513240814\n",
      "training error:  0.6472312211990356  and  1.1873719692230225  and  0.9324456453323364\n",
      "total error:  2.7670488357543945\n",
      "training error:  0.6635128855705261  and  1.1580743789672852  and  0.9413702487945557\n",
      "total error:  2.762957513332367\n",
      "training error:  0.6926061511039734  and  1.1862834692001343  and  0.9402005076408386\n",
      "total error:  2.8190901279449463\n",
      "training error:  0.6851142048835754  and  1.213732123374939  and  1.0887491703033447\n",
      "total error:  2.987595498561859\n",
      "training error:  0.671649694442749  and  1.2063559293746948  and  0.9554901719093323\n",
      "total error:  2.833495795726776\n",
      "training error:  0.6607155799865723  and  1.3625521659851074  and  1.0543321371078491\n",
      "total error:  3.077599883079529\n",
      "training error:  0.6612976789474487  and  1.1726171970367432  and  0.9578510522842407\n",
      "total error:  2.7917659282684326\n",
      "training error:  0.682891309261322  and  1.3003076314926147  and  1.0185421705245972\n",
      "total error:  3.001741111278534\n",
      "training error:  0.6861295700073242  and  1.2099804878234863  and  0.9923292994499207\n",
      "total error:  2.888439357280731\n",
      "training error:  0.6944374442100525  and  1.2034664154052734  and  1.0250012874603271\n",
      "total error:  2.922905147075653\n",
      "training error:  0.6718073487281799  and  1.2348682880401611  and  0.9881777763366699\n",
      "total error:  2.894853413105011\n",
      "training error:  0.6758096218109131  and  1.1847538948059082  and  0.9353127479553223\n",
      "total error:  2.7958762645721436\n",
      "training error:  0.6694029569625854  and  1.1455532312393188  and  0.9296491146087646\n",
      "total error:  2.744605302810669\n",
      "training error:  0.6794534921646118  and  1.4357587099075317  and  0.9794119000434875\n",
      "total error:  3.094624102115631\n",
      "training error:  0.6634119749069214  and  1.1912713050842285  and  0.9755153656005859\n",
      "total error:  2.830198645591736\n",
      "training error:  0.6537725925445557  and  1.1717207431793213  and  0.965468168258667\n",
      "total error:  2.790961503982544\n",
      "training error:  0.6794365048408508  and  1.2250375747680664  and  1.018690586090088\n",
      "total error:  2.923164665699005\n",
      "training error:  0.6570726633071899  and  1.1843407154083252  and  1.0267937183380127\n",
      "total error:  2.868207097053528\n",
      "training error:  0.6962950229644775  and  1.2198853492736816  and  1.1744561195373535\n",
      "total error:  3.0906364917755127\n",
      "training error:  0.6721731424331665  and  1.1615041494369507  and  0.9388669729232788\n",
      "total error:  2.772544264793396\n",
      "training error:  0.6747477054595947  and  1.1947351694107056  and  1.0003999471664429\n",
      "total error:  2.869882822036743\n",
      "training error:  0.6545405983924866  and  1.1598923206329346  and  0.9278765916824341\n",
      "total error:  2.7423095107078552\n",
      "training error:  0.65840744972229  and  1.1866282224655151  and  0.9530528783798218\n",
      "total error:  2.798088550567627\n",
      "training error:  0.6740899085998535  and  1.189378023147583  and  1.0209910869598389\n",
      "total error:  2.8844590187072754\n",
      "training error:  0.6562255620956421  and  1.16926109790802  and  0.9307088851928711\n",
      "total error:  2.756195545196533\n",
      "training error:  0.6496214270591736  and  1.1694941520690918  and  0.9624612331390381\n",
      "total error:  2.7815768122673035\n",
      "training error:  0.6616525053977966  and  1.4714033603668213  and  1.0976775884628296\n",
      "total error:  3.2307334542274475\n",
      "training error:  0.6562813520431519  and  1.1842968463897705  and  0.9420738220214844\n",
      "total error:  2.7826520204544067\n",
      "training error:  0.6675972938537598  and  1.2136391401290894  and  1.0090731382369995\n",
      "total error:  2.8903095722198486\n",
      "training error:  0.6504225730895996  and  1.1317981481552124  and  0.9425853490829468\n",
      "total error:  2.724806070327759\n",
      "training error:  0.6582076549530029  and  1.1922293901443481  and  0.9842818975448608\n",
      "total error:  2.834718942642212\n",
      "training error:  0.6460254192352295  and  1.1879522800445557  and  0.9614020586013794\n",
      "total error:  2.7953797578811646\n",
      "training error:  0.6378878355026245  and  1.1660089492797852  and  1.005112886428833\n",
      "total error:  2.8090096712112427\n",
      "training error:  0.6847624778747559  and  1.2843276262283325  and  1.0773534774780273\n",
      "total error:  3.0464435815811157\n",
      "training error:  0.6586213707923889  and  1.1675500869750977  and  1.0209112167358398\n",
      "total error:  2.8470826745033264\n",
      "training error:  0.7209978103637695  and  1.1895124912261963  and  1.007167935371399\n",
      "total error:  2.9176782369613647\n",
      "training error:  0.63848876953125  and  1.1030336618423462  and  0.9314184784889221\n",
      "total error:  2.6729409098625183\n",
      "training error:  0.6708254814147949  and  1.3393245935440063  and  1.0210521221160889\n",
      "total error:  3.03120219707489\n",
      "training error:  0.6519989967346191  and  1.3070392608642578  and  0.9899997711181641\n",
      "total error:  2.949038028717041\n",
      "training error:  0.6366558074951172  and  1.1735038757324219  and  0.9467604756355286\n",
      "total error:  2.7569201588630676\n",
      "training error:  0.681577205657959  and  1.2387944459915161  and  0.9106796383857727\n",
      "total error:  2.831051290035248\n",
      "training error:  0.6465388536453247  and  1.1719191074371338  and  0.9673953056335449\n",
      "total error:  2.7858532667160034\n",
      "training error:  0.6384317874908447  and  1.1533808708190918  and  0.9106072187423706\n",
      "total error:  2.702419877052307\n",
      "training error:  0.6405308246612549  and  1.153313398361206  and  0.9565694332122803\n",
      "total error:  2.750413656234741\n",
      "training error:  0.6303876042366028  and  1.1375728845596313  and  0.9270857572555542\n",
      "total error:  2.6950462460517883\n",
      "training error:  0.6562349200248718  and  1.2049949169158936  and  0.9972445368766785\n",
      "total error:  2.858474373817444\n",
      "training error:  0.6506710052490234  and  1.1635196208953857  and  0.9527252912521362\n",
      "total error:  2.7669159173965454\n",
      "training error:  0.6989579200744629  and  1.2820687294006348  and  1.054780125617981\n",
      "total error:  3.0358067750930786\n",
      "training error:  0.635033369064331  and  1.1536352634429932  and  1.038632869720459\n",
      "total error:  2.827301502227783\n",
      "training error:  0.6871287822723389  and  1.2271068096160889  and  0.9696323871612549\n",
      "total error:  2.8838679790496826\n",
      "training error:  0.6798254251480103  and  1.2598464488983154  and  1.0994956493377686\n",
      "total error:  3.0391675233840942\n",
      "training error:  0.6604834198951721  and  1.1642632484436035  and  1.0251286029815674\n",
      "total error:  2.849875271320343\n",
      "training error:  0.6562440395355225  and  1.2321093082427979  and  1.0123809576034546\n",
      "total error:  2.900734305381775\n",
      "training error:  0.6574106216430664  and  1.1579164266586304  and  1.0098915100097656\n",
      "total error:  2.8252185583114624\n",
      "training error:  0.6341497898101807  and  1.186983346939087  and  0.9719768762588501\n",
      "total error:  2.7931100130081177\n",
      "training error:  0.6581637859344482  and  1.1489439010620117  and  0.9614955186843872\n",
      "total error:  2.768603205680847\n",
      "training error:  0.6820111274719238  and  1.1748175621032715  and  0.9443955421447754\n",
      "total error:  2.8012242317199707\n",
      "training error:  0.6436657905578613  and  1.1328773498535156  and  0.9456270933151245\n",
      "total error:  2.7221702337265015\n",
      "training error:  0.6516194343566895  and  1.135633111000061  and  0.9791039824485779\n",
      "total error:  2.7663565278053284\n",
      "training error:  0.6446951627731323  and  1.1695431470870972  and  0.9458110332489014\n",
      "total error:  2.760049343109131\n",
      "training error:  0.6240555644035339  and  1.2705540657043457  and  0.9765448570251465\n",
      "total error:  2.871154487133026\n",
      "training error:  0.6508449912071228  and  1.2666065692901611  and  0.9915271997451782\n",
      "total error:  2.908978760242462\n",
      "training error:  0.6460188031196594  and  1.2455569505691528  and  0.9062173366546631\n",
      "total error:  2.7977930903434753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6901647448539734  and  1.2427270412445068  and  0.9746440052986145\n",
      "total error:  2.9075357913970947\n",
      "training error:  0.646152138710022  and  1.3003041744232178  and  0.9761883616447449\n",
      "total error:  2.9226446747779846\n",
      "training error:  0.6844474077224731  and  1.256443738937378  and  1.0504143238067627\n",
      "total error:  2.9913054704666138\n",
      "training error:  0.6847940683364868  and  1.193699598312378  and  1.0006481409072876\n",
      "total error:  2.8791418075561523\n",
      "training error:  0.640740156173706  and  1.133882999420166  and  0.9328166246414185\n",
      "total error:  2.7074397802352905\n",
      "training error:  0.6335743069648743  and  1.129737377166748  and  0.913902759552002\n",
      "total error:  2.6772144436836243\n",
      "training error:  0.6251822113990784  and  1.1489068269729614  and  0.9998871088027954\n",
      "total error:  2.773976147174835\n",
      "training error:  0.6914986968040466  and  1.1929787397384644  and  0.94221031665802\n",
      "total error:  2.826687753200531\n",
      "training error:  0.6511985659599304  and  1.3147785663604736  and  0.9731962084770203\n",
      "total error:  2.9391733407974243\n",
      "training error:  0.6744608879089355  and  1.5672359466552734  and  1.0895984172821045\n",
      "total error:  3.3312952518463135\n",
      "training error:  0.6697312593460083  and  1.158342957496643  and  0.9969931840896606\n",
      "total error:  2.825067400932312\n",
      "training error:  0.6637324690818787  and  1.2869004011154175  and  1.0975117683410645\n",
      "total error:  3.0481446385383606\n",
      "training error:  0.6372151374816895  and  1.1176186800003052  and  0.9145415425300598\n",
      "total error:  2.6693753600120544\n",
      "training error:  0.6848242282867432  and  1.2620630264282227  and  1.0094307661056519\n",
      "total error:  2.9563180208206177\n",
      "training error:  0.670893669128418  and  1.2520300149917603  and  0.9791265726089478\n",
      "total error:  2.902050256729126\n",
      "training error:  0.6440286636352539  and  1.144031286239624  and  0.937305212020874\n",
      "total error:  2.725365161895752\n",
      "training error:  0.7084019184112549  and  1.4780619144439697  and  1.085495948791504\n",
      "total error:  3.2719597816467285\n",
      "training error:  0.7230418920516968  and  1.2327897548675537  and  1.0341594219207764\n",
      "total error:  2.989991068840027\n",
      "training error:  0.646312952041626  and  1.1982522010803223  and  1.0338149070739746\n",
      "total error:  2.878380060195923\n",
      "training error:  0.6486104726791382  and  1.1996970176696777  and  0.9602813124656677\n",
      "total error:  2.8085888028144836\n",
      "training error:  0.6366748809814453  and  1.1492159366607666  and  1.011324405670166\n",
      "total error:  2.797215223312378\n",
      "training error:  0.6489546298980713  and  1.1208579540252686  and  0.9182473421096802\n",
      "total error:  2.68805992603302\n",
      "training error:  0.6290450692176819  and  1.1477456092834473  and  1.0295937061309814\n",
      "total error:  2.8063843846321106\n",
      "training error:  0.6225512027740479  and  1.1748887300491333  and  0.9688277840614319\n",
      "total error:  2.766267716884613\n",
      "training error:  0.6686186790466309  and  1.1231415271759033  and  0.9769061207771301\n",
      "total error:  2.7686663269996643\n",
      "training error:  0.642428994178772  and  1.2308552265167236  and  0.9898996949195862\n",
      "total error:  2.863183915615082\n",
      "training error:  0.6709964275360107  and  1.1575790643692017  and  0.9745292663574219\n",
      "total error:  2.8031047582626343\n",
      "training error:  0.6628486514091492  and  1.212285041809082  and  1.0179307460784912\n",
      "total error:  2.8930644392967224\n",
      "training error:  0.646510124206543  and  1.1262553930282593  and  0.9539142847061157\n",
      "total error:  2.726679801940918\n",
      "training error:  0.6462893486022949  and  1.1793091297149658  and  1.000563383102417\n",
      "total error:  2.8261618614196777\n",
      "training error:  0.6676479578018188  and  1.1430656909942627  and  1.0108609199523926\n",
      "total error:  2.821574568748474\n",
      "training error:  0.6508632898330688  and  1.185711145401001  and  0.9556789398193359\n",
      "total error:  2.7922533750534058\n",
      "training error:  0.9426530003547668  and  2.650498390197754  and  1.671525239944458\n",
      "total error:  5.264676630496979\n",
      "training error:  0.6812103986740112  and  1.2241414785385132  and  1.051396131515503\n",
      "total error:  2.9567480087280273\n",
      "training error:  0.6466835141181946  and  1.1544852256774902  and  0.9185692071914673\n",
      "total error:  2.719737946987152\n",
      "training error:  0.6495591998100281  and  1.1819766759872437  and  0.9232473373413086\n",
      "total error:  2.7547832131385803\n",
      "training error:  0.6621805429458618  and  1.228426456451416  and  1.1125373840332031\n",
      "total error:  3.003144383430481\n",
      "training error:  0.6715819835662842  and  1.1864352226257324  and  1.0515300035476685\n",
      "total error:  2.909547209739685\n",
      "training error:  0.6503219604492188  and  1.1251225471496582  and  0.9455744624137878\n",
      "total error:  2.721018970012665\n",
      "training error:  0.6754876375198364  and  1.1654608249664307  and  0.9721719026565552\n",
      "total error:  2.8131203651428223\n",
      "training error:  0.6435745358467102  and  1.1840989589691162  and  0.9974910616874695\n",
      "total error:  2.825164556503296\n",
      "training error:  0.6598378419876099  and  1.140917181968689  and  0.8899881839752197\n",
      "total error:  2.6907432079315186\n",
      "training error:  0.6505045890808105  and  1.1814312934875488  and  0.961422324180603\n",
      "total error:  2.7933582067489624\n",
      "training error:  0.6354758143424988  and  1.3066468238830566  and  0.9856956005096436\n",
      "total error:  2.927818238735199\n",
      "training error:  0.6494894623756409  and  1.1218398809432983  and  0.9179505109786987\n",
      "total error:  2.689279854297638\n",
      "training error:  0.6435459852218628  and  1.3886675834655762  and  0.9595721960067749\n",
      "total error:  2.991785764694214\n",
      "training error:  0.7052580118179321  and  1.1927595138549805  and  1.0101921558380127\n",
      "total error:  2.9082096815109253\n",
      "training error:  0.6323928833007812  and  1.1617389917373657  and  0.9285624623298645\n",
      "total error:  2.7226943373680115\n",
      "training error:  0.6920665502548218  and  1.2049286365509033  and  0.9217673540115356\n",
      "total error:  2.8187625408172607\n",
      "training error:  0.6857877373695374  and  1.1668472290039062  and  1.029015064239502\n",
      "total error:  2.8816500306129456\n",
      "training error:  0.6522831916809082  and  1.203150749206543  and  0.93207848072052\n",
      "total error:  2.787512421607971\n",
      "training error:  0.6350058913230896  and  1.2237119674682617  and  1.020905613899231\n",
      "total error:  2.8796234726905823\n",
      "training error:  0.684049129486084  and  1.175727128982544  and  0.928679347038269\n",
      "total error:  2.788455605506897\n",
      "training error:  0.6428245306015015  and  1.1737871170043945  and  1.0131957530975342\n",
      "total error:  2.82980740070343\n",
      "training error:  0.6345876455307007  and  1.423625111579895  and  0.9919140934944153\n",
      "total error:  3.050126850605011\n",
      "training error:  0.6366540193557739  and  1.1637873649597168  and  0.9761521816253662\n",
      "total error:  2.776593565940857\n",
      "training error:  0.6400018930435181  and  1.1280550956726074  and  0.9735074043273926\n",
      "total error:  2.741564393043518\n",
      "training error:  0.6331198215484619  and  1.1881237030029297  and  0.9527002573013306\n",
      "total error:  2.773943781852722\n",
      "training error:  0.6261268258094788  and  1.1121180057525635  and  0.9522945880889893\n",
      "total error:  2.6905394196510315\n",
      "training error:  0.6317285299301147  and  1.2091883420944214  and  0.9827386140823364\n",
      "total error:  2.8236554861068726\n",
      "training error:  0.6634352207183838  and  1.147489309310913  and  0.883224368095398\n",
      "total error:  2.694148898124695\n",
      "training error:  0.6168000102043152  and  1.0905323028564453  and  0.9009029269218445\n",
      "total error:  2.608235239982605\n",
      "training error:  0.6355701684951782  and  1.2327985763549805  and  0.9636958837509155\n",
      "total error:  2.832064628601074\n",
      "training error:  0.6273624897003174  and  1.1243165731430054  and  0.9526255130767822\n",
      "total error:  2.704304575920105\n",
      "training error:  0.6182504296302795  and  1.1619542837142944  and  0.8970232009887695\n",
      "total error:  2.6772279143333435\n",
      "training error:  0.6459990739822388  and  1.1250433921813965  and  0.910607099533081\n",
      "total error:  2.6816495656967163\n",
      "training error:  0.7167601585388184  and  1.2984225749969482  and  1.0693175792694092\n",
      "total error:  3.084500312805176\n",
      "training error:  0.631730854511261  and  1.2288250923156738  and  0.8845937252044678\n",
      "total error:  2.7451496720314026\n",
      "training error:  0.6653329133987427  and  1.1536705493927002  and  0.9497994184494019\n",
      "total error:  2.7688028812408447\n",
      "training error:  0.5995668768882751  and  1.1488919258117676  and  0.943526566028595\n",
      "total error:  2.6919853687286377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6528568267822266  and  1.2293591499328613  and  0.9455841779708862\n",
      "total error:  2.827800154685974\n",
      "training error:  0.6322684288024902  and  1.1228128671646118  and  0.953754186630249\n",
      "total error:  2.708835482597351\n",
      "training error:  0.6425156593322754  and  1.1538352966308594  and  0.9618743658065796\n",
      "total error:  2.7582253217697144\n",
      "training error:  0.6806371212005615  and  1.189725399017334  and  0.9361158609390259\n",
      "total error:  2.8064783811569214\n",
      "training error:  0.6652708649635315  and  1.2302072048187256  and  1.021905779838562\n",
      "total error:  2.917383849620819\n",
      "training error:  0.656363308429718  and  1.1718088388442993  and  0.9855109453201294\n",
      "total error:  2.8136830925941467\n",
      "training error:  0.6682276725769043  and  1.2502787113189697  and  1.152758002281189\n",
      "total error:  3.071264386177063\n",
      "training error:  0.6397720575332642  and  1.2224669456481934  and  0.9515810012817383\n",
      "total error:  2.813820004463196\n",
      "training error:  0.629645824432373  and  1.1645078659057617  and  0.918430507183075\n",
      "total error:  2.7125841975212097\n",
      "training error:  0.6485406756401062  and  1.2352180480957031  and  0.9712584018707275\n",
      "total error:  2.855017125606537\n",
      "training error:  0.6251319646835327  and  1.1279199123382568  and  0.9526242613792419\n",
      "total error:  2.7056761384010315\n",
      "training error:  0.6388319730758667  and  1.2158632278442383  and  1.034358263015747\n",
      "total error:  2.889053463935852\n",
      "training error:  0.664036750793457  and  1.195254921913147  and  1.0554749965667725\n",
      "total error:  2.9147666692733765\n",
      "training error:  0.7933671474456787  and  1.197455644607544  and  1.0067007541656494\n",
      "total error:  2.997523546218872\n",
      "training error:  0.6429598927497864  and  1.1927070617675781  and  0.8854926824569702\n",
      "total error:  2.7211596369743347\n",
      "training error:  0.6393023133277893  and  1.1193854808807373  and  0.9699068665504456\n",
      "total error:  2.728594660758972\n",
      "training error:  0.6412243843078613  and  1.123535394668579  and  0.9219771027565002\n",
      "total error:  2.6867368817329407\n",
      "training error:  0.6249839067459106  and  1.1126904487609863  and  0.9594542384147644\n",
      "total error:  2.6971285939216614\n",
      "training error:  0.6650422811508179  and  1.11005699634552  and  0.9120813012123108\n",
      "total error:  2.6871805787086487\n",
      "training error:  0.676055908203125  and  1.1756041049957275  and  0.9816309213638306\n",
      "total error:  2.833290934562683\n",
      "training error:  0.6511033773422241  and  1.109871506690979  and  0.9195738434791565\n",
      "total error:  2.6805487275123596\n",
      "training error:  0.6538311243057251  and  1.160976529121399  and  0.9550596475601196\n",
      "total error:  2.7698673009872437\n",
      "training error:  0.649004340171814  and  1.1306414604187012  and  0.8903334736824036\n",
      "total error:  2.6699792742729187\n",
      "training error:  0.6264857649803162  and  1.1836838722229004  and  0.9555907845497131\n",
      "total error:  2.7657604217529297\n",
      "training error:  0.6266526579856873  and  1.1436084508895874  and  0.927444338798523\n",
      "total error:  2.6977054476737976\n",
      "training error:  0.6421376466751099  and  1.1932429075241089  and  0.999213695526123\n",
      "total error:  2.834594249725342\n",
      "training error:  0.6269056797027588  and  1.1880486011505127  and  0.9150232672691345\n",
      "total error:  2.729977548122406\n",
      "training error:  0.625522792339325  and  1.1498303413391113  and  0.921000063419342\n",
      "total error:  2.6963531970977783\n",
      "training error:  0.6383864283561707  and  1.0920186042785645  and  0.9076354503631592\n",
      "total error:  2.6380404829978943\n",
      "training error:  0.6372986435890198  and  1.344921350479126  and  0.9953697919845581\n",
      "total error:  2.977589786052704\n",
      "training error:  0.6537096500396729  and  1.1573047637939453  and  1.0386310815811157\n",
      "total error:  2.849645495414734\n",
      "training error:  0.6146682500839233  and  1.1021223068237305  and  0.8939844369888306\n",
      "total error:  2.6107749938964844\n",
      "training error:  0.6310128569602966  and  1.1779156923294067  and  1.0234198570251465\n",
      "total error:  2.83234840631485\n",
      "training error:  0.6247612237930298  and  1.1571295261383057  and  1.0150901079177856\n",
      "total error:  2.796980857849121\n",
      "training error:  0.626315176486969  and  1.1748038530349731  and  1.0805621147155762\n",
      "total error:  2.8816811442375183\n",
      "training error:  0.6188748478889465  and  1.1972951889038086  and  0.9379070997238159\n",
      "total error:  2.754077136516571\n",
      "training error:  0.6278254985809326  and  1.1294424533843994  and  1.0254632234573364\n",
      "total error:  2.7827311754226685\n",
      "training error:  0.6215022206306458  and  1.1424142122268677  and  1.0175749063491821\n",
      "total error:  2.7814913392066956\n",
      "training error:  0.6281087398529053  and  1.1303904056549072  and  0.8963190913200378\n",
      "total error:  2.6548182368278503\n",
      "training error:  0.6163555979728699  and  1.1638532876968384  and  0.9419772624969482\n",
      "total error:  2.7221861481666565\n",
      "training error:  0.7250210046768188  and  1.2036699056625366  and  1.0214056968688965\n",
      "total error:  2.950096607208252\n",
      "training error:  0.6575021147727966  and  1.2939131259918213  and  0.9514540433883667\n",
      "total error:  2.9028692841529846\n",
      "training error:  0.6153684854507446  and  1.1498637199401855  and  0.996919572353363\n",
      "total error:  2.762151777744293\n",
      "training error:  0.6671062707901001  and  1.150728702545166  and  0.9829964637756348\n",
      "total error:  2.800831437110901\n",
      "training error:  0.6740419864654541  and  1.512506127357483  and  0.9765255451202393\n",
      "total error:  3.1630736589431763\n",
      "training error:  0.6185563802719116  and  1.1092250347137451  and  0.8614901304244995\n",
      "total error:  2.5892715454101562\n",
      "training error:  0.6129441261291504  and  1.1270126104354858  and  0.9682720899581909\n",
      "total error:  2.708228826522827\n",
      "training error:  0.6247116327285767  and  1.1340301036834717  and  0.9434522986412048\n",
      "total error:  2.702194035053253\n",
      "training error:  0.6559063792228699  and  1.1047124862670898  and  0.9329035878181458\n",
      "total error:  2.6935224533081055\n",
      "training error:  0.6602506637573242  and  1.1580159664154053  and  0.9404305219650269\n",
      "total error:  2.7586971521377563\n",
      "training error:  0.664303183555603  and  1.1792206764221191  and  0.9384454488754272\n",
      "total error:  2.7819693088531494\n",
      "training error:  0.6289007067680359  and  1.189361333847046  and  1.0047321319580078\n",
      "total error:  2.8229941725730896\n",
      "training error:  0.6505953669548035  and  1.083752989768982  and  0.9182313084602356\n",
      "total error:  2.652579665184021\n",
      "training error:  0.6404362320899963  and  1.1656211614608765  and  0.9713074564933777\n",
      "total error:  2.7773648500442505\n",
      "training error:  0.6794825792312622  and  1.160323977470398  and  0.8987737894058228\n",
      "total error:  2.738580346107483\n",
      "training error:  0.6483334302902222  and  1.1158925294876099  and  0.9892675876617432\n",
      "total error:  2.753493547439575\n",
      "training error:  0.6356314420700073  and  1.2019531726837158  and  0.9910402297973633\n",
      "total error:  2.8286248445510864\n",
      "training error:  0.6608392596244812  and  1.1727672815322876  and  1.0114545822143555\n",
      "total error:  2.8450611233711243\n",
      "training error:  0.6188568472862244  and  1.4613583087921143  and  0.9782930612564087\n",
      "total error:  3.0585082173347473\n",
      "training error:  0.6612014770507812  and  1.3009586334228516  and  0.9819959998130798\n",
      "total error:  2.9441561102867126\n",
      "training error:  0.6141474843025208  and  1.091346263885498  and  0.9473722577095032\n",
      "total error:  2.652866005897522\n",
      "training error:  0.6630085110664368  and  1.1069189310073853  and  0.9259284734725952\n",
      "total error:  2.6958559155464172\n",
      "training error:  0.6434350609779358  and  1.109497308731079  and  0.9253532886505127\n",
      "total error:  2.6782856583595276\n",
      "training error:  0.6692190170288086  and  1.1310632228851318  and  1.03537118434906\n",
      "total error:  2.8356534242630005\n",
      "training error:  0.6109927892684937  and  1.0826815366744995  and  0.9203782081604004\n",
      "total error:  2.6140525341033936\n",
      "training error:  0.6903347969055176  and  1.2001545429229736  and  0.94923996925354\n",
      "total error:  2.8397293090820312\n",
      "training error:  0.6362136602401733  and  1.1336345672607422  and  0.9009939432144165\n",
      "total error:  2.670842170715332\n",
      "training error:  0.6446435451507568  and  1.131464958190918  and  0.9461112022399902\n",
      "total error:  2.722219705581665\n",
      "training error:  0.638102650642395  and  1.105384111404419  and  0.8817405104637146\n",
      "total error:  2.6252272725105286\n",
      "training error:  0.6531262993812561  and  1.1910490989685059  and  1.0200608968734741\n",
      "total error:  2.864236295223236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6296592950820923  and  1.1123582124710083  and  0.9012173414230347\n",
      "total error:  2.6432348489761353\n",
      "training error:  0.6207695007324219  and  1.1285018920898438  and  0.9541512131690979\n",
      "total error:  2.7034226059913635\n",
      "training error:  0.6220389604568481  and  1.1194097995758057  and  0.9633434414863586\n",
      "total error:  2.7047922015190125\n",
      "training error:  0.6231241226196289  and  1.1856775283813477  and  1.0114245414733887\n",
      "total error:  2.8202261924743652\n",
      "training error:  0.6180993318557739  and  1.3820956945419312  and  0.9581710696220398\n",
      "total error:  2.958366096019745\n",
      "training error:  0.6260132789611816  and  1.1586109399795532  and  0.988207221031189\n",
      "total error:  2.772831439971924\n",
      "training error:  0.6181572675704956  and  1.1070677042007446  and  0.9029533267021179\n",
      "total error:  2.628178298473358\n",
      "training error:  0.6378037929534912  and  1.1876260042190552  and  0.9548255205154419\n",
      "total error:  2.7802553176879883\n",
      "training error:  0.620019793510437  and  1.0960140228271484  and  0.918220043182373\n",
      "total error:  2.6342538595199585\n",
      "training error:  0.6020423173904419  and  1.1339585781097412  and  0.8668156862258911\n",
      "total error:  2.602816581726074\n",
      "training error:  0.6323374509811401  and  1.1481465101242065  and  0.9116716384887695\n",
      "total error:  2.692155599594116\n",
      "training error:  0.6384637355804443  and  1.2172216176986694  and  1.043048620223999\n",
      "total error:  2.898733973503113\n",
      "training error:  0.6271152496337891  and  1.1506474018096924  and  0.911395251750946\n",
      "total error:  2.6891579031944275\n",
      "training error:  0.6126651763916016  and  1.167847752571106  and  1.0098025798797607\n",
      "total error:  2.7903155088424683\n",
      "training error:  0.6060601472854614  and  1.1270365715026855  and  0.894524335861206\n",
      "total error:  2.627621054649353\n",
      "training error:  0.6251175403594971  and  1.1080700159072876  and  0.896670937538147\n",
      "total error:  2.6298584938049316\n",
      "training error:  0.6317398548126221  and  1.1566216945648193  and  0.9082202315330505\n",
      "total error:  2.696581780910492\n",
      "training error:  0.6535348892211914  and  1.6306746006011963  and  1.2277722358703613\n",
      "total error:  3.511981725692749\n",
      "training error:  0.6436505317687988  and  1.2074840068817139  and  0.9145632982254028\n",
      "total error:  2.7656978368759155\n",
      "training error:  0.6147137880325317  and  1.1300033330917358  and  0.8945618271827698\n",
      "total error:  2.6392789483070374\n",
      "training error:  0.6224696636199951  and  1.1740162372589111  and  0.9687244296073914\n",
      "total error:  2.7652103304862976\n",
      "training error:  0.6217805743217468  and  1.1094629764556885  and  0.9999277591705322\n",
      "total error:  2.7311713099479675\n",
      "training error:  0.7063051462173462  and  1.184195637702942  and  0.9398053288459778\n",
      "total error:  2.830306112766266\n",
      "training error:  0.6233192682266235  and  1.1796913146972656  and  0.9759994149208069\n",
      "total error:  2.779009997844696\n",
      "training error:  0.6121844053268433  and  1.1278817653656006  and  0.9353508949279785\n",
      "total error:  2.6754170656204224\n",
      "training error:  0.6082822680473328  and  1.1431764364242554  and  1.0408296585083008\n",
      "total error:  2.792288362979889\n",
      "training error:  0.6605536937713623  and  1.1948055028915405  and  1.0021696090698242\n",
      "total error:  2.857528805732727\n",
      "training error:  0.6691877841949463  and  1.314211368560791  and  1.0402488708496094\n",
      "total error:  3.0236480236053467\n",
      "training error:  0.6373765468597412  and  1.264875054359436  and  0.963961124420166\n",
      "total error:  2.8662127256393433\n",
      "training error:  0.6146764755249023  and  1.1299502849578857  and  0.9889320135116577\n",
      "total error:  2.733558773994446\n",
      "training error:  0.6171629428863525  and  1.1599106788635254  and  0.8762540221214294\n",
      "total error:  2.6533276438713074\n",
      "training error:  0.6162779331207275  and  1.1267883777618408  and  1.0030150413513184\n",
      "total error:  2.7460813522338867\n",
      "training error:  0.6173198819160461  and  1.2280611991882324  and  0.9966655373573303\n",
      "total error:  2.842046618461609\n",
      "training error:  0.6099486351013184  and  1.0690780878067017  and  0.933742105960846\n",
      "total error:  2.612768828868866\n",
      "training error:  0.6036324501037598  and  1.0920228958129883  and  0.9475794434547424\n",
      "total error:  2.6432347893714905\n",
      "training error:  0.6017640233039856  and  1.0831949710845947  and  0.9419854283332825\n",
      "total error:  2.626944422721863\n",
      "training error:  0.6008512377738953  and  1.132399082183838  and  0.9340389370918274\n",
      "total error:  2.6672892570495605\n",
      "training error:  0.6220552921295166  and  1.0909534692764282  and  0.928410530090332\n",
      "total error:  2.641419291496277\n",
      "training error:  0.5970370769500732  and  1.128023624420166  and  0.9863791465759277\n",
      "total error:  2.711439847946167\n",
      "training error:  0.6187962293624878  and  1.1351255178451538  and  1.0428369045257568\n",
      "total error:  2.7967586517333984\n",
      "training error:  0.6462923288345337  and  1.116028070449829  and  0.9426407217979431\n",
      "total error:  2.704961121082306\n",
      "training error:  0.6183314323425293  and  1.155664086341858  and  0.9860975742340088\n",
      "total error:  2.760093092918396\n",
      "training error:  0.6543569564819336  and  1.1632623672485352  and  0.9219932556152344\n",
      "total error:  2.739612579345703\n",
      "training error:  0.636484682559967  and  1.118082046508789  and  0.9474830031394958\n",
      "total error:  2.702049732208252\n",
      "training error:  0.6185623407363892  and  1.1529815196990967  and  0.9188109636306763\n",
      "total error:  2.690354824066162\n",
      "training error:  0.5925490856170654  and  1.0618269443511963  and  0.926893949508667\n",
      "total error:  2.5812699794769287\n",
      "training error:  0.6197429895401001  and  1.1427299976348877  and  0.934685468673706\n",
      "total error:  2.697158455848694\n",
      "training error:  0.6423835754394531  and  1.0980770587921143  and  0.8774557709693909\n",
      "total error:  2.6179164052009583\n",
      "training error:  0.6414439678192139  and  1.2305755615234375  and  0.9045546650886536\n",
      "total error:  2.776574194431305\n",
      "training error:  0.6283648014068604  and  1.196448802947998  and  1.004744529724121\n",
      "total error:  2.8295581340789795\n",
      "training error:  0.6285068988800049  and  1.1299941539764404  and  0.8967165350914001\n",
      "total error:  2.6552175879478455\n",
      "training error:  0.6446129083633423  and  1.1833434104919434  and  0.9413225650787354\n",
      "total error:  2.769278883934021\n",
      "training error:  0.6539176106452942  and  1.130308747291565  and  0.9838507771492004\n",
      "total error:  2.7680771350860596\n",
      "training error:  0.6218936443328857  and  1.1238830089569092  and  0.9991796016693115\n",
      "total error:  2.7449562549591064\n",
      "training error:  0.7030112147331238  and  1.1986502408981323  and  0.9759270548820496\n",
      "total error:  2.8775885105133057\n",
      "training error:  0.6238725185394287  and  1.0996824502944946  and  0.940097451210022\n",
      "total error:  2.6636524200439453\n",
      "training error:  0.6268895268440247  and  1.0525870323181152  and  0.9133933186531067\n",
      "total error:  2.5928698778152466\n",
      "training error:  0.6878842711448669  and  1.1915867328643799  and  1.1060606241226196\n",
      "total error:  2.9855316281318665\n",
      "training error:  0.6220763921737671  and  1.1308921575546265  and  0.8653931021690369\n",
      "total error:  2.6183616518974304\n",
      "training error:  0.6270498037338257  and  1.1863576173782349  and  0.9572292566299438\n",
      "total error:  2.7706366777420044\n",
      "training error:  0.6379073858261108  and  1.2501755952835083  and  0.9423098564147949\n",
      "total error:  2.830392837524414\n",
      "training error:  0.6431711912155151  and  1.322604775428772  and  0.9753698110580444\n",
      "total error:  2.9411457777023315\n",
      "training error:  0.636323094367981  and  1.1203175783157349  and  0.9463632702827454\n",
      "total error:  2.703003942966461\n",
      "training error:  0.6681157350540161  and  1.328614354133606  and  1.106123924255371\n",
      "total error:  3.102854013442993\n",
      "training error:  0.6167792081832886  and  1.1719038486480713  and  0.9287295341491699\n",
      "total error:  2.71741259098053\n",
      "training error:  0.6301226615905762  and  1.0830087661743164  and  0.8512921929359436\n",
      "total error:  2.564423620700836\n",
      "training error:  0.6228197813034058  and  1.1504521369934082  and  0.9747067093849182\n",
      "total error:  2.747978627681732\n",
      "training error:  0.6240081787109375  and  1.1504849195480347  and  0.986735463142395\n",
      "total error:  2.761228561401367\n",
      "training error:  0.5879217386245728  and  1.0950381755828857  and  0.9091877341270447\n",
      "total error:  2.592147648334503\n",
      "training error:  0.614982545375824  and  1.104333519935608  and  0.9459315538406372\n",
      "total error:  2.665247619152069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6189955472946167  and  1.0743584632873535  and  0.9079983830451965\n",
      "total error:  2.6013523936271667\n",
      "training error:  0.6312130689620972  and  1.1470919847488403  and  0.8951655626296997\n",
      "total error:  2.673470616340637\n",
      "training error:  0.6614522933959961  and  1.2211087942123413  and  0.9311130046844482\n",
      "total error:  2.8136740922927856\n",
      "training error:  0.6108990907669067  and  1.1266937255859375  and  0.9494856595993042\n",
      "total error:  2.6870784759521484\n",
      "training error:  0.5999575853347778  and  1.090692400932312  and  0.9276531338691711\n",
      "total error:  2.618303120136261\n",
      "training error:  0.6165540814399719  and  1.1291528940200806  and  0.9687210917472839\n",
      "total error:  2.7144280672073364\n",
      "training error:  0.6345139741897583  and  1.150397777557373  and  0.9373883008956909\n",
      "total error:  2.7223000526428223\n",
      "training error:  0.6250115036964417  and  1.1456294059753418  and  0.9294856786727905\n",
      "total error:  2.700126588344574\n",
      "training error:  0.604793131351471  and  1.2066844701766968  and  0.9636667370796204\n",
      "total error:  2.775144338607788\n",
      "training error:  0.6061156988143921  and  1.1497230529785156  and  0.968471884727478\n",
      "total error:  2.7243106365203857\n",
      "training error:  0.6284266114234924  and  1.1875983476638794  and  0.9705743193626404\n",
      "total error:  2.786599278450012\n",
      "training error:  0.6121671199798584  and  1.1324995756149292  and  1.1414289474487305\n",
      "total error:  2.886095643043518\n",
      "training error:  0.5830910801887512  and  1.069358468055725  and  0.8697187900543213\n",
      "total error:  2.5221683382987976\n",
      "training error:  0.6154335737228394  and  1.103454828262329  and  0.9259926676750183\n",
      "total error:  2.6448810696601868\n",
      "training error:  0.6219727993011475  and  1.2643609046936035  and  1.212924838066101\n",
      "total error:  3.099258542060852\n",
      "training error:  0.6384444832801819  and  1.1774437427520752  and  0.904046893119812\n",
      "total error:  2.719935119152069\n",
      "training error:  0.6159679889678955  and  1.111925721168518  and  0.9526193141937256\n",
      "total error:  2.680513024330139\n",
      "training error:  0.5934013724327087  and  1.1220581531524658  and  0.9285163879394531\n",
      "total error:  2.6439759135246277\n",
      "training error:  0.6329642534255981  and  1.1006726026535034  and  0.9091459512710571\n",
      "total error:  2.6427828073501587\n",
      "training error:  0.6250127553939819  and  1.1312146186828613  and  0.955041766166687\n",
      "total error:  2.7112691402435303\n",
      "training error:  0.5872502326965332  and  1.1592397689819336  and  1.0013399124145508\n",
      "total error:  2.7478299140930176\n",
      "training error:  0.6050035953521729  and  1.0840842723846436  and  0.9585747718811035\n",
      "total error:  2.64766263961792\n",
      "training error:  0.6296418309211731  and  1.1185407638549805  and  0.9092434048652649\n",
      "total error:  2.6574259996414185\n",
      "training error:  0.6332945823669434  and  1.334256887435913  and  1.0348297357559204\n",
      "total error:  3.002381205558777\n",
      "training error:  0.5992445349693298  and  1.093587875366211  and  0.9655293226242065\n",
      "total error:  2.6583617329597473\n",
      "training error:  0.635453462600708  and  1.1720333099365234  and  0.9580469131469727\n",
      "total error:  2.765533685684204\n",
      "training error:  0.6094560623168945  and  1.1376919746398926  and  0.9484227895736694\n",
      "total error:  2.6955708265304565\n",
      "training error:  0.6216780543327332  and  1.1320593357086182  and  0.9456708431243896\n",
      "total error:  2.699408233165741\n",
      "training error:  0.6067954301834106  and  1.1531355381011963  and  0.8710028529167175\n",
      "total error:  2.6309338212013245\n",
      "training error:  0.6145509481430054  and  1.1246147155761719  and  0.9079045057296753\n",
      "total error:  2.6470701694488525\n",
      "training error:  0.613265872001648  and  1.0834071636199951  and  0.8473881483078003\n",
      "total error:  2.5440611839294434\n",
      "training error:  0.6038092970848083  and  1.0870610475540161  and  0.931916356086731\n",
      "total error:  2.6227867007255554\n",
      "training error:  0.5993914604187012  and  1.1283951997756958  and  0.8735819458961487\n",
      "total error:  2.6013686060905457\n",
      "training error:  0.6042673587799072  and  1.0770906209945679  and  0.9113726019859314\n",
      "total error:  2.5927305817604065\n",
      "training error:  0.6040019989013672  and  1.1128053665161133  and  0.862922728061676\n",
      "total error:  2.5797300934791565\n",
      "training error:  0.5850573778152466  and  1.1110855340957642  and  0.9474697113037109\n",
      "total error:  2.6436126232147217\n",
      "training error:  0.5987457633018494  and  1.128247857093811  and  0.9603722095489502\n",
      "total error:  2.6873658299446106\n",
      "training error:  0.5744490623474121  and  1.1001378297805786  and  0.8938227295875549\n",
      "total error:  2.5684096217155457\n",
      "training error:  0.6390383839607239  and  1.1433756351470947  and  0.9598777294158936\n",
      "total error:  2.742291748523712\n",
      "training error:  0.5878914594650269  and  1.0784828662872314  and  0.878132700920105\n",
      "total error:  2.5445070266723633\n",
      "training error:  0.622347354888916  and  1.1501103639602661  and  0.8972994685173035\n",
      "total error:  2.6697571873664856\n",
      "training error:  0.6207830905914307  and  1.2522106170654297  and  0.9794031381607056\n",
      "total error:  2.852396845817566\n",
      "training error:  0.607952356338501  and  1.090644359588623  and  1.0161646604537964\n",
      "total error:  2.7147613763809204\n",
      "training error:  0.5981554388999939  and  1.1476855278015137  and  0.9933425188064575\n",
      "total error:  2.739183485507965\n",
      "training error:  0.6007890701293945  and  1.1265647411346436  and  0.9590893983840942\n",
      "total error:  2.6864432096481323\n",
      "training error:  0.6676003932952881  and  1.2374329566955566  and  0.9342886209487915\n",
      "total error:  2.8393219709396362\n",
      "training error:  0.6397327184677124  and  1.1823394298553467  and  0.9517585039138794\n",
      "total error:  2.7738306522369385\n",
      "training error:  0.6425179839134216  and  1.2229769229888916  and  1.0735650062561035\n",
      "total error:  2.9390599131584167\n",
      "training error:  0.609835684299469  and  1.145090103149414  and  0.8907054662704468\n",
      "total error:  2.64563125371933\n",
      "training error:  0.6135696172714233  and  1.139652967453003  and  1.0641114711761475\n",
      "total error:  2.8173340559005737\n",
      "training error:  0.587045431137085  and  1.1013187170028687  and  0.9045301675796509\n",
      "total error:  2.5928943157196045\n",
      "training error:  0.6076561808586121  and  1.1266810894012451  and  0.9071793556213379\n",
      "total error:  2.641516625881195\n",
      "training error:  0.6117029190063477  and  1.0830929279327393  and  0.8923206329345703\n",
      "total error:  2.5871164798736572\n",
      "training error:  0.6202800273895264  and  1.1226134300231934  and  0.9199997186660767\n",
      "total error:  2.6628931760787964\n",
      "training error:  0.6212960481643677  and  1.1041998863220215  and  0.8648645281791687\n",
      "total error:  2.590360462665558\n",
      "training error:  0.6443774700164795  and  1.2150230407714844  and  0.9222190380096436\n",
      "total error:  2.7816195487976074\n",
      "training error:  0.5830292701721191  and  1.070202112197876  and  0.9349755048751831\n",
      "total error:  2.5882068872451782\n",
      "training error:  0.586017370223999  and  1.0771929025650024  and  0.9159815311431885\n",
      "total error:  2.57919180393219\n",
      "training error:  0.5969909429550171  and  1.0831506252288818  and  0.8723008036613464\n",
      "total error:  2.5524423718452454\n",
      "training error:  0.638020396232605  and  1.1649110317230225  and  1.0058388710021973\n",
      "total error:  2.8087702989578247\n",
      "training error:  0.6032907962799072  and  1.0910276174545288  and  0.9394928812980652\n",
      "total error:  2.633811295032501\n",
      "training error:  0.6070159673690796  and  1.2357343435287476  and  0.9272394180297852\n",
      "total error:  2.7699897289276123\n",
      "training error:  0.5954115390777588  and  1.0940560102462769  and  1.016162395477295\n",
      "total error:  2.7056299448013306\n",
      "training error:  0.5846342444419861  and  1.0624722242355347  and  0.8968071937561035\n",
      "total error:  2.5439136624336243\n",
      "training error:  0.6073320508003235  and  1.0911895036697388  and  0.9081946611404419\n",
      "total error:  2.606716215610504\n",
      "training error:  0.5802533030509949  and  1.0952056646347046  and  0.9123072624206543\n",
      "total error:  2.5877662301063538\n",
      "training error:  0.5852572917938232  and  1.0976052284240723  and  1.0657570362091064\n",
      "total error:  2.748619556427002\n",
      "training error:  0.613483726978302  and  1.1354405879974365  and  0.9578859806060791\n",
      "total error:  2.7068102955818176\n",
      "training error:  0.6165557503700256  and  1.12813138961792  and  0.9454132318496704\n",
      "total error:  2.690100371837616\n",
      "training error:  0.5689256191253662  and  1.058780550956726  and  0.885953962802887\n",
      "total error:  2.5136601328849792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.590773344039917  and  1.0679402351379395  and  0.9013017416000366\n",
      "total error:  2.560015320777893\n",
      "training error:  0.5844342112541199  and  1.2073030471801758  and  0.962181806564331\n",
      "total error:  2.7539190649986267\n",
      "training error:  0.5687960386276245  and  1.0959392786026  and  1.0307977199554443\n",
      "total error:  2.695533037185669\n",
      "training error:  0.5851587653160095  and  1.0840516090393066  and  0.8800610303878784\n",
      "total error:  2.5492714047431946\n",
      "training error:  0.6344858407974243  and  1.683504581451416  and  1.024016261100769\n",
      "total error:  3.3420066833496094\n",
      "training error:  0.6080040335655212  and  1.0964322090148926  and  0.9907436370849609\n",
      "total error:  2.6951798796653748\n",
      "training error:  0.6123466491699219  and  1.1005594730377197  and  0.9320604205131531\n",
      "total error:  2.6449665427207947\n",
      "training error:  0.5832064151763916  and  1.0985394716262817  and  0.9107155799865723\n",
      "total error:  2.5924614667892456\n",
      "training error:  0.6010260581970215  and  1.0786491632461548  and  0.9441417455673218\n",
      "total error:  2.623816967010498\n",
      "training error:  0.5524556040763855  and  1.0618003606796265  and  0.8999897241592407\n",
      "total error:  2.5142456889152527\n",
      "training error:  0.5899986028671265  and  1.0553090572357178  and  0.8831890225410461\n",
      "total error:  2.5284966826438904\n",
      "training error:  0.6251072883605957  and  1.267988681793213  and  0.9231861233711243\n",
      "total error:  2.816282093524933\n",
      "training error:  0.5974684953689575  and  1.0797221660614014  and  0.9177435040473938\n",
      "total error:  2.5949341654777527\n",
      "training error:  0.592761754989624  and  1.0958175659179688  and  0.9012637734413147\n",
      "total error:  2.5898430943489075\n",
      "training error:  0.6091718673706055  and  1.1674754619598389  and  0.8975666761398315\n",
      "total error:  2.674214005470276\n",
      "training error:  0.6112783551216125  and  1.077168583869934  and  0.8699150681495667\n",
      "total error:  2.5583620071411133\n",
      "training error:  0.5983926653862  and  1.202174186706543  and  1.008512020111084\n",
      "total error:  2.809078872203827\n",
      "training error:  0.585072934627533  and  1.1261886358261108  and  0.9004185199737549\n",
      "total error:  2.6116800904273987\n",
      "training error:  0.5768550634384155  and  1.106641173362732  and  0.888468861579895\n",
      "total error:  2.5719650983810425\n",
      "training error:  0.640007734298706  and  1.147524118423462  and  0.9389348030090332\n",
      "total error:  2.726466655731201\n",
      "training error:  0.6244712471961975  and  1.1872148513793945  and  0.8732255697250366\n",
      "total error:  2.6849116683006287\n",
      "training error:  0.6074613928794861  and  1.0967681407928467  and  0.9426636695861816\n",
      "total error:  2.6468932032585144\n",
      "training error:  0.5793286561965942  and  1.087073802947998  and  0.9919518232345581\n",
      "total error:  2.6583542823791504\n",
      "training error:  0.602480411529541  and  1.1728335618972778  and  1.0905137062072754\n",
      "total error:  2.8658276796340942\n",
      "training error:  0.6182593107223511  and  1.0842204093933105  and  0.9061521291732788\n",
      "total error:  2.6086318492889404\n",
      "training error:  0.6196680068969727  and  1.250685691833496  and  1.0230774879455566\n",
      "total error:  2.8934311866760254\n",
      "training error:  0.5909017324447632  and  1.0595568418502808  and  0.8553433418273926\n",
      "total error:  2.5058019161224365\n",
      "training error:  0.6034033298492432  and  1.129366159439087  and  0.9345400333404541\n",
      "total error:  2.667309522628784\n",
      "training error:  0.6071560382843018  and  1.1225695610046387  and  0.8949673771858215\n",
      "total error:  2.624692976474762\n",
      "training error:  0.585708737373352  and  1.2751058340072632  and  1.1017124652862549\n",
      "total error:  2.96252703666687\n",
      "training error:  0.6271263957023621  and  1.129323124885559  and  1.0274533033370972\n",
      "total error:  2.7839028239250183\n",
      "training error:  0.6100277900695801  and  1.1951937675476074  and  0.900457501411438\n",
      "total error:  2.7056790590286255\n",
      "training error:  0.6087608337402344  and  1.2532501220703125  and  0.9789093732833862\n",
      "total error:  2.840920329093933\n",
      "training error:  0.5966511368751526  and  1.145655632019043  and  0.9422166347503662\n",
      "total error:  2.6845234036445618\n",
      "training error:  0.6083366870880127  and  1.168045997619629  and  0.9864751100540161\n",
      "total error:  2.7628577947616577\n",
      "training error:  0.6623988151550293  and  1.1579108238220215  and  0.9134714603424072\n",
      "total error:  2.733781099319458\n",
      "training error:  0.8006191849708557  and  1.430401086807251  and  1.1748104095458984\n",
      "total error:  3.405830681324005\n",
      "training error:  0.6702673435211182  and  1.2474923133850098  and  1.0069079399108887\n",
      "total error:  2.9246675968170166\n",
      "training error:  0.5981088876724243  and  1.0860893726348877  and  0.9112595319747925\n",
      "total error:  2.5954577922821045\n",
      "training error:  0.5903332233428955  and  1.1889287233352661  and  0.9254282116889954\n",
      "total error:  2.704690158367157\n",
      "training error:  0.6108520030975342  and  1.128348469734192  and  0.9153411388397217\n",
      "total error:  2.6545416116714478\n",
      "training error:  0.5936664938926697  and  1.1500937938690186  and  0.9480076432228088\n",
      "total error:  2.691767930984497\n",
      "training error:  0.5719414353370667  and  1.0569628477096558  and  0.8415868282318115\n",
      "total error:  2.470491111278534\n",
      "training error:  0.5846694707870483  and  1.0494422912597656  and  0.8495911359786987\n",
      "total error:  2.4837028980255127\n",
      "training error:  0.5852522850036621  and  1.08146333694458  and  0.8541697263717651\n",
      "total error:  2.5208853483200073\n",
      "training error:  0.5748109817504883  and  1.2474844455718994  and  0.9216839075088501\n",
      "total error:  2.743979334831238\n",
      "training error:  0.6281558275222778  and  1.277695894241333  and  1.0534639358520508\n",
      "total error:  2.9593156576156616\n",
      "training error:  0.6053049564361572  and  1.1017968654632568  and  0.942023515701294\n",
      "total error:  2.649125337600708\n",
      "training error:  0.6323965787887573  and  1.1279305219650269  and  0.9746484160423279\n",
      "total error:  2.734975516796112\n",
      "training error:  0.6129992008209229  and  1.1598610877990723  and  0.9768677353858948\n",
      "total error:  2.74972802400589\n",
      "training error:  0.6121624708175659  and  1.1196346282958984  and  0.956850528717041\n",
      "total error:  2.6886476278305054\n",
      "training error:  0.6226013898849487  and  1.1862150430679321  and  0.9883251190185547\n",
      "total error:  2.7971415519714355\n",
      "training error:  0.66957688331604  and  1.2480149269104004  and  0.9376332759857178\n",
      "total error:  2.855225086212158\n",
      "training error:  0.5767002701759338  and  1.1197760105133057  and  0.9006279110908508\n",
      "total error:  2.5971041917800903\n",
      "training error:  0.6083934307098389  and  1.130159616470337  and  0.8596178293228149\n",
      "total error:  2.5981708765029907\n",
      "training error:  0.6185083389282227  and  1.1102418899536133  and  0.8673592209815979\n",
      "total error:  2.596109449863434\n",
      "training error:  0.613579511642456  and  1.139336347579956  and  0.9122603535652161\n",
      "total error:  2.665176212787628\n",
      "training error:  0.6210081577301025  and  1.3098878860473633  and  1.0224555730819702\n",
      "total error:  2.953351616859436\n",
      "training error:  0.5819042921066284  and  1.0754669904708862  and  0.9442038536071777\n",
      "total error:  2.6015751361846924\n",
      "training error:  0.5672944188117981  and  1.049321174621582  and  0.9024940729141235\n",
      "total error:  2.5191096663475037\n",
      "training error:  0.6521670818328857  and  1.1470179557800293  and  0.989192008972168\n",
      "total error:  2.788377046585083\n",
      "training error:  0.6268765926361084  and  1.1840943098068237  and  0.921745777130127\n",
      "total error:  2.732716679573059\n",
      "training error:  0.6050559878349304  and  1.0979920625686646  and  0.9022643566131592\n",
      "total error:  2.605312407016754\n",
      "training error:  0.5650270581245422  and  1.0606098175048828  and  0.9639098048210144\n",
      "total error:  2.5895466804504395\n",
      "training error:  0.6070001125335693  and  1.0747654438018799  and  0.897630512714386\n",
      "total error:  2.579396069049835\n",
      "training error:  0.6090506315231323  and  1.096822738647461  and  0.8905396461486816\n",
      "total error:  2.596413016319275\n",
      "training error:  0.6363477110862732  and  1.1640655994415283  and  0.9184825420379639\n",
      "total error:  2.7188958525657654\n",
      "training error:  0.5680103302001953  and  1.131954312324524  and  0.8740043640136719\n",
      "total error:  2.573969006538391\n",
      "training error:  0.6329909563064575  and  1.1247098445892334  and  0.9412815570831299\n",
      "total error:  2.698982357978821\n",
      "training error:  0.6001834869384766  and  1.065334439277649  and  0.8849181532859802\n",
      "total error:  2.5504360795021057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.5843263268470764  and  1.070948600769043  and  0.8959866762161255\n",
      "total error:  2.551261603832245\n",
      "training error:  0.6053873300552368  and  1.0791934728622437  and  0.8962466716766357\n",
      "total error:  2.580827474594116\n",
      "training error:  0.6082632541656494  and  1.1611309051513672  and  0.931553065776825\n",
      "total error:  2.7009472250938416\n",
      "training error:  0.5819931030273438  and  1.0872492790222168  and  0.9279971122741699\n",
      "total error:  2.5972394943237305\n",
      "training error:  0.6512244939804077  and  1.3085980415344238  and  1.049906611442566\n",
      "total error:  3.0097291469573975\n",
      "training error:  0.5941983461380005  and  1.07937490940094  and  1.0505716800689697\n",
      "total error:  2.72414493560791\n",
      "training error:  0.6145918369293213  and  1.120802402496338  and  0.980650782585144\n",
      "total error:  2.7160450220108032\n",
      "training error:  0.6379356384277344  and  1.661158800125122  and  1.0834890604019165\n",
      "total error:  3.382583498954773\n",
      "training error:  0.6112608909606934  and  1.1401499509811401  and  0.9377003908157349\n",
      "total error:  2.6891112327575684\n",
      "training error:  0.6080833673477173  and  1.1304954290390015  and  0.905462384223938\n",
      "total error:  2.6440411806106567\n",
      "training error:  0.6151795983314514  and  1.142827033996582  and  0.8653420805931091\n",
      "total error:  2.6233487129211426\n",
      "training error:  0.6389425992965698  and  1.173515796661377  and  0.8846977353096008\n",
      "total error:  2.6971561312675476\n",
      "training error:  0.6730912923812866  and  1.1581315994262695  and  0.8948866128921509\n",
      "total error:  2.726109504699707\n",
      "training error:  0.6027252674102783  and  1.0774571895599365  and  0.884961724281311\n",
      "total error:  2.565144181251526\n",
      "training error:  0.5867148637771606  and  1.052213430404663  and  0.9197095632553101\n",
      "total error:  2.558637857437134\n",
      "training error:  0.6065853834152222  and  1.1037245988845825  and  0.9174660444259644\n",
      "total error:  2.627776026725769\n",
      "training error:  0.5800511837005615  and  1.1321351528167725  and  0.8821254372596741\n",
      "total error:  2.594311773777008\n",
      "training error:  0.6189442873001099  and  1.4297099113464355  and  0.9192577600479126\n",
      "total error:  2.967911958694458\n",
      "training error:  0.6281623244285583  and  1.1859043836593628  and  0.9529715180397034\n",
      "total error:  2.7670382261276245\n",
      "training error:  0.5783330202102661  and  1.0649676322937012  and  0.9368422031402588\n",
      "total error:  2.580142855644226\n",
      "training error:  0.6063359379768372  and  1.1664817333221436  and  0.9274177551269531\n",
      "total error:  2.700235426425934\n",
      "training error:  0.7872495055198669  and  1.3711847066879272  and  1.2430896759033203\n",
      "total error:  3.4015238881111145\n",
      "training error:  0.6481248140335083  and  1.1480755805969238  and  0.9349092245101929\n",
      "total error:  2.731109619140625\n",
      "training error:  0.6254227161407471  and  1.1488802433013916  and  1.0105476379394531\n",
      "total error:  2.784850597381592\n",
      "training error:  0.597122311592102  and  1.096268653869629  and  0.8904809355735779\n",
      "total error:  2.583871901035309\n",
      "training error:  0.59522944688797  and  1.114732027053833  and  0.9324417114257812\n",
      "total error:  2.6424031853675842\n",
      "training error:  0.6309149265289307  and  1.1286845207214355  and  0.9012987613677979\n",
      "total error:  2.660898208618164\n",
      "training error:  0.5805625915527344  and  1.123186469078064  and  0.8769313097000122\n",
      "total error:  2.5806803703308105\n",
      "training error:  0.5808075070381165  and  1.079863429069519  and  0.8705755472183228\n",
      "total error:  2.5312464833259583\n",
      "training error:  0.5792515277862549  and  1.1406290531158447  and  1.0335943698883057\n",
      "total error:  2.7534749507904053\n",
      "training error:  0.5912409424781799  and  1.0595660209655762  and  0.8644146919250488\n",
      "total error:  2.515221655368805\n",
      "training error:  0.6283996105194092  and  1.1055586338043213  and  1.0481663942337036\n",
      "total error:  2.782124638557434\n",
      "training error:  0.5805999040603638  and  1.1475603580474854  and  0.8870608806610107\n",
      "total error:  2.61522114276886\n",
      "training error:  0.6345769166946411  and  1.0845284461975098  and  0.8934711217880249\n",
      "total error:  2.612576484680176\n",
      "training error:  0.5778056383132935  and  1.2014490365982056  and  0.9223178625106812\n",
      "total error:  2.70157253742218\n",
      "training error:  0.6102797985076904  and  1.1368016004562378  and  0.9452146887779236\n",
      "total error:  2.692296087741852\n",
      "training error:  0.5592188239097595  and  1.1387693881988525  and  0.9138703346252441\n",
      "total error:  2.611858546733856\n",
      "training error:  0.581896185874939  and  1.1096725463867188  and  0.9084759950637817\n",
      "total error:  2.6000447273254395\n",
      "training error:  0.5737656354904175  and  1.093163251876831  and  0.8870803713798523\n",
      "total error:  2.554009258747101\n",
      "training error:  0.5918918251991272  and  1.1390595436096191  and  0.8380163908004761\n",
      "total error:  2.5689677596092224\n",
      "training error:  0.5814051628112793  and  1.0668379068374634  and  0.8673665523529053\n",
      "total error:  2.515609622001648\n",
      "training error:  0.5910608172416687  and  1.1075167655944824  and  0.9388799667358398\n",
      "total error:  2.637457549571991\n",
      "training error:  0.5861912965774536  and  1.1119022369384766  and  0.8859332203865051\n",
      "total error:  2.5840267539024353\n",
      "training error:  0.5976805686950684  and  1.0885250568389893  and  0.8973983526229858\n",
      "total error:  2.5836039781570435\n",
      "training error:  0.6182400584220886  and  1.0561071634292603  and  0.885472297668457\n",
      "total error:  2.559819519519806\n",
      "training error:  0.5577067136764526  and  1.047673225402832  and  0.8820037245750427\n",
      "total error:  2.4873836636543274\n",
      "training error:  0.5607486963272095  and  1.0597939491271973  and  0.885566771030426\n",
      "total error:  2.5061094164848328\n",
      "training error:  0.5879050493240356  and  1.042351245880127  and  0.854103684425354\n",
      "total error:  2.4843599796295166\n",
      "training error:  0.5961995124816895  and  1.1038048267364502  and  0.834947943687439\n",
      "total error:  2.5349522829055786\n",
      "training error:  0.5933235883712769  and  1.1697485446929932  and  0.9902961850166321\n",
      "total error:  2.753368318080902\n",
      "training error:  0.5977681279182434  and  1.1026790142059326  and  0.8882975578308105\n",
      "total error:  2.5887446999549866\n",
      "training error:  0.5741881132125854  and  1.0549683570861816  and  0.8921750783920288\n",
      "total error:  2.521331548690796\n",
      "training error:  0.6072466969490051  and  1.0529110431671143  and  0.8634260296821594\n",
      "total error:  2.523583769798279\n",
      "training error:  0.5971707105636597  and  1.0718560218811035  and  0.8901475667953491\n",
      "total error:  2.5591742992401123\n",
      "training error:  0.5846979022026062  and  1.0927956104278564  and  0.8972650170326233\n",
      "total error:  2.574758529663086\n",
      "training error:  0.5899954438209534  and  1.0803883075714111  and  0.8794641494750977\n",
      "total error:  2.549847900867462\n",
      "training error:  0.5853111147880554  and  1.0683298110961914  and  0.9264096617698669\n",
      "total error:  2.5800505876541138\n",
      "training error:  0.5855553150177002  and  1.070875644683838  and  0.8620688915252686\n",
      "total error:  2.5184998512268066\n",
      "training error:  0.6008601188659668  and  1.1280115842819214  and  0.9058663845062256\n",
      "total error:  2.6347380876541138\n",
      "training error:  0.5976546406745911  and  1.0617936849594116  and  0.9251257181167603\n",
      "total error:  2.584574043750763\n",
      "training error:  0.5864382982254028  and  1.0718953609466553  and  0.9750200510025024\n",
      "total error:  2.6333537101745605\n",
      "training error:  0.5788354277610779  and  1.064611792564392  and  0.9043785333633423\n",
      "total error:  2.5478257536888123\n",
      "training error:  0.5903180837631226  and  1.0906617641448975  and  0.8488630056381226\n",
      "total error:  2.5298428535461426\n",
      "training error:  0.5979882478713989  and  1.2274349927902222  and  0.8614543676376343\n",
      "total error:  2.6868776082992554\n",
      "training error:  0.5810768604278564  and  1.1131709814071655  and  0.9872635006904602\n",
      "total error:  2.681511342525482\n",
      "training error:  0.5869027972221375  and  1.0679011344909668  and  0.971731424331665\n",
      "total error:  2.6265353560447693\n",
      "training error:  0.5826748013496399  and  1.0569201707839966  and  0.8393104672431946\n",
      "total error:  2.478905439376831\n",
      "training error:  0.581440269947052  and  1.0879391431808472  and  0.9331711530685425\n",
      "total error:  2.6025505661964417\n",
      "training error:  0.5663989186286926  and  1.0671027898788452  and  0.9289566278457642\n",
      "total error:  2.562458336353302\n",
      "training error:  0.598093569278717  and  1.0807033777236938  and  0.8606337904930115\n",
      "total error:  2.5394307374954224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6149425506591797  and  1.222476840019226  and  0.8993515968322754\n",
      "total error:  2.736770987510681\n",
      "training error:  0.5943453907966614  and  1.1529688835144043  and  1.0544289350509644\n",
      "total error:  2.80174320936203\n",
      "training error:  0.605589747428894  and  1.0661239624023438  and  0.8701940774917603\n",
      "total error:  2.541907787322998\n",
      "training error:  0.578535795211792  and  1.0792478322982788  and  0.862877368927002\n",
      "total error:  2.5206609964370728\n",
      "training error:  0.5929731130599976  and  1.1002919673919678  and  0.9969443082809448\n",
      "total error:  2.69020938873291\n",
      "training error:  0.5867084860801697  and  1.1044914722442627  and  0.8831661939620972\n",
      "total error:  2.5743661522865295\n",
      "training error:  0.5832327604293823  and  1.089476466178894  and  0.9030553698539734\n",
      "total error:  2.5757645964622498\n",
      "training error:  0.5964058637619019  and  1.223304271697998  and  0.9159661531448364\n",
      "total error:  2.7356762886047363\n",
      "training error:  0.5807750225067139  and  1.099353313446045  and  0.8899482488632202\n",
      "total error:  2.570076584815979\n",
      "training error:  0.5712865591049194  and  1.1110173463821411  and  0.9117945432662964\n",
      "total error:  2.594098448753357\n",
      "training error:  0.6103096604347229  and  1.1255545616149902  and  0.9327765703201294\n",
      "total error:  2.6686407923698425\n",
      "training error:  0.5801876783370972  and  1.0676506757736206  and  0.8637512922286987\n",
      "total error:  2.5115896463394165\n",
      "training error:  0.579145073890686  and  1.0872950553894043  and  0.9441132545471191\n",
      "total error:  2.6105533838272095\n",
      "training error:  0.5785490274429321  and  1.0943882465362549  and  0.894230842590332\n",
      "total error:  2.567168116569519\n",
      "training error:  0.5641380548477173  and  1.043651819229126  and  0.8652030229568481\n",
      "total error:  2.4729928970336914\n",
      "training error:  0.561744213104248  and  1.0367259979248047  and  0.8999751806259155\n",
      "total error:  2.4984453916549683\n",
      "training error:  0.61473548412323  and  1.1225378513336182  and  0.8875914812088013\n",
      "total error:  2.6248648166656494\n",
      "training error:  0.6388056874275208  and  1.2208081483840942  and  0.9946534633636475\n",
      "total error:  2.8542672991752625\n",
      "training error:  0.5612981915473938  and  1.1134530305862427  and  0.9451134204864502\n",
      "total error:  2.6198646426200867\n",
      "training error:  0.5784748196601868  and  1.0355243682861328  and  0.8741370439529419\n",
      "total error:  2.4881362318992615\n",
      "training error:  0.5716153979301453  and  1.1152153015136719  and  0.9066694378852844\n",
      "total error:  2.5935001373291016\n",
      "training error:  0.5762631893157959  and  1.1223716735839844  and  0.9957058429718018\n",
      "total error:  2.694340705871582\n",
      "training error:  0.5840010643005371  and  1.141685962677002  and  0.9725255966186523\n",
      "total error:  2.6982126235961914\n",
      "training error:  0.57990562915802  and  1.084841012954712  and  1.0372506380081177\n",
      "total error:  2.7019972801208496\n",
      "training error:  0.5854977965354919  and  1.0955320596694946  and  0.9218812584877014\n",
      "total error:  2.602911114692688\n",
      "training error:  0.5882280468940735  and  1.075087547302246  and  0.8770572543144226\n",
      "total error:  2.540372848510742\n",
      "training error:  0.5613646507263184  and  1.0738451480865479  and  0.924761950969696\n",
      "total error:  2.5599717497825623\n",
      "training error:  0.5558252334594727  and  1.068103551864624  and  0.9816423058509827\n",
      "total error:  2.6055710911750793\n",
      "training error:  0.6126583814620972  and  1.1109189987182617  and  0.8962854146957397\n",
      "total error:  2.6198627948760986\n",
      "training error:  0.5492160320281982  and  1.0622670650482178  and  0.8716354370117188\n",
      "total error:  2.4831185340881348\n",
      "training error:  0.5669233798980713  and  1.073730707168579  and  0.9485450983047485\n",
      "total error:  2.589199185371399\n",
      "training error:  0.5694582462310791  and  1.0709712505340576  and  0.9358339905738831\n",
      "total error:  2.5762634873390198\n",
      "training error:  0.5524445176124573  and  1.0677978992462158  and  0.8842199444770813\n",
      "total error:  2.5044623613357544\n",
      "training error:  0.567297101020813  and  1.0347161293029785  and  0.8435976505279541\n",
      "total error:  2.4456108808517456\n",
      "training error:  0.5984538793563843  and  1.1932373046875  and  0.8834595680236816\n",
      "total error:  2.675150752067566\n",
      "training error:  0.5812445878982544  and  1.2316511869430542  and  1.0900691747665405\n",
      "total error:  2.902964949607849\n",
      "training error:  0.5862256288528442  and  1.0648493766784668  and  0.8994981646537781\n",
      "total error:  2.550573170185089\n",
      "training error:  0.5770204067230225  and  1.2099860906600952  and  0.9183436632156372\n",
      "total error:  2.705350160598755\n",
      "training error:  0.5950435996055603  and  1.0779881477355957  and  0.8807452321052551\n",
      "total error:  2.553776979446411\n",
      "training error:  0.5906249284744263  and  1.0886523723602295  and  0.8294634222984314\n",
      "total error:  2.508740723133087\n",
      "training error:  0.5840576887130737  and  1.1247868537902832  and  0.8618941307067871\n",
      "total error:  2.570738673210144\n",
      "training error:  0.5958019495010376  and  1.1280137300491333  and  0.9300197958946228\n",
      "total error:  2.6538354754447937\n",
      "training error:  0.5938123464584351  and  1.0828267335891724  and  0.8925336003303528\n",
      "total error:  2.56917268037796\n",
      "training error:  0.5543978214263916  and  1.0576889514923096  and  0.8727418780326843\n",
      "total error:  2.4848286509513855\n",
      "training error:  0.5717253088951111  and  1.0124930143356323  and  0.8633489608764648\n",
      "total error:  2.4475672841072083\n",
      "training error:  0.5726938843727112  and  1.0637805461883545  and  0.9840415120124817\n",
      "total error:  2.6205159425735474\n",
      "training error:  0.5752201080322266  and  1.128499984741211  and  0.8993182182312012\n",
      "total error:  2.6030383110046387\n",
      "training error:  0.5685778260231018  and  1.170911431312561  and  0.8557227253913879\n",
      "total error:  2.595211982727051\n",
      "training error:  0.5751917362213135  and  1.163902759552002  and  0.8621689081192017\n",
      "total error:  2.601263403892517\n",
      "training error:  0.5934447050094604  and  1.1001226902008057  and  0.9673625826835632\n",
      "total error:  2.6609299778938293\n",
      "training error:  0.5743348002433777  and  1.0377519130706787  and  0.9471208453178406\n",
      "total error:  2.559207558631897\n",
      "training error:  0.568085789680481  and  1.0406134128570557  and  0.8487341403961182\n",
      "total error:  2.457433342933655\n",
      "training error:  0.5554243326187134  and  1.08087158203125  and  0.8858739137649536\n",
      "total error:  2.522169828414917\n",
      "training error:  0.5924420952796936  and  1.081560492515564  and  1.0425151586532593\n",
      "total error:  2.716517746448517\n",
      "training error:  0.5734227299690247  and  1.1238503456115723  and  0.9045497179031372\n",
      "total error:  2.601822793483734\n",
      "training error:  0.5890985727310181  and  1.242945909500122  and  0.8570608496665955\n",
      "total error:  2.6891053318977356\n",
      "training error:  0.5587093830108643  and  1.1117517948150635  and  0.9074768424034119\n",
      "total error:  2.5779380202293396\n",
      "training error:  0.5597729682922363  and  1.1309674978256226  and  0.8670777678489685\n",
      "total error:  2.5578182339668274\n",
      "training error:  0.566902756690979  and  1.0726380348205566  and  0.8857955932617188\n",
      "total error:  2.5253363847732544\n",
      "training error:  0.5848339796066284  and  1.1809170246124268  and  0.869953989982605\n",
      "total error:  2.63570499420166\n",
      "training error:  0.5935013294219971  and  1.1109726428985596  and  0.8993431329727173\n",
      "total error:  2.603817105293274\n",
      "training error:  0.6070183515548706  and  1.128157138824463  and  0.9033100605010986\n",
      "total error:  2.638485550880432\n",
      "training error:  0.5822783708572388  and  1.0531878471374512  and  0.8640520572662354\n",
      "total error:  2.4995182752609253\n",
      "training error:  0.6027531623840332  and  1.1266193389892578  and  0.8584733009338379\n",
      "total error:  2.587845802307129\n",
      "training error:  0.5756947994232178  and  1.0316736698150635  and  0.8749231100082397\n",
      "total error:  2.482291579246521\n",
      "training error:  0.5911476612091064  and  1.086910367012024  and  0.9287154674530029\n",
      "total error:  2.6067734956741333\n",
      "training error:  0.6079686284065247  and  1.0740442276000977  and  0.8439557552337646\n",
      "total error:  2.525968611240387\n",
      "training error:  0.5856025218963623  and  1.2430166006088257  and  0.8802645206451416\n",
      "total error:  2.7088836431503296\n",
      "training error:  0.5825088024139404  and  1.1196792125701904  and  0.8634949922561646\n",
      "total error:  2.5656830072402954\n",
      "training error:  0.6327797770500183  and  1.1960952281951904  and  0.960847020149231\n",
      "total error:  2.7897220253944397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.5691409707069397  and  1.0899503231048584  and  0.9325540065765381\n",
      "total error:  2.591645300388336\n",
      "training error:  0.5640130043029785  and  1.0682308673858643  and  0.9696282148361206\n",
      "total error:  2.6018720865249634\n",
      "training error:  0.5611695051193237  and  1.0765448808670044  and  0.9462345838546753\n",
      "total error:  2.5839489698410034\n",
      "training error:  0.5938529968261719  and  1.1145788431167603  and  0.8866311311721802\n",
      "total error:  2.5950629711151123\n",
      "training error:  0.5410531759262085  and  1.0432841777801514  and  0.8847142457962036\n",
      "total error:  2.4690515995025635\n",
      "training error:  0.5932449102401733  and  1.1334179639816284  and  0.815707802772522\n",
      "total error:  2.5423706769943237\n",
      "training error:  0.5658793449401855  and  1.1663259267807007  and  0.9260292649269104\n",
      "total error:  2.6582345366477966\n",
      "training error:  0.5632228255271912  and  1.096134901046753  and  0.8992069959640503\n",
      "total error:  2.5585647225379944\n",
      "training error:  0.5478334426879883  and  1.0408061742782593  and  0.9596633315086365\n",
      "total error:  2.548302948474884\n",
      "training error:  0.5836375951766968  and  1.092690348625183  and  0.9422516822814941\n",
      "total error:  2.618579626083374\n",
      "training error:  0.5679477453231812  and  1.252063274383545  and  0.9913032650947571\n",
      "total error:  2.811314284801483\n",
      "training error:  0.5456719994544983  and  1.0626227855682373  and  0.84061598777771\n",
      "total error:  2.4489107728004456\n",
      "training error:  0.5658103823661804  and  1.0589869022369385  and  0.8535226583480835\n",
      "total error:  2.4783199429512024\n",
      "training error:  0.5555596351623535  and  1.07185959815979  and  0.9590055346488953\n",
      "total error:  2.586424767971039\n",
      "training error:  0.5986639261245728  and  1.1029441356658936  and  0.8617904186248779\n",
      "total error:  2.5633984804153442\n",
      "training error:  0.6922864317893982  and  1.1776032447814941  and  1.0932097434997559\n",
      "total error:  2.963099420070648\n",
      "training error:  0.5870116949081421  and  1.08162260055542  and  0.8769567012786865\n",
      "total error:  2.5455909967422485\n",
      "training error:  0.5593196153640747  and  1.084942102432251  and  0.8728747367858887\n",
      "total error:  2.5171364545822144\n",
      "training error:  0.5601893663406372  and  1.04404878616333  and  0.8907591104507446\n",
      "total error:  2.494997262954712\n",
      "training error:  0.5700898766517639  and  1.2673569917678833  and  0.8855531811714172\n",
      "total error:  2.7230000495910645\n",
      "training error:  0.5626401305198669  and  1.076600432395935  and  0.8719671964645386\n",
      "total error:  2.5112077593803406\n",
      "training error:  0.5773748159408569  and  1.139146089553833  and  0.8669435977935791\n",
      "total error:  2.583464503288269\n",
      "training error:  0.5529423952102661  and  1.105252742767334  and  1.0010757446289062\n",
      "total error:  2.6592708826065063\n",
      "training error:  0.563357949256897  and  1.0653988122940063  and  0.8619208931922913\n",
      "total error:  2.4906776547431946\n",
      "training error:  0.5525678396224976  and  1.0978485345840454  and  0.9134597778320312\n",
      "total error:  2.563876152038574\n",
      "training error:  0.5570456385612488  and  1.0661113262176514  and  0.8818454146385193\n",
      "total error:  2.5050023794174194\n",
      "training error:  0.6105543375015259  and  1.3047573566436768  and  0.923640251159668\n",
      "total error:  2.8389519453048706\n",
      "training error:  0.5766818523406982  and  1.2906765937805176  and  1.1692957878112793\n",
      "total error:  3.036654233932495\n",
      "training error:  0.5454336404800415  and  1.053030252456665  and  0.8691083192825317\n",
      "total error:  2.4675722122192383\n",
      "training error:  0.5648643970489502  and  1.0525864362716675  and  0.913224995136261\n",
      "total error:  2.5306758284568787\n",
      "training error:  0.573982834815979  and  1.0884275436401367  and  0.9456143379211426\n",
      "total error:  2.6080247163772583\n",
      "training error:  0.5849952697753906  and  1.0811247825622559  and  0.9338659048080444\n",
      "total error:  2.599985957145691\n",
      "training error:  0.619465708732605  and  1.1816855669021606  and  0.9010447263717651\n",
      "total error:  2.7021960020065308\n",
      "training error:  0.5799603462219238  and  1.262683391571045  and  0.8815125226974487\n",
      "total error:  2.7241562604904175\n",
      "training error:  0.5784615278244019  and  1.150273323059082  and  0.9646199941635132\n",
      "total error:  2.693354845046997\n",
      "training error:  0.6116640567779541  and  1.0798580646514893  and  0.9154418110847473\n",
      "total error:  2.6069639325141907\n",
      "training error:  0.5806772112846375  and  1.0812257528305054  and  0.8228268027305603\n",
      "total error:  2.484729766845703\n",
      "training error:  0.5824223756790161  and  1.1131250858306885  and  0.9360643625259399\n",
      "total error:  2.6316118240356445\n",
      "training error:  0.5465914011001587  and  1.093172311782837  and  0.884551465511322\n",
      "total error:  2.5243151783943176\n",
      "training error:  0.6126567125320435  and  1.3258323669433594  and  0.864017128944397\n",
      "total error:  2.8025062084198\n",
      "training error:  0.561208963394165  and  1.0715677738189697  and  0.89546799659729\n",
      "total error:  2.528244733810425\n",
      "training error:  0.5631906986236572  and  1.0638355016708374  and  0.9062751531600952\n",
      "total error:  2.53330135345459\n",
      "training error:  0.612492561340332  and  1.1783967018127441  and  0.8998897075653076\n",
      "total error:  2.690778970718384\n",
      "training error:  0.6954478621482849  and  1.226364016532898  and  1.0371155738830566\n",
      "total error:  2.9589274525642395\n",
      "training error:  0.5780086517333984  and  1.0438849925994873  and  0.8925928473472595\n",
      "total error:  2.5144864916801453\n",
      "training error:  0.57973313331604  and  1.0864784717559814  and  0.9357898235321045\n",
      "total error:  2.602001428604126\n",
      "training error:  0.5492465496063232  and  1.0608593225479126  and  0.9901163578033447\n",
      "total error:  2.6002222299575806\n",
      "training error:  0.5793840885162354  and  1.0656249523162842  and  0.8562116622924805\n",
      "total error:  2.501220703125\n",
      "training error:  0.5670998692512512  and  1.0644307136535645  and  0.8596442341804504\n",
      "total error:  2.491174817085266\n",
      "training error:  0.5613014101982117  and  1.0524158477783203  and  0.8894940614700317\n",
      "total error:  2.5032113194465637\n",
      "training error:  0.5594107508659363  and  1.126205563545227  and  0.9443285465240479\n",
      "total error:  2.629944860935211\n",
      "training error:  0.5436055660247803  and  1.2287912368774414  and  0.9689664840698242\n",
      "total error:  2.741363286972046\n",
      "training error:  0.5597835779190063  and  1.1024696826934814  and  0.9787575006484985\n",
      "total error:  2.6410107612609863\n",
      "training error:  0.5625214576721191  and  1.050546646118164  and  0.8589518070220947\n",
      "total error:  2.472019910812378\n",
      "training error:  0.5588375329971313  and  1.0926634073257446  and  0.9047021865844727\n",
      "total error:  2.5562031269073486\n",
      "training error:  0.5651169419288635  and  1.0890369415283203  and  0.8685885071754456\n",
      "total error:  2.5227423906326294\n",
      "training error:  0.5517033338546753  and  1.0472190380096436  and  0.8468799591064453\n",
      "total error:  2.445802330970764\n",
      "training error:  0.6016117930412292  and  1.1447420120239258  and  0.9722005724906921\n",
      "total error:  2.718554377555847\n",
      "training error:  0.6198250651359558  and  1.1752235889434814  and  0.92107093334198\n",
      "total error:  2.7161195874214172\n",
      "training error:  0.5581110119819641  and  1.0388492345809937  and  0.8548144102096558\n",
      "total error:  2.4517746567726135\n",
      "training error:  0.5657828450202942  and  1.0849872827529907  and  0.9475623369216919\n",
      "total error:  2.598332464694977\n",
      "training error:  0.5559557676315308  and  1.0519342422485352  and  0.817944347858429\n",
      "total error:  2.425834357738495\n",
      "training error:  0.5435268878936768  and  1.0022658109664917  and  0.8509958982467651\n",
      "total error:  2.3967885971069336\n",
      "training error:  0.5513259172439575  and  1.086674451828003  and  0.8506950736045837\n",
      "total error:  2.488695442676544\n",
      "training error:  0.5525701642036438  and  1.0960570573806763  and  0.9340338706970215\n",
      "total error:  2.5826610922813416\n",
      "training error:  0.6103191375732422  and  1.0902732610702515  and  0.8843680620193481\n",
      "total error:  2.584960460662842\n",
      "training error:  0.5629777312278748  and  1.095526933670044  and  0.8962079882621765\n",
      "total error:  2.554712653160095\n",
      "training error:  0.5619194507598877  and  1.1264252662658691  and  1.0116758346557617\n",
      "total error:  2.7000205516815186\n",
      "training error:  0.5904691219329834  and  1.0962764024734497  and  0.8644921183586121\n",
      "total error:  2.551237642765045\n",
      "training error:  0.6016490459442139  and  1.1107841730117798  and  0.9326884746551514\n",
      "total error:  2.645121693611145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.5747042894363403  and  1.08788001537323  and  0.8941582441329956\n",
      "total error:  2.556742548942566\n",
      "training error:  0.5724764466285706  and  1.0812737941741943  and  0.8877294659614563\n",
      "total error:  2.541479706764221\n",
      "training error:  0.5449273586273193  and  1.092639446258545  and  0.8873257040977478\n",
      "total error:  2.524892508983612\n",
      "training error:  0.5314903259277344  and  1.0682547092437744  and  0.8884127140045166\n",
      "total error:  2.4881577491760254\n",
      "training error:  0.5776450634002686  and  1.1643296480178833  and  0.8928090333938599\n",
      "total error:  2.6347837448120117\n",
      "training error:  0.5427354574203491  and  1.080774188041687  and  0.8571670055389404\n",
      "total error:  2.4806766510009766\n",
      "training error:  0.5484751462936401  and  1.104699730873108  and  0.8764665126800537\n",
      "total error:  2.5296413898468018\n",
      "training error:  0.598063588142395  and  1.1469672918319702  and  0.9141020178794861\n",
      "total error:  2.6591328978538513\n",
      "training error:  0.6297982335090637  and  1.0761170387268066  and  0.887382984161377\n",
      "total error:  2.5932982563972473\n",
      "training error:  0.5781760811805725  and  1.0450186729431152  and  0.828941285610199\n",
      "total error:  2.4521360397338867\n",
      "training error:  0.5704970359802246  and  1.0542998313903809  and  0.8554893732070923\n",
      "total error:  2.4802862405776978\n",
      "training error:  0.5505892038345337  and  1.0725655555725098  and  0.9787602424621582\n",
      "total error:  2.6019150018692017\n",
      "training error:  0.5701044797897339  and  1.0761113166809082  and  0.8861891031265259\n",
      "total error:  2.532404899597168\n",
      "training error:  0.5595753192901611  and  1.0357823371887207  and  0.8780336976051331\n",
      "total error:  2.473391354084015\n",
      "training error:  0.5636067390441895  and  1.0341205596923828  and  0.984329342842102\n",
      "total error:  2.5820566415786743\n",
      "training error:  0.5517898797988892  and  1.0387792587280273  and  0.8967304825782776\n",
      "total error:  2.487299621105194\n",
      "training error:  0.5720770955085754  and  1.2965881824493408  and  0.846644937992096\n",
      "total error:  2.715310215950012\n",
      "training error:  0.5830927491188049  and  1.0706093311309814  and  0.9471063613891602\n",
      "total error:  2.6008084416389465\n",
      "training error:  0.5977940559387207  and  1.2656757831573486  and  0.9986001253128052\n",
      "total error:  2.8620699644088745\n",
      "training error:  0.5437290072441101  and  1.0269279479980469  and  0.8767982721328735\n",
      "total error:  2.4474552273750305\n",
      "training error:  0.5753677487373352  and  1.0858831405639648  and  0.954746663570404\n",
      "total error:  2.615997552871704\n",
      "training error:  0.5641070008277893  and  1.1003185510635376  and  0.9389767646789551\n",
      "total error:  2.603402316570282\n",
      "training error:  0.564659833908081  and  1.0431222915649414  and  0.9209529161453247\n",
      "total error:  2.528735041618347\n",
      "training error:  0.5636262893676758  and  1.071482539176941  and  0.8745164275169373\n",
      "total error:  2.509625256061554\n",
      "training error:  0.5822783708572388  and  1.0990160703659058  and  0.9160109758377075\n",
      "total error:  2.597305417060852\n",
      "training error:  0.5826717615127563  and  1.1840897798538208  and  0.883531391620636\n",
      "total error:  2.650292932987213\n",
      "training error:  0.5663943886756897  and  1.1031540632247925  and  0.9065731763839722\n",
      "total error:  2.5761216282844543\n",
      "training error:  0.5906140804290771  and  1.1201610565185547  and  1.1055655479431152\n",
      "total error:  2.816340684890747\n",
      "training error:  0.5592354536056519  and  1.0745030641555786  and  0.8289057016372681\n",
      "total error:  2.4626442193984985\n",
      "training error:  0.6028632521629333  and  1.0784459114074707  and  0.914046049118042\n",
      "total error:  2.595355212688446\n",
      "training error:  0.5315929055213928  and  1.0516433715820312  and  0.8666588068008423\n",
      "total error:  2.4498950839042664\n",
      "training error:  0.5369925498962402  and  1.052354097366333  and  0.8270975947380066\n",
      "total error:  2.41644424200058\n",
      "training error:  0.5470046401023865  and  1.0372302532196045  and  0.8917399644851685\n",
      "total error:  2.4759748578071594\n",
      "training error:  0.6075052618980408  and  1.1002920866012573  and  0.8562794327735901\n",
      "total error:  2.564076781272888\n",
      "training error:  0.5788343548774719  and  1.1832194328308105  and  0.9253502488136292\n",
      "total error:  2.6874040365219116\n",
      "training error:  0.6833078861236572  and  1.134626030921936  and  0.9019421339035034\n",
      "total error:  2.7198760509490967\n",
      "training error:  0.5534083843231201  and  1.1224024295806885  and  0.883159339427948\n",
      "total error:  2.5589701533317566\n",
      "training error:  0.5753028988838196  and  1.0965174436569214  and  1.0526235103607178\n",
      "total error:  2.7244438529014587\n",
      "training error:  0.5646138191223145  and  1.1026853322982788  and  0.8496012687683105\n",
      "total error:  2.516900420188904\n",
      "training error:  0.5521114468574524  and  1.0331836938858032  and  0.8576298952102661\n",
      "total error:  2.4429250359535217\n",
      "training error:  0.5688502192497253  and  1.1310300827026367  and  0.8888410925865173\n",
      "total error:  2.5887213945388794\n",
      "training error:  0.5590835809707642  and  1.0378897190093994  and  0.9288130402565002\n",
      "total error:  2.525786340236664\n",
      "training error:  0.545931875705719  and  1.0275511741638184  and  0.8893424868583679\n",
      "total error:  2.4628255367279053\n",
      "training error:  0.5469475388526917  and  1.0893142223358154  and  0.9377996921539307\n",
      "total error:  2.5740614533424377\n",
      "training error:  0.5358310341835022  and  1.0648629665374756  and  0.8486185073852539\n",
      "total error:  2.4493125081062317\n",
      "training error:  0.5803324580192566  and  1.0952415466308594  and  0.856743574142456\n",
      "total error:  2.532317578792572\n",
      "training error:  0.5482403039932251  and  1.0937905311584473  and  0.8182217478752136\n",
      "total error:  2.460252583026886\n",
      "training error:  0.5846927165985107  and  1.1019258499145508  and  0.9415971636772156\n",
      "total error:  2.628215730190277\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys,os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy import linalg\n",
    "from numpy import dot\n",
    "import geomloss as gs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import grad\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.modules import Linear\n",
    "from torch.autograd.functional import jacobian,hessian,vjp,vhp,hvp\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "FilePath = '../../'\n",
    "\n",
    "file_list = ['GSM1599494_ES_d0_main.csv', 'GSM1599497_ES_d2_LIFminus.csv', 'GSM1599498_ES_d4_LIFminus.csv', 'GSM1599499_ES_d7_LIFminus.csv']\n",
    "\n",
    "table_list = []\n",
    "for filein in file_list:\n",
    "    table_list.append(pd.read_csv(FilePath+filein, header=None))\n",
    "\n",
    "matrix_list = []\n",
    "gene_names = table_list[0].values[:,0]\n",
    "for table in table_list:\n",
    "    matrix_list.append(table.values[:,1:].astype('float32'))\n",
    "\n",
    "cell_counts = [matrix.shape[1] for matrix in matrix_list]\n",
    "\n",
    "def normalize_run(mat):\n",
    "    rpm = np.sum(mat,0)/1e6\n",
    "    detect_pr = np.sum(mat==0,0)/float(mat.shape[0])\n",
    "    return np.log(mat*(np.median(detect_pr)/detect_pr)*1.0/rpm + 1.0)\n",
    "\n",
    "norm_mat = [normalize_run(matrix) for matrix in matrix_list]\n",
    "\n",
    "qt_mat = [np.percentile(norm_in,q=np.linspace(0,100,50),axis=1) for norm_in in norm_mat] \n",
    "wdiv=np.sum((qt_mat[0]-qt_mat[3])**2,0)\n",
    "w_order = np.argsort(-wdiv)\n",
    "\n",
    "wsub = w_order[0:100]\n",
    "\n",
    "\n",
    "def nmf(X, latent_features, max_iter=100, error_limit=1e-6, fit_error_limit=1e-6, print_iter=200):\n",
    "    \"\"\"\n",
    "    Decompose X to A*Y\n",
    "    \"\"\"\n",
    "    eps = 1e-5\n",
    "    print('Starting NMF decomposition with {} latent features and {} iterations.'.format(latent_features, max_iter))\n",
    "    #X = X.toarray()   I am passing in a scipy sparse matrix\n",
    "\n",
    "    # mask\n",
    "    mask = np.sign(X)\n",
    "\n",
    "    # initial matrices. A is random [0,1] and Y is A\\X.\n",
    "    rows, columns = X.shape\n",
    "    A = np.random.rand(rows, latent_features)\n",
    "    A = np.maximum(A, eps)\n",
    "\n",
    "    Y = linalg.lstsq(A, X)[0]\n",
    "    Y = np.maximum(Y, eps)\n",
    "\n",
    "    masked_X = mask * X\n",
    "    X_est_prev = dot(A, Y)\n",
    "    for i in range(1, max_iter + 1):\n",
    "        # ===== updates =====\n",
    "        # Matlab: A=A.*(((W.*X)*Y')./((W.*(A*Y))*Y'));\n",
    "        top = dot(masked_X, Y.T)\n",
    "        bottom = (dot((mask * dot(A, Y)), Y.T)) + eps\n",
    "        A *= top / bottom\n",
    "\n",
    "        A = np.maximum(A, eps)\n",
    "        # print 'A',  np.round(A, 2)\n",
    "\n",
    "        # Matlab: Y=Y.*((A'*(W.*X))./(A'*(W.*(A*Y))));\n",
    "        top = dot(A.T, masked_X)\n",
    "        bottom = dot(A.T, mask * dot(A, Y)) + eps\n",
    "        Y *= top / bottom\n",
    "        Y = np.maximum(Y, eps)\n",
    "        # print 'Y', np.round(Y, 2)\n",
    "\n",
    "\n",
    "        # ==== evaluation ====\n",
    "        if i % print_iter == 0 or i == 1 or i == max_iter:\n",
    "            print('Iteration {}:'.format(i),)\n",
    "            X_est = dot(A, Y)\n",
    "            err = mask * (X_est_prev - X_est)\n",
    "            fit_residual = np.sqrt(np.sum(err ** 2))\n",
    "            X_est_prev = X_est\n",
    "\n",
    "            curRes = linalg.norm(mask * (X - X_est), ord='fro')\n",
    "            print('fit residual', np.round(fit_residual, 4),)\n",
    "            print('total residual', np.round(curRes, 4))\n",
    "            if curRes < error_limit or fit_residual < fit_error_limit:\n",
    "                break\n",
    "    return A, Y, dot(A,Y)\n",
    "\n",
    "np.random.seed(0)\n",
    "norm_imputed = [nmf(normin[wsub,:], latent_features = len(wsub)*4, max_iter=500)[2] for normin in norm_mat]\n",
    "\n",
    "norm_adj = np.mean(norm_imputed[3],1)[:,np.newaxis]\n",
    "subvec = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "gnvec = gene_names[w_order[subvec]]\n",
    "\n",
    "cov_mat = np.cov(norm_imputed[3][subvec,:])\n",
    "whiten = np.diag(np.diag(cov_mat)**(-0.5))\n",
    "unwhiten = np.diag(np.diag(cov_mat)**(0.5))\n",
    "\n",
    "norm_imputed2 = [np.dot(whiten,(normin - norm_adj)[subvec,:]) for normin in norm_imputed]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, dim_hidden=64, num_hidden=0, activation=nn.LeakyReLU()):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        if num_hidden == 0:\n",
    "            self.linears = nn.ModuleList([nn.Linear(dim_in, dim_out)])\n",
    "        elif num_hidden >= 1:\n",
    "            self.linears = nn.ModuleList() \n",
    "            self.linears.append(nn.Linear(dim_in, dim_hidden))\n",
    "            self.linears.extend([nn.Linear(dim_hidden, dim_hidden) for _ in range(num_hidden-1)])\n",
    "            self.linears.append(nn.Linear(dim_hidden, dim_out))\n",
    "        else:\n",
    "            raise Exception('number of hidden layers must be positive')\n",
    "\n",
    "        for m in self.linears:\n",
    "            #nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.uniform_(m.bias,a=-0.1,b=0.1)\n",
    "            #nn.init.constant_(m.bias,0) \n",
    " \n",
    "        self.activation = activation \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for m in self.linears[:-1]:\n",
    "            x = self.activation(m(x))\n",
    "            #x = F.dropout(x,p=0.5)\n",
    "\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "\n",
    "def compute_gradient_penalty(D, real_sample, fake_sample,k,p):\n",
    "    real_samples = real_sample.requires_grad_(True)\n",
    "    fake_samples = fake_sample.requires_grad_(True)\n",
    "\n",
    "    real_validity = D(real_samples)\n",
    "    fake_validity = D(fake_samples)\n",
    "\n",
    "    real_grad_out = torch.ones((real_samples.shape[0],1),dtype=torch.float32,requires_grad=False,device=\"cuda\")\n",
    "    real_grad = grad(\n",
    "        real_validity, real_samples, real_grad_out, create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    real_grad_norm = real_grad.view(real_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "\n",
    "    fake_grad_out = torch.ones((fake_samples.shape[0],1),dtype=torch.float32,requires_grad=False,device=\"cuda\")\n",
    "    fake_grad = grad(\n",
    "        fake_validity, fake_samples, fake_grad_out, create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    fake_grad_norm = fake_grad.view(fake_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "\n",
    "    return (torch.sum(real_grad_norm) + torch.sum(fake_grad_norm)) * k / (real_sample.shape[0]+fake_sample.shape[0])\n",
    "\n",
    "class JumpEulerForwardCuda(nn.Module):\n",
    "    def __init__(self,in_features,num_hidden,dim_hidden,step_size):\n",
    "        super(JumpEulerForwardCuda,self).__init__()\n",
    "\n",
    "        self.drift = MLP(in_features,in_features,dim_hidden,num_hidden)\n",
    "        self.intensity = torch.tensor(intensity,device=\"cuda\")\n",
    "        self.mean = nn.Parameter(0.01*torch.ones(in_features))\n",
    "        self.covHalf = nn.Parameter(0.08*torch.eye(in_features))\n",
    "        self.diffusion = nn.Parameter(torch.ones(bd,10))\n",
    "        self.in_features = in_features\n",
    "        self.jump = MLP(in_features,in_features,dim_hidden,num_hidden)\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def forward(self,z0,Nsim,steps):\n",
    "\n",
    "        PopulationPath = torch.empty(size = (Nsim,steps+1,self.in_features),device=\"cuda\")\n",
    "        PopulationPath[:,0,:] = z0\n",
    "        state = z0\n",
    "\n",
    "        for i in range(1,steps+1):\n",
    "            DP = D.poisson.Poisson(self.intensity*self.step_size)\n",
    "            pois = DP.sample((Nsim,1)).cuda()\n",
    "            state = state + self.drift(state)*self.step_size + math.sqrt(self.step_size)*torch.normal(0,1,size=(Nsim,bd),device=\"cuda\")@self.diffusion+\\\n",
    "                (pois*self.mean + pois**(0.5)*torch.normal(0,1,size=(Nsim,self.in_features),device=\"cuda\")@self.covHalf)*self.jump(state)\n",
    "            PopulationPath[:,i,:] = state\n",
    "        return PopulationPath\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "sed = 200\n",
    "setup_seed(sed)\n",
    "\n",
    "a=gs.SamplesLoss(loss='sinkhorn',p=2,blur=0.01)\n",
    "\n",
    "\n",
    "train_data = norm_imputed2\n",
    "\n",
    "train0 = torch.tensor(train_data[0],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "train2 = torch.tensor(train_data[1],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "train4 = torch.tensor(train_data[2],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "train7 = torch.tensor(train_data[3],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "\n",
    "train0 = train0+0.01*torch.normal(0,1,size=train0.shape,device=\"cuda\")\n",
    "train2 = train2+0.01*torch.normal(0,1,size=train2.shape,device=\"cuda\")\n",
    "train4 = train4+0.01*torch.normal(0,1,size=train4.shape,device=\"cuda\")\n",
    "train7 = train7+0.01*torch.normal(0,1,size=train7.shape,device=\"cuda\")\n",
    "\n",
    "\n",
    "intensity = 10\n",
    "lr = 0.0003\n",
    "step_size = 0.03\n",
    "kuan = 256\n",
    "ceng = 4\n",
    "bd = 2\n",
    "n_critic = 3\n",
    "k = 2\n",
    "p = 6\n",
    "\n",
    "n_sims = train0.shape[0]\n",
    "in_features = train0.shape[1]\n",
    "n_steps = [10,20,35]\n",
    "\n",
    "\n",
    "netG = JumpEulerForwardCuda(10,ceng,kuan,step_size).cuda()\n",
    "netD1 = MLP(10,1,dim_hidden=kuan,num_hidden=ceng).cuda()\n",
    "netD2 = MLP(10,1,dim_hidden=kuan,num_hidden=ceng).cuda()\n",
    "netD3 = MLP(10,1,dim_hidden=kuan,num_hidden=ceng).cuda()\n",
    "\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerSD1 = optim.Adam(netD1.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerSD2 = optim.Adam(netD2.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerSD3 = optim.Adam(netD3.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "n_epochs =  20000\n",
    "\n",
    "wd = []\n",
    "for epoch in range(n_epochs):\n",
    "  \n",
    "\n",
    "    for _ in range(n_critic):\n",
    "        fake_data = netG(train0,n_sims,n_steps[2])\n",
    "        fake1 = fake_data[:,n_steps[0],:]\n",
    "        fake2 = fake_data[:,n_steps[1],:]\n",
    "        fake3 = fake_data[:,n_steps[2],:]\n",
    "\n",
    "        optimizerSD1.zero_grad()\n",
    "\n",
    "        div_gp1 = compute_gradient_penalty(netD1,train2,fake1,k,p)\n",
    "        d1_loss = -torch.mean(netD1(train2))+torch.mean(netD1(fake1))+div_gp1\n",
    "        d1_loss.backward(retain_graph=True) # retain_graph=True\n",
    "\n",
    "        optimizerSD1.step()\n",
    "\n",
    "\n",
    "        optimizerSD2.zero_grad()\n",
    "        \n",
    "        div_gp2 = compute_gradient_penalty(netD2,train4,fake2,k,p)\n",
    "        d2_loss = -torch.mean(netD2(train4))+torch.mean(netD2(fake2))+div_gp2\n",
    "        d2_loss.backward(retain_graph=True)\n",
    "\n",
    "        optimizerSD2.step()\n",
    "        \n",
    "        \n",
    "        optimizerSD3.zero_grad()\n",
    "        \n",
    "        div_gp3 = compute_gradient_penalty(netD3,train7,fake3,k,p)\n",
    "        d3_loss = -torch.mean(netD3(train7))+torch.mean(netD3(fake3))+div_gp3\n",
    "        d3_loss.backward(retain_graph=True)\n",
    "\n",
    "        optimizerSD3.step()\n",
    "        \n",
    "    \n",
    "    for _ in range(1):\n",
    "        optimizerG.zero_grad()\n",
    "        fake_data = netG(train0,n_sims,n_steps[2])\n",
    "        fake1 = fake_data[:,n_steps[0],:]\n",
    "        fake2 = fake_data[:,n_steps[1],:]\n",
    "        fake3 = fake_data[:,n_steps[2],:]\n",
    "        g_loss = -torch.mean(netD1(fake1))-torch.mean(netD2(fake2))-torch.mean(netD3(fake3))\n",
    "        g_loss.backward() \n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "    if epoch %10==0:\n",
    "        x1 = a(fake_data[:,n_steps[0],:],train2).item()\n",
    "        x2 = a(fake_data[:,n_steps[1],:],train4).item()\n",
    "        x3 = a(fake_data[:,n_steps[2],:],train7).item()\n",
    "        \n",
    "        wd.append(x1+x2+x3)\n",
    "        \n",
    "        print(\"training error: \",x1,\" and \",x2,\" and \",x3)\n",
    "        print(\"total error: \",wd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(),\"./epsilon0.01.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
