{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 4000.1233\n",
      "total residual 138.7959\n",
      "Iteration 200:\n",
      "fit residual 123.6643\n",
      "total residual 31.8464\n",
      "Iteration 400:\n",
      "fit residual 21.7894\n",
      "total residual 12.6115\n",
      "Iteration 500:\n",
      "fit residual 4.3442\n",
      "total residual 8.6686\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 2257.8358\n",
      "total residual 58.2265\n",
      "Iteration 200:\n",
      "fit residual 56.8198\n",
      "total residual 4.0447\n",
      "Iteration 400:\n",
      "fit residual 3.4777\n",
      "total residual 0.6841\n",
      "Iteration 500:\n",
      "fit residual 0.3744\n",
      "total residual 0.3185\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 2925.3664\n",
      "total residual 80.2657\n",
      "Iteration 200:\n",
      "fit residual 78.1148\n",
      "total residual 6.1559\n",
      "Iteration 400:\n",
      "fit residual 5.1661\n",
      "total residual 1.1917\n",
      "Iteration 500:\n",
      "fit residual 0.6183\n",
      "total residual 0.5917\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 3719.0401\n",
      "total residual 121.8387\n",
      "Iteration 200:\n",
      "fit residual 113.1429\n",
      "total residual 20.967\n",
      "Iteration 400:\n",
      "fit residual 15.2785\n",
      "total residual 7.12\n",
      "Iteration 500:\n",
      "fit residual 2.7305\n",
      "total residual 4.5793\n",
      "training error:  8.595394134521484  and  27.793006896972656  and  84.16348266601562\n",
      "total error:  120.55188369750977\n",
      "training error:  5.662008285522461  and  11.916024208068848  and  27.05083465576172\n",
      "total error:  44.62886714935303\n",
      "training error:  5.949681282043457  and  7.398290634155273  and  10.182361602783203\n",
      "total error:  23.530333518981934\n",
      "training error:  5.104194164276123  and  5.230372428894043  and  5.894243240356445\n",
      "total error:  16.22880983352661\n",
      "training error:  4.124109745025635  and  3.894552230834961  and  4.427698135375977\n",
      "total error:  12.446360111236572\n",
      "training error:  3.7329723834991455  and  3.5125057697296143  and  3.2471699714660645\n",
      "total error:  10.492648124694824\n",
      "training error:  3.441666603088379  and  3.565979242324829  and  2.685667037963867\n",
      "total error:  9.693312883377075\n",
      "training error:  3.4351794719696045  and  3.182863235473633  and  2.7226691246032715\n",
      "total error:  9.340711832046509\n",
      "training error:  3.2781224250793457  and  3.0318875312805176  and  2.363844394683838\n",
      "total error:  8.673854351043701\n",
      "training error:  2.9890542030334473  and  3.752089262008667  and  2.227710485458374\n",
      "total error:  8.968853950500488\n",
      "training error:  2.934946060180664  and  3.378242015838623  and  2.8634650707244873\n",
      "total error:  9.176653146743774\n",
      "training error:  3.1831603050231934  and  4.185519218444824  and  2.5310893058776855\n",
      "total error:  9.899768829345703\n",
      "training error:  2.7508492469787598  and  3.694838762283325  and  4.112636089324951\n",
      "total error:  10.558324098587036\n",
      "training error:  2.597027063369751  and  2.9377970695495605  and  2.7633981704711914\n",
      "total error:  8.298222303390503\n",
      "training error:  2.585693597793579  and  3.66715931892395  and  3.6192433834075928\n",
      "total error:  9.872096300125122\n",
      "training error:  2.5957541465759277  and  3.1368603706359863  and  3.0497820377349854\n",
      "total error:  8.7823965549469\n",
      "training error:  2.4271886348724365  and  2.7739388942718506  and  2.1720967292785645\n",
      "total error:  7.373224258422852\n",
      "training error:  2.5249080657958984  and  2.9008922576904297  and  2.144918441772461\n",
      "total error:  7.570718765258789\n",
      "training error:  2.531344413757324  and  3.145247220993042  and  1.9340299367904663\n",
      "total error:  7.6106215715408325\n",
      "training error:  2.559506416320801  and  3.445756435394287  and  1.906388282775879\n",
      "total error:  7.911651134490967\n",
      "training error:  2.3996925354003906  and  3.194481372833252  and  1.961782455444336\n",
      "total error:  7.5559563636779785\n",
      "training error:  2.365299701690674  and  3.257528305053711  and  1.858513355255127\n",
      "total error:  7.481341361999512\n",
      "training error:  2.388145685195923  and  3.0398428440093994  and  1.9841442108154297\n",
      "total error:  7.412132740020752\n",
      "training error:  2.317798137664795  and  2.9770708084106445  and  1.606972336769104\n",
      "total error:  6.9018412828445435\n",
      "training error:  2.1135449409484863  and  2.383024215698242  and  1.7815343141555786\n",
      "total error:  6.278103470802307\n",
      "training error:  2.184523344039917  and  2.2938666343688965  and  1.8776649236679077\n",
      "total error:  6.356054902076721\n",
      "training error:  2.08378005027771  and  2.483645439147949  and  2.1210756301879883\n",
      "total error:  6.6885011196136475\n",
      "training error:  2.1563127040863037  and  2.5029993057250977  and  2.0041003227233887\n",
      "total error:  6.66341233253479\n",
      "training error:  2.0711371898651123  and  2.545067548751831  and  2.7481331825256348\n",
      "total error:  7.364337921142578\n",
      "training error:  2.217644214630127  and  3.0509698390960693  and  2.0777580738067627\n",
      "total error:  7.346372127532959\n",
      "training error:  2.1635894775390625  and  2.7213616371154785  and  1.8169100284576416\n",
      "total error:  6.701861143112183\n",
      "training error:  2.062286853790283  and  2.2951157093048096  and  1.9737604856491089\n",
      "total error:  6.331163048744202\n",
      "training error:  2.3518617153167725  and  5.070936679840088  and  1.9726951122283936\n",
      "total error:  9.395493507385254\n",
      "training error:  2.0501134395599365  and  2.5929465293884277  and  1.8768829107284546\n",
      "total error:  6.519942879676819\n",
      "training error:  2.0120887756347656  and  3.388521432876587  and  2.52392315864563\n",
      "total error:  7.924533367156982\n",
      "training error:  2.0813984870910645  and  2.861341953277588  and  2.1558620929718018\n",
      "total error:  7.098602533340454\n",
      "training error:  2.0319643020629883  and  3.0915184020996094  and  1.8270556926727295\n",
      "total error:  6.950538396835327\n",
      "training error:  2.0186171531677246  and  2.8616833686828613  and  2.7303340435028076\n",
      "total error:  7.6106345653533936\n",
      "training error:  2.035761833190918  and  2.561185836791992  and  1.6343330144882202\n",
      "total error:  6.23128068447113\n",
      "training error:  2.1315460205078125  and  3.493248462677002  and  1.8440395593643188\n",
      "total error:  7.468834042549133\n",
      "training error:  1.9475892782211304  and  2.3800435066223145  and  1.921200156211853\n",
      "total error:  6.248832941055298\n",
      "training error:  1.853402018547058  and  2.429764986038208  and  2.2155091762542725\n",
      "total error:  6.498676180839539\n",
      "training error:  1.8854906558990479  and  3.189314603805542  and  2.360919237136841\n",
      "total error:  7.435724496841431\n",
      "training error:  2.0083112716674805  and  4.424587726593018  and  2.0965678691864014\n",
      "total error:  8.5294668674469\n",
      "training error:  1.8595001697540283  and  2.95631742477417  and  2.120023727416992\n",
      "total error:  6.93584132194519\n",
      "training error:  1.8270232677459717  and  2.2881174087524414  and  1.5777833461761475\n",
      "total error:  5.6929240226745605\n",
      "training error:  2.0263915061950684  and  2.8677656650543213  and  1.8779561519622803\n",
      "total error:  6.77211332321167\n",
      "training error:  1.7751688957214355  and  2.5948128700256348  and  2.0201220512390137\n",
      "total error:  6.390103816986084\n",
      "training error:  1.941454529762268  and  3.2530574798583984  and  1.7461636066436768\n",
      "total error:  6.940675616264343\n",
      "training error:  1.8338937759399414  and  2.3273210525512695  and  2.162102699279785\n",
      "total error:  6.323317527770996\n",
      "training error:  1.7924351692199707  and  2.5082759857177734  and  1.9668391942977905\n",
      "total error:  6.267550349235535\n",
      "training error:  1.842931866645813  and  3.462881088256836  and  1.7150487899780273\n",
      "total error:  7.020861744880676\n",
      "training error:  1.8042882680892944  and  2.3802976608276367  and  1.8254834413528442\n",
      "total error:  6.010069370269775\n",
      "training error:  1.8013434410095215  and  2.872027635574341  and  2.028913974761963\n",
      "total error:  6.702285051345825\n",
      "training error:  1.9303463697433472  and  3.648160457611084  and  1.651958703994751\n",
      "total error:  7.230465531349182\n",
      "training error:  1.704826831817627  and  2.499861717224121  and  2.0692644119262695\n",
      "total error:  6.273952960968018\n",
      "training error:  1.7855154275894165  and  2.6657543182373047  and  1.3992011547088623\n",
      "total error:  5.8504709005355835\n",
      "training error:  1.7862095832824707  and  2.154186964035034  and  1.5934561491012573\n",
      "total error:  5.533852696418762\n",
      "training error:  1.6981631517410278  and  2.2488372325897217  and  1.8827760219573975\n",
      "total error:  5.829776406288147\n",
      "training error:  1.7186858654022217  and  2.382847309112549  and  1.5346739292144775\n",
      "total error:  5.636207103729248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.7011101245880127  and  2.2476353645324707  and  1.7428810596466064\n",
      "total error:  5.69162654876709\n",
      "training error:  1.721401333808899  and  2.3279826641082764  and  1.611617088317871\n",
      "total error:  5.661001086235046\n",
      "training error:  1.703049659729004  and  2.387953758239746  and  1.9327341318130493\n",
      "total error:  6.023737549781799\n",
      "training error:  1.6429194211959839  and  2.144483804702759  and  1.5250988006591797\n",
      "total error:  5.312502026557922\n",
      "training error:  1.6946673393249512  and  2.2536120414733887  and  1.5457133054733276\n",
      "total error:  5.4939926862716675\n",
      "training error:  1.6849839687347412  and  2.7425713539123535  and  2.1006228923797607\n",
      "total error:  6.5281782150268555\n",
      "training error:  1.845886468887329  and  3.0585598945617676  and  1.5953292846679688\n",
      "total error:  6.499775648117065\n",
      "training error:  1.7131321430206299  and  3.988844633102417  and  2.4839773178100586\n",
      "total error:  8.185954093933105\n",
      "training error:  1.8092446327209473  and  3.6468639373779297  and  1.786781907081604\n",
      "total error:  7.242890477180481\n",
      "training error:  1.712144136428833  and  2.5554075241088867  and  2.312765121459961\n",
      "total error:  6.580316781997681\n",
      "training error:  1.6482484340667725  and  2.36051607131958  and  1.8328441381454468\n",
      "total error:  5.841608643531799\n",
      "training error:  1.8174487352371216  and  3.425419569015503  and  1.5892176628112793\n",
      "total error:  6.832085967063904\n",
      "training error:  1.6359091997146606  and  2.357250213623047  and  1.8245106935501099\n",
      "total error:  5.817670106887817\n",
      "training error:  1.588881254196167  and  2.161940574645996  and  1.4899448156356812\n",
      "total error:  5.240766644477844\n",
      "training error:  1.7096123695373535  and  4.0407395362854  and  1.783275842666626\n",
      "total error:  7.53362774848938\n",
      "training error:  1.6084672212600708  and  3.149362802505493  and  2.0704517364501953\n",
      "total error:  6.828281760215759\n",
      "training error:  1.6184215545654297  and  2.2131237983703613  and  1.723829984664917\n",
      "total error:  5.555375337600708\n",
      "training error:  1.7173564434051514  and  3.3386178016662598  and  1.9030518531799316\n",
      "total error:  6.959026098251343\n",
      "training error:  1.630723476409912  and  2.968285083770752  and  2.5809497833251953\n",
      "total error:  7.179958343505859\n",
      "training error:  1.5888481140136719  and  2.20736026763916  and  1.6471351385116577\n",
      "total error:  5.44334352016449\n",
      "training error:  1.6769545078277588  and  3.1813714504241943  and  1.6316180229187012\n",
      "total error:  6.489943981170654\n",
      "training error:  1.534071445465088  and  2.755387306213379  and  2.3112330436706543\n",
      "total error:  6.600691795349121\n",
      "training error:  1.602529764175415  and  2.0975146293640137  and  1.8312721252441406\n",
      "total error:  5.531316518783569\n",
      "training error:  1.7821589708328247  and  4.091829299926758  and  1.5698131322860718\n",
      "total error:  7.443801403045654\n",
      "training error:  1.564098596572876  and  2.1336872577667236  and  1.5550792217254639\n",
      "total error:  5.2528650760650635\n",
      "training error:  1.5162956714630127  and  2.177640914916992  and  1.675884485244751\n",
      "total error:  5.369821071624756\n",
      "training error:  1.5841968059539795  and  2.4264414310455322  and  1.6977049112319946\n",
      "total error:  5.708343148231506\n",
      "training error:  1.505852222442627  and  2.394148111343384  and  1.978315830230713\n",
      "total error:  5.878316164016724\n",
      "training error:  1.6100941896438599  and  3.2579479217529297  and  1.694031834602356\n",
      "total error:  6.5620739459991455\n",
      "training error:  1.4574161767959595  and  2.492603063583374  and  1.6205072402954102\n",
      "total error:  5.570526480674744\n",
      "training error:  1.5132094621658325  and  2.035684108734131  and  1.5003697872161865\n",
      "total error:  5.04926335811615\n",
      "training error:  1.5616694688796997  and  3.271245241165161  and  1.7903797626495361\n",
      "total error:  6.623294472694397\n",
      "training error:  1.5473556518554688  and  2.556025505065918  and  1.8310520648956299\n",
      "total error:  5.934433221817017\n",
      "training error:  1.493452787399292  and  2.5835471153259277  and  1.7858901023864746\n",
      "total error:  5.862890005111694\n",
      "training error:  1.7226810455322266  and  3.16086745262146  and  1.7448742389678955\n",
      "total error:  6.628422737121582\n",
      "training error:  1.476639986038208  and  2.4769248962402344  and  1.5456762313842773\n",
      "total error:  5.49924111366272\n",
      "training error:  1.5060175657272339  and  2.7048895359039307  and  1.8730143308639526\n",
      "total error:  6.083921432495117\n",
      "training error:  1.4495278596878052  and  2.214693546295166  and  1.7058532238006592\n",
      "total error:  5.37007462978363\n",
      "training error:  1.6212365627288818  and  2.5592315196990967  and  1.7799155712127686\n",
      "total error:  5.960383653640747\n",
      "training error:  1.543758749961853  and  3.4251368045806885  and  2.4948549270629883\n",
      "total error:  7.46375048160553\n",
      "training error:  1.4860782623291016  and  2.3342013359069824  and  1.588066577911377\n",
      "total error:  5.408346176147461\n",
      "training error:  1.4573330879211426  and  2.46793270111084  and  1.7532379627227783\n",
      "total error:  5.678503751754761\n",
      "training error:  1.4313178062438965  and  2.9605460166931152  and  2.2421841621398926\n",
      "total error:  6.634047985076904\n",
      "training error:  1.4706672430038452  and  2.2499382495880127  and  1.5181246995925903\n",
      "total error:  5.238730192184448\n",
      "training error:  1.5526704788208008  and  2.8204901218414307  and  1.6863348484039307\n",
      "total error:  6.059495449066162\n",
      "training error:  1.419079065322876  and  2.5497426986694336  and  1.92723548412323\n",
      "total error:  5.8960572481155396\n",
      "training error:  1.4493272304534912  and  2.1392922401428223  and  1.919474482536316\n",
      "total error:  5.508093953132629\n",
      "training error:  1.515840768814087  and  3.5330886840820312  and  1.91900634765625\n",
      "total error:  6.967935800552368\n",
      "training error:  1.532270073890686  and  4.573838233947754  and  2.5715436935424805\n",
      "total error:  8.67765200138092\n",
      "training error:  1.5050544738769531  and  2.1213538646698  and  1.5030088424682617\n",
      "total error:  5.129417181015015\n",
      "training error:  1.4712132215499878  and  2.7398717403411865  and  1.5766608715057373\n",
      "total error:  5.787745833396912\n",
      "training error:  1.4042716026306152  and  2.652036666870117  and  1.555210828781128\n",
      "total error:  5.61151909828186\n",
      "training error:  1.4048583507537842  and  2.0373706817626953  and  1.4533352851867676\n",
      "total error:  4.895564317703247\n",
      "training error:  1.593702793121338  and  4.761042594909668  and  1.551141619682312\n",
      "total error:  7.905887007713318\n",
      "training error:  1.4290580749511719  and  2.245763063430786  and  1.6444051265716553\n",
      "total error:  5.319226264953613\n",
      "training error:  1.3847708702087402  and  2.278916597366333  and  1.6111624240875244\n",
      "total error:  5.274849891662598\n",
      "training error:  1.5687551498413086  and  4.070537567138672  and  1.7658158540725708\n",
      "total error:  7.405108571052551\n",
      "training error:  1.4415680170059204  and  2.346160650253296  and  1.742038607597351\n",
      "total error:  5.529767274856567\n",
      "training error:  1.4431092739105225  and  3.113023042678833  and  2.1994972229003906\n",
      "total error:  6.755629539489746\n",
      "training error:  1.4182013273239136  and  2.0130863189697266  and  1.613484263420105\n",
      "total error:  5.044771909713745\n",
      "training error:  1.5442579984664917  and  3.68464994430542  and  1.6859192848205566\n",
      "total error:  6.914827227592468\n",
      "training error:  1.3681954145431519  and  2.1581692695617676  and  1.9121222496032715\n",
      "total error:  5.438486933708191\n",
      "training error:  1.4012115001678467  and  2.9037129878997803  and  2.07926869392395\n",
      "total error:  6.384193181991577\n",
      "training error:  1.450864553451538  and  2.009354829788208  and  1.6247947216033936\n",
      "total error:  5.08501410484314\n",
      "training error:  1.5265984535217285  and  3.582350730895996  and  1.5384941101074219\n",
      "total error:  6.6474432945251465\n",
      "training error:  1.3572001457214355  and  1.97794508934021  and  1.575218915939331\n",
      "total error:  4.910364151000977\n",
      "training error:  1.3554407358169556  and  2.562903642654419  and  1.6765005588531494\n",
      "total error:  5.594844937324524\n",
      "training error:  1.4812566041946411  and  3.3669373989105225  and  1.5778875350952148\n",
      "total error:  6.426081538200378\n",
      "training error:  1.3814438581466675  and  2.232537031173706  and  1.6548941135406494\n",
      "total error:  5.268875002861023\n",
      "training error:  1.3557744026184082  and  3.091362476348877  and  1.966134786605835\n",
      "total error:  6.41327166557312\n",
      "training error:  1.3617184162139893  and  2.3833487033843994  and  1.5901765823364258\n",
      "total error:  5.3352437019348145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.388885736465454  and  2.369910717010498  and  1.4723564386367798\n",
      "total error:  5.231152892112732\n",
      "training error:  1.323105812072754  and  1.9557515382766724  and  1.5739493370056152\n",
      "total error:  4.8528066873550415\n",
      "training error:  1.3234117031097412  and  2.0262324810028076  and  1.479233741760254\n",
      "total error:  4.828877925872803\n",
      "training error:  1.3494226932525635  and  1.9852122068405151  and  1.4553626775741577\n",
      "total error:  4.789997577667236\n",
      "training error:  1.3052940368652344  and  2.2015583515167236  and  1.590705156326294\n",
      "total error:  5.097557544708252\n",
      "training error:  1.3390984535217285  and  2.522245407104492  and  1.5170924663543701\n",
      "total error:  5.378436326980591\n",
      "training error:  1.3283052444458008  and  2.1996772289276123  and  1.6121844053268433\n",
      "total error:  5.140166878700256\n",
      "training error:  1.283340334892273  and  2.2637763023376465  and  1.5765883922576904\n",
      "total error:  5.12370502948761\n",
      "training error:  1.3050131797790527  and  2.1493589878082275  and  1.468032717704773\n",
      "total error:  4.922404885292053\n",
      "training error:  1.3411786556243896  and  3.933443069458008  and  2.3704779148101807\n",
      "total error:  7.645099639892578\n",
      "training error:  1.352484941482544  and  3.402334213256836  and  1.4668333530426025\n",
      "total error:  6.221652507781982\n",
      "training error:  1.3123635053634644  and  1.9320173263549805  and  1.404883623123169\n",
      "total error:  4.649264454841614\n",
      "training error:  1.2965434789657593  and  2.865257740020752  and  1.546959638595581\n",
      "total error:  5.708760857582092\n",
      "training error:  1.3345792293548584  and  3.916665554046631  and  1.4839589595794678\n",
      "total error:  6.735203742980957\n",
      "training error:  1.269634485244751  and  2.0677895545959473  and  1.906093716621399\n",
      "total error:  5.243517756462097\n",
      "training error:  1.2991222143173218  and  3.6508541107177734  and  2.495683193206787\n",
      "total error:  7.445659518241882\n",
      "training error:  1.6306068897247314  and  3.8953003883361816  and  1.6286956071853638\n",
      "total error:  7.154602885246277\n",
      "training error:  1.3026540279388428  and  1.9556832313537598  and  1.4179834127426147\n",
      "total error:  4.676320672035217\n",
      "training error:  1.3695403337478638  and  4.004043102264404  and  2.301675796508789\n",
      "total error:  7.675259232521057\n",
      "training error:  1.2817882299423218  and  1.9934091567993164  and  1.4451971054077148\n",
      "total error:  4.720394492149353\n",
      "training error:  1.4259569644927979  and  4.203755855560303  and  1.654689073562622\n",
      "total error:  7.284401893615723\n",
      "training error:  1.465761423110962  and  4.026413917541504  and  1.784253478050232\n",
      "total error:  7.276428818702698\n",
      "training error:  1.349778652191162  and  2.745525360107422  and  1.584404468536377\n",
      "total error:  5.679708480834961\n",
      "training error:  1.398964762687683  and  2.5254666805267334  and  1.5985753536224365\n",
      "total error:  5.523006796836853\n",
      "training error:  1.3118414878845215  and  1.9165065288543701  and  1.5681073665618896\n",
      "total error:  4.796455383300781\n",
      "training error:  1.2917203903198242  and  2.139167308807373  and  1.7492088079452515\n",
      "total error:  5.180096507072449\n",
      "training error:  1.316619873046875  and  2.0186920166015625  and  1.6563524007797241\n",
      "total error:  4.991664290428162\n",
      "training error:  1.2186317443847656  and  2.185734748840332  and  1.54572331905365\n",
      "total error:  4.950089812278748\n",
      "training error:  1.229229211807251  and  2.0326764583587646  and  1.5165276527404785\n",
      "total error:  4.778433322906494\n",
      "training error:  1.2306355237960815  and  1.9802393913269043  and  1.4563153982162476\n",
      "total error:  4.667190313339233\n",
      "training error:  1.264746904373169  and  1.7809009552001953  and  1.4589773416519165\n",
      "total error:  4.504625201225281\n",
      "training error:  1.2437829971313477  and  1.8064074516296387  and  1.4379135370254517\n",
      "total error:  4.488103985786438\n",
      "training error:  1.2521491050720215  and  2.0348196029663086  and  1.44172203540802\n",
      "total error:  4.72869074344635\n",
      "training error:  1.2572654485702515  and  1.970811128616333  and  1.5226879119873047\n",
      "total error:  4.750764489173889\n",
      "training error:  1.2543553113937378  and  1.8710343837738037  and  1.3833681344985962\n",
      "total error:  4.508757829666138\n",
      "training error:  1.2482976913452148  and  1.8152519464492798  and  1.330225944519043\n",
      "total error:  4.393775582313538\n",
      "training error:  1.2458100318908691  and  2.1356496810913086  and  1.5478711128234863\n",
      "total error:  4.929330825805664\n",
      "training error:  1.2179744243621826  and  1.9530774354934692  and  1.3797972202301025\n",
      "total error:  4.550849080085754\n",
      "training error:  1.2481651306152344  and  2.543891429901123  and  1.44452965259552\n",
      "total error:  5.236586213111877\n",
      "training error:  1.253906488418579  and  2.271768569946289  and  1.472296118736267\n",
      "total error:  4.997971177101135\n",
      "training error:  1.2525137662887573  and  2.0337040424346924  and  1.4371756315231323\n",
      "total error:  4.723393440246582\n",
      "training error:  1.287626028060913  and  3.096031665802002  and  1.6012492179870605\n",
      "total error:  5.984906911849976\n",
      "training error:  1.4938278198242188  and  4.019800186157227  and  1.5630183219909668\n",
      "total error:  7.076646327972412\n",
      "training error:  1.276584267616272  and  2.033205270767212  and  1.4469223022460938\n",
      "total error:  4.756711840629578\n",
      "training error:  1.2884321212768555  and  2.978917360305786  and  1.7178808450698853\n",
      "total error:  5.985230326652527\n",
      "training error:  1.2697601318359375  and  1.9590312242507935  and  1.5038232803344727\n",
      "total error:  4.732614636421204\n",
      "training error:  1.2631375789642334  and  2.35298752784729  and  1.4500030279159546\n",
      "total error:  5.066128134727478\n",
      "training error:  1.2832788228988647  and  2.3087375164031982  and  1.4348857402801514\n",
      "total error:  5.026902079582214\n",
      "training error:  1.2060173749923706  and  2.029153823852539  and  1.7939516305923462\n",
      "total error:  5.029122829437256\n",
      "training error:  1.231687068939209  and  1.8247181177139282  and  1.4888403415679932\n",
      "total error:  4.54524552822113\n",
      "training error:  1.2045989036560059  and  1.9037675857543945  and  1.4370720386505127\n",
      "total error:  4.545438528060913\n",
      "training error:  1.2803863286972046  and  1.955923080444336  and  1.4627851247787476\n",
      "total error:  4.699094533920288\n",
      "training error:  1.2356984615325928  and  2.6593213081359863  and  1.4445677995681763\n",
      "total error:  5.339587569236755\n",
      "training error:  1.2220946550369263  and  1.9075853824615479  and  1.5330719947814941\n",
      "total error:  4.662752032279968\n",
      "training error:  1.1990396976470947  and  1.822706699371338  and  1.4391871690750122\n",
      "total error:  4.460933566093445\n",
      "training error:  1.266364574432373  and  2.682992458343506  and  1.5219653844833374\n",
      "total error:  5.471322417259216\n",
      "training error:  1.2573723793029785  and  3.1580467224121094  and  1.4374220371246338\n",
      "total error:  5.852841138839722\n",
      "training error:  1.2037080526351929  and  1.8610155582427979  and  1.5659594535827637\n",
      "total error:  4.630683064460754\n",
      "training error:  1.2603386640548706  and  3.140197992324829  and  1.462083101272583\n",
      "total error:  5.862619757652283\n",
      "training error:  1.2877358198165894  and  3.9539682865142822  and  1.4571728706359863\n",
      "total error:  6.698876976966858\n",
      "training error:  1.2386246919631958  and  1.8288187980651855  and  1.4831163883209229\n",
      "total error:  4.550559878349304\n",
      "training error:  1.2389731407165527  and  2.997678279876709  and  1.7915180921554565\n",
      "total error:  6.028169512748718\n",
      "training error:  1.2590620517730713  and  1.8755794763565063  and  1.3824081420898438\n",
      "total error:  4.517049670219421\n",
      "training error:  1.2395884990692139  and  3.1903061866760254  and  1.4763212203979492\n",
      "total error:  5.9062159061431885\n",
      "training error:  1.2538633346557617  and  1.8910945653915405  and  1.5941619873046875\n",
      "total error:  4.73911988735199\n",
      "training error:  1.2149478197097778  and  2.4391210079193115  and  1.4570810794830322\n",
      "total error:  5.111149907112122\n",
      "training error:  1.245512843132019  and  2.5539681911468506  and  1.375326156616211\n",
      "total error:  5.174807190895081\n",
      "training error:  1.2112442255020142  and  1.8310534954071045  and  1.507987141609192\n",
      "total error:  4.5502848625183105\n",
      "training error:  1.3010034561157227  and  2.9016435146331787  and  1.639458179473877\n",
      "total error:  5.842105150222778\n",
      "training error:  1.2079365253448486  and  1.8621963262557983  and  1.4164795875549316\n",
      "total error:  4.486612439155579\n",
      "training error:  1.2574927806854248  and  2.684039354324341  and  1.4642164707183838\n",
      "total error:  5.405748605728149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.2382625341415405  and  1.842114806175232  and  1.4792684316635132\n",
      "total error:  4.559645771980286\n",
      "training error:  1.2185481786727905  and  1.8434884548187256  and  1.4447506666183472\n",
      "total error:  4.506787300109863\n",
      "training error:  1.2255820035934448  and  1.8285335302352905  and  1.4103959798812866\n",
      "total error:  4.464511513710022\n",
      "training error:  1.230910062789917  and  1.772304892539978  and  1.4131287336349487\n",
      "total error:  4.416343688964844\n",
      "training error:  1.1909915208816528  and  1.8601007461547852  and  1.2593872547149658\n",
      "total error:  4.310479521751404\n",
      "training error:  1.204993724822998  and  1.881202220916748  and  1.4790095090866089\n",
      "total error:  4.565205454826355\n",
      "training error:  1.2116484642028809  and  1.8271564245224  and  1.379004955291748\n",
      "total error:  4.417809844017029\n",
      "training error:  1.1799834966659546  and  1.8964927196502686  and  1.3821077346801758\n",
      "total error:  4.458583950996399\n",
      "training error:  1.1827647686004639  and  1.7544901371002197  and  1.3492724895477295\n",
      "total error:  4.286527395248413\n",
      "training error:  1.1980913877487183  and  1.8604490756988525  and  1.3393123149871826\n",
      "total error:  4.397852778434753\n",
      "training error:  1.1831060647964478  and  1.764664649963379  and  1.3012304306030273\n",
      "total error:  4.249001145362854\n",
      "training error:  1.1829190254211426  and  1.7576171159744263  and  1.3216273784637451\n",
      "total error:  4.262163519859314\n",
      "training error:  1.1659533977508545  and  1.7308552265167236  and  1.3351373672485352\n",
      "total error:  4.231945991516113\n",
      "training error:  1.158586859703064  and  1.7146282196044922  and  1.3377354145050049\n",
      "total error:  4.210950493812561\n",
      "training error:  1.1432249546051025  and  1.7294307947158813  and  1.342059850692749\n",
      "total error:  4.214715600013733\n",
      "training error:  1.1629070043563843  and  1.6847373247146606  and  1.3712602853775024\n",
      "total error:  4.218904614448547\n",
      "training error:  1.185811161994934  and  1.6786761283874512  and  1.3447823524475098\n",
      "total error:  4.209269642829895\n",
      "training error:  1.1628506183624268  and  1.7776906490325928  and  1.343901515007019\n",
      "total error:  4.284442782402039\n",
      "training error:  1.156796932220459  and  1.685091257095337  and  1.3099669218063354\n",
      "total error:  4.151855111122131\n",
      "training error:  1.1873623132705688  and  1.744258999824524  and  1.3367209434509277\n",
      "total error:  4.2683422565460205\n",
      "training error:  1.2108266353607178  and  1.73345947265625  and  1.3726928234100342\n",
      "total error:  4.316978931427002\n",
      "training error:  1.1497220993041992  and  1.7392969131469727  and  1.3036715984344482\n",
      "total error:  4.19269061088562\n",
      "training error:  1.1655619144439697  and  1.8306548595428467  and  1.3438749313354492\n",
      "total error:  4.340091705322266\n",
      "training error:  1.1527743339538574  and  1.698028326034546  and  1.322166919708252\n",
      "total error:  4.172969579696655\n",
      "training error:  1.1732845306396484  and  1.70589017868042  and  1.3009757995605469\n",
      "total error:  4.180150508880615\n",
      "training error:  1.1574819087982178  and  1.6741055250167847  and  1.2840468883514404\n",
      "total error:  4.115634322166443\n",
      "training error:  1.166503667831421  and  1.6369640827178955  and  1.3136903047561646\n",
      "total error:  4.117158055305481\n",
      "training error:  1.1617534160614014  and  1.7479360103607178  and  1.3076183795928955\n",
      "total error:  4.217307806015015\n",
      "training error:  1.185836672782898  and  1.7521662712097168  and  1.3169705867767334\n",
      "total error:  4.254973530769348\n",
      "training error:  1.1519668102264404  and  1.7431437969207764  and  1.3182382583618164\n",
      "total error:  4.213348865509033\n",
      "training error:  1.1715788841247559  and  1.7317951917648315  and  1.2983832359313965\n",
      "total error:  4.201757311820984\n",
      "training error:  1.169088363647461  and  1.7316980361938477  and  1.3461934328079224\n",
      "total error:  4.246979832649231\n",
      "training error:  1.1391018629074097  and  1.6830947399139404  and  1.3199634552001953\n",
      "total error:  4.142160058021545\n",
      "training error:  1.1427031755447388  and  1.7496933937072754  and  1.3125683069229126\n",
      "total error:  4.204964876174927\n",
      "training error:  1.1316828727722168  and  1.6604235172271729  and  1.2937283515930176\n",
      "total error:  4.085834741592407\n",
      "training error:  1.1631317138671875  and  1.6486701965332031  and  1.312307357788086\n",
      "total error:  4.124109268188477\n",
      "training error:  1.1356314420700073  and  1.7206696271896362  and  1.3479396104812622\n",
      "total error:  4.204240679740906\n",
      "training error:  1.1673496961593628  and  1.6877573728561401  and  1.2655951976776123\n",
      "total error:  4.120702266693115\n",
      "training error:  1.128356695175171  and  1.6198034286499023  and  1.328773856163025\n",
      "total error:  4.076933979988098\n",
      "training error:  1.1556340456008911  and  1.6838722229003906  and  1.3571587800979614\n",
      "total error:  4.196665048599243\n",
      "training error:  1.1324586868286133  and  1.689510703086853  and  1.3384790420532227\n",
      "total error:  4.160448431968689\n",
      "training error:  1.1464513540267944  and  1.6739253997802734  and  1.3203305006027222\n",
      "total error:  4.14070725440979\n",
      "training error:  1.1347143650054932  and  1.619957447052002  and  1.2644691467285156\n",
      "total error:  4.019140958786011\n",
      "training error:  1.1602239608764648  and  1.6444790363311768  and  1.2924220561981201\n",
      "total error:  4.097125053405762\n",
      "training error:  1.1543021202087402  and  1.6966900825500488  and  1.3152544498443604\n",
      "total error:  4.166246652603149\n",
      "training error:  1.1700308322906494  and  1.6509945392608643  and  1.2945793867111206\n",
      "total error:  4.115604758262634\n",
      "training error:  1.1331568956375122  and  1.6480122804641724  and  1.3240387439727783\n",
      "total error:  4.105207920074463\n",
      "training error:  1.1128730773925781  and  1.6294440031051636  and  1.2777576446533203\n",
      "total error:  4.020074725151062\n",
      "training error:  1.1000165939331055  and  1.6282639503479004  and  1.2701075077056885\n",
      "total error:  3.9983880519866943\n",
      "training error:  1.1159254312515259  and  1.6749753952026367  and  1.3066151142120361\n",
      "total error:  4.097515940666199\n",
      "training error:  1.1065678596496582  and  1.7092100381851196  and  1.3367130756378174\n",
      "total error:  4.152490973472595\n",
      "training error:  1.1371777057647705  and  1.6333447694778442  and  1.2898352146148682\n",
      "total error:  4.060357689857483\n",
      "training error:  1.122945785522461  and  1.6674079895019531  and  1.2627384662628174\n",
      "total error:  4.0530922412872314\n",
      "training error:  1.1461937427520752  and  1.6204099655151367  and  1.300097942352295\n",
      "total error:  4.066701650619507\n",
      "training error:  1.0878413915634155  and  1.5974687337875366  and  1.291446328163147\n",
      "total error:  3.976756453514099\n",
      "training error:  1.129084587097168  and  1.637550950050354  and  1.2882564067840576\n",
      "total error:  4.05489194393158\n",
      "training error:  1.1404772996902466  and  1.6555445194244385  and  1.296583890914917\n",
      "total error:  4.092605710029602\n",
      "training error:  1.1397185325622559  and  1.6042579412460327  and  1.2939836978912354\n",
      "total error:  4.037960171699524\n",
      "training error:  1.1405138969421387  and  1.6967546939849854  and  1.2702319622039795\n",
      "total error:  4.1075005531311035\n",
      "training error:  1.1027557849884033  and  1.7606384754180908  and  1.3544533252716064\n",
      "total error:  4.217847585678101\n",
      "training error:  1.102906346321106  and  1.612341046333313  and  1.324979543685913\n",
      "total error:  4.040226936340332\n",
      "training error:  1.1142120361328125  and  1.629263162612915  and  1.2961523532867432\n",
      "total error:  4.039627552032471\n",
      "training error:  1.1546003818511963  and  1.6495208740234375  and  1.2440539598464966\n",
      "total error:  4.04817521572113\n",
      "training error:  1.122565746307373  and  1.5839030742645264  and  1.2963848114013672\n",
      "total error:  4.002853631973267\n",
      "training error:  1.1486281156539917  and  1.7535384893417358  and  1.292764663696289\n",
      "total error:  4.194931268692017\n",
      "training error:  1.1372278928756714  and  1.7078406810760498  and  1.332696795463562\n",
      "total error:  4.177765369415283\n",
      "training error:  1.123631477355957  and  1.7277463674545288  and  1.345996379852295\n",
      "total error:  4.197374224662781\n",
      "training error:  1.0915732383728027  and  1.6667358875274658  and  1.3070720434188843\n",
      "total error:  4.065381169319153\n",
      "training error:  1.1293936967849731  and  1.7663202285766602  and  1.2552642822265625\n",
      "total error:  4.150978207588196\n",
      "training error:  1.101313591003418  and  1.583564281463623  and  1.2621102333068848\n",
      "total error:  3.946988105773926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.1419776678085327  and  1.8048418760299683  and  1.3770833015441895\n",
      "total error:  4.32390284538269\n",
      "training error:  1.1107831001281738  and  1.8029401302337646  and  1.2556047439575195\n",
      "total error:  4.169327974319458\n",
      "training error:  1.1197806596755981  and  1.8206379413604736  and  1.3343799114227295\n",
      "total error:  4.274798512458801\n",
      "training error:  1.125493049621582  and  1.6406711339950562  and  1.253355860710144\n",
      "total error:  4.019520044326782\n",
      "training error:  1.1225330829620361  and  1.6401667594909668  and  1.2924052476882935\n",
      "total error:  4.055105090141296\n",
      "training error:  1.0880311727523804  and  1.7003262042999268  and  1.275097131729126\n",
      "total error:  4.063454508781433\n",
      "training error:  1.1091110706329346  and  1.6247878074645996  and  1.2767285108566284\n",
      "total error:  4.010627388954163\n",
      "training error:  1.1131985187530518  and  1.6765235662460327  and  1.2772393226623535\n",
      "total error:  4.066961407661438\n",
      "training error:  1.1287081241607666  and  1.8007919788360596  and  1.2796846628189087\n",
      "total error:  4.209184765815735\n",
      "training error:  1.1485260725021362  and  1.5711556673049927  and  1.275019884109497\n",
      "total error:  3.994701623916626\n",
      "training error:  1.0959632396697998  and  1.5663714408874512  and  1.3102489709854126\n",
      "total error:  3.9725836515426636\n",
      "training error:  1.114123821258545  and  1.6066503524780273  and  1.2258843183517456\n",
      "total error:  3.946658492088318\n",
      "training error:  1.1176238059997559  and  1.6042348146438599  and  1.2916738986968994\n",
      "total error:  4.013532519340515\n",
      "training error:  1.1376630067825317  and  1.6736723184585571  and  1.2375991344451904\n",
      "total error:  4.048934459686279\n",
      "training error:  1.1090610027313232  and  1.6113110780715942  and  1.3034780025482178\n",
      "total error:  4.023850083351135\n",
      "training error:  1.1218301057815552  and  1.656960129737854  and  1.2640243768692017\n",
      "total error:  4.042814612388611\n",
      "training error:  1.1192550659179688  and  1.6083744764328003  and  1.2894600629806519\n",
      "total error:  4.017089605331421\n",
      "training error:  1.1034778356552124  and  1.593684434890747  and  1.242760181427002\n",
      "total error:  3.9399224519729614\n",
      "training error:  1.0982017517089844  and  1.6765414476394653  and  1.285588026046753\n",
      "total error:  4.060331225395203\n",
      "training error:  1.127418041229248  and  1.745676875114441  and  1.320932388305664\n",
      "total error:  4.194027304649353\n",
      "training error:  1.1112724542617798  and  1.7601964473724365  and  1.2829780578613281\n",
      "total error:  4.154446959495544\n",
      "training error:  1.1084134578704834  and  1.6134827136993408  and  1.276486873626709\n",
      "total error:  3.998383045196533\n",
      "training error:  1.0694814920425415  and  1.6439082622528076  and  1.2553871870040894\n",
      "total error:  3.9687769412994385\n",
      "training error:  1.0697420835494995  and  1.5746856927871704  and  1.3358930349349976\n",
      "total error:  3.9803208112716675\n",
      "training error:  1.124570369720459  and  1.702547550201416  and  1.2669645547866821\n",
      "total error:  4.094082474708557\n",
      "training error:  1.0893702507019043  and  1.5544250011444092  and  1.2563605308532715\n",
      "total error:  3.900155782699585\n",
      "training error:  1.1168044805526733  and  1.5981242656707764  and  1.2434508800506592\n",
      "total error:  3.958379626274109\n",
      "training error:  1.0645798444747925  and  1.5882235765457153  and  1.2502005100250244\n",
      "total error:  3.9030039310455322\n",
      "training error:  1.0957175493240356  and  1.5605357885360718  and  1.26775062084198\n",
      "total error:  3.9240039587020874\n",
      "training error:  1.1179403066635132  and  1.6564617156982422  and  1.224316120147705\n",
      "total error:  3.9987181425094604\n",
      "training error:  1.0880696773529053  and  1.5694694519042969  and  1.275246262550354\n",
      "total error:  3.932785391807556\n",
      "training error:  1.0763113498687744  and  1.6231770515441895  and  1.2474874258041382\n",
      "total error:  3.946975827217102\n",
      "training error:  1.0966624021530151  and  1.6227669715881348  and  1.2601819038391113\n",
      "total error:  3.9796112775802612\n",
      "training error:  1.104620099067688  and  1.5667757987976074  and  1.2190563678741455\n",
      "total error:  3.890452265739441\n",
      "training error:  1.1080646514892578  and  1.5672353506088257  and  1.2409147024154663\n",
      "total error:  3.91621470451355\n",
      "training error:  1.0687395334243774  and  1.5363248586654663  and  1.224102258682251\n",
      "total error:  3.8291666507720947\n",
      "training error:  1.057947039604187  and  1.5710349082946777  and  1.2616753578186035\n",
      "total error:  3.8906573057174683\n",
      "training error:  1.0958131551742554  and  1.5152287483215332  and  1.2441145181655884\n",
      "total error:  3.855156421661377\n",
      "training error:  1.0606318712234497  and  1.5427026748657227  and  1.2740017175674438\n",
      "total error:  3.877336263656616\n",
      "training error:  1.060194492340088  and  1.589076280593872  and  1.2275315523147583\n",
      "total error:  3.8768023252487183\n",
      "training error:  1.0805745124816895  and  1.5829291343688965  and  1.2895296812057495\n",
      "total error:  3.9530333280563354\n",
      "training error:  1.0977792739868164  and  1.5696563720703125  and  1.238947868347168\n",
      "total error:  3.906383514404297\n",
      "training error:  1.0837268829345703  and  1.552060842514038  and  1.2272146940231323\n",
      "total error:  3.8630024194717407\n",
      "training error:  1.08426833152771  and  1.5453269481658936  and  1.2330918312072754\n",
      "total error:  3.862687110900879\n",
      "training error:  1.047646403312683  and  1.5438945293426514  and  1.2086732387542725\n",
      "total error:  3.800214171409607\n",
      "training error:  1.1133333444595337  and  1.6392240524291992  and  1.26950204372406\n",
      "total error:  4.022059440612793\n",
      "training error:  1.1158549785614014  and  1.6147067546844482  and  1.2452716827392578\n",
      "total error:  3.9758334159851074\n",
      "training error:  1.074782133102417  and  1.5234453678131104  and  1.2328166961669922\n",
      "total error:  3.8310441970825195\n",
      "training error:  1.0858887434005737  and  1.5796152353286743  and  1.2271676063537598\n",
      "total error:  3.892671585083008\n",
      "training error:  1.0977506637573242  and  1.570748209953308  and  1.233019471168518\n",
      "total error:  3.9015183448791504\n",
      "training error:  1.0291249752044678  and  1.5448112487792969  and  1.2281430959701538\n",
      "total error:  3.8020793199539185\n",
      "training error:  1.077035665512085  and  1.5272727012634277  and  1.2404861450195312\n",
      "total error:  3.844794511795044\n",
      "training error:  1.0977725982666016  and  1.741224765777588  and  1.2727222442626953\n",
      "total error:  4.111719608306885\n",
      "training error:  1.044228196144104  and  1.579716444015503  and  1.3021161556243896\n",
      "total error:  3.9260607957839966\n",
      "training error:  1.073635458946228  and  1.579634189605713  and  1.3229702711105347\n",
      "total error:  3.9762399196624756\n",
      "training error:  1.075822353363037  and  1.7157495021820068  and  1.3091827630996704\n",
      "total error:  4.100754618644714\n",
      "training error:  1.0821857452392578  and  1.5460405349731445  and  1.1942566633224487\n",
      "total error:  3.822482943534851\n",
      "training error:  1.094362735748291  and  1.527665376663208  and  1.2250075340270996\n",
      "total error:  3.8470356464385986\n",
      "training error:  1.0908458232879639  and  1.497706651687622  and  1.2255845069885254\n",
      "total error:  3.8141369819641113\n",
      "training error:  1.1102359294891357  and  1.626841425895691  and  1.3194565773010254\n",
      "total error:  4.056533932685852\n",
      "training error:  1.08383309841156  and  1.5562057495117188  and  1.28013014793396\n",
      "total error:  3.9201689958572388\n",
      "training error:  1.0982407331466675  and  1.5750813484191895  and  1.2781130075454712\n",
      "total error:  3.951435089111328\n",
      "training error:  1.0669914484024048  and  1.577073574066162  and  1.246218204498291\n",
      "total error:  3.890283226966858\n",
      "training error:  1.070241093635559  and  1.577193021774292  and  1.2195582389831543\n",
      "total error:  3.8669923543930054\n",
      "training error:  1.0914583206176758  and  1.54490065574646  and  1.181204080581665\n",
      "total error:  3.817563056945801\n",
      "training error:  1.0564461946487427  and  1.6541154384613037  and  1.3042056560516357\n",
      "total error:  4.014767289161682\n",
      "training error:  1.063481330871582  and  1.5310826301574707  and  1.2297019958496094\n",
      "total error:  3.824265956878662\n",
      "training error:  1.0606048107147217  and  1.5323156118392944  and  1.2833664417266846\n",
      "total error:  3.8762868642807007\n",
      "training error:  1.1102631092071533  and  1.681105613708496  and  1.2478461265563965\n",
      "total error:  4.039214849472046\n",
      "training error:  1.0603687763214111  and  1.5494024753570557  and  1.2312289476394653\n",
      "total error:  3.841000199317932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.089461326599121  and  1.6288198232650757  and  1.2641561031341553\n",
      "total error:  3.982437252998352\n",
      "training error:  1.1010892391204834  and  1.5396218299865723  and  1.247085690498352\n",
      "total error:  3.8877967596054077\n",
      "training error:  1.095976710319519  and  1.5253914594650269  and  1.2092905044555664\n",
      "total error:  3.8306586742401123\n",
      "training error:  1.0600839853286743  and  1.8766369819641113  and  1.2565028667449951\n",
      "total error:  4.193223834037781\n",
      "training error:  1.0790317058563232  and  1.5622378587722778  and  1.2273986339569092\n",
      "total error:  3.8686681985855103\n",
      "training error:  1.0479933023452759  and  1.5503958463668823  and  1.2713379859924316\n",
      "total error:  3.86972713470459\n",
      "training error:  1.0633140802383423  and  1.4948043823242188  and  1.2623090744018555\n",
      "total error:  3.8204275369644165\n",
      "training error:  1.0474002361297607  and  1.5202728509902954  and  1.1977450847625732\n",
      "total error:  3.7654181718826294\n",
      "training error:  1.0461562871932983  and  1.500603437423706  and  1.253312349319458\n",
      "total error:  3.8000720739364624\n",
      "training error:  1.0699162483215332  and  1.5068241357803345  and  1.2311336994171143\n",
      "total error:  3.807874083518982\n",
      "training error:  1.0628856420516968  and  1.5438768863677979  and  1.233708143234253\n",
      "total error:  3.8404706716537476\n",
      "training error:  1.0519757270812988  and  1.5952346324920654  and  1.2995896339416504\n",
      "total error:  3.9467999935150146\n",
      "training error:  1.090017318725586  and  1.777637243270874  and  1.3155194520950317\n",
      "total error:  4.183174014091492\n",
      "training error:  1.0605180263519287  and  1.653703212738037  and  1.2871462106704712\n",
      "total error:  4.001367449760437\n",
      "training error:  1.0705440044403076  and  1.6320805549621582  and  1.198050618171692\n",
      "total error:  3.9006751775741577\n",
      "training error:  1.0241985321044922  and  1.5187876224517822  and  1.2196197509765625\n",
      "total error:  3.762605905532837\n",
      "training error:  1.037695050239563  and  1.6169182062149048  and  1.2315673828125\n",
      "total error:  3.8861806392669678\n",
      "training error:  1.0438026189804077  and  1.632919192314148  and  1.256945013999939\n",
      "total error:  3.9336668252944946\n",
      "training error:  1.0620958805084229  and  1.4860529899597168  and  1.2431831359863281\n",
      "total error:  3.7913320064544678\n",
      "training error:  1.0579832792282104  and  1.5180437564849854  and  1.2858762741088867\n",
      "total error:  3.8619033098220825\n",
      "training error:  1.0661756992340088  and  1.5270648002624512  and  1.1883533000946045\n",
      "total error:  3.7815937995910645\n",
      "training error:  1.07931649684906  and  1.522498607635498  and  1.2372324466705322\n",
      "total error:  3.8390475511550903\n",
      "training error:  1.036165475845337  and  1.6229901313781738  and  1.2358392477035522\n",
      "total error:  3.894994854927063\n",
      "training error:  1.0673601627349854  and  1.4864836931228638  and  1.199061393737793\n",
      "total error:  3.752905249595642\n",
      "training error:  1.0781543254852295  and  1.501073956489563  and  1.2622325420379639\n",
      "total error:  3.8414608240127563\n",
      "training error:  1.0553178787231445  and  1.5541138648986816  and  1.2544381618499756\n",
      "total error:  3.8638699054718018\n",
      "training error:  1.070132851600647  and  1.4779853820800781  and  1.211751103401184\n",
      "total error:  3.759869337081909\n",
      "training error:  1.0462775230407715  and  1.4837374687194824  and  1.184302806854248\n",
      "total error:  3.714317798614502\n",
      "training error:  1.0624499320983887  and  1.4897434711456299  and  1.2331346273422241\n",
      "total error:  3.7853280305862427\n",
      "training error:  1.0500211715698242  and  1.7110872268676758  and  1.2207627296447754\n",
      "total error:  3.9818711280822754\n",
      "training error:  1.055647373199463  and  1.714184284210205  and  1.2317813634872437\n",
      "total error:  4.001613020896912\n",
      "training error:  1.0629267692565918  and  1.5449126958847046  and  1.2128844261169434\n",
      "total error:  3.8207238912582397\n",
      "training error:  1.0488073825836182  and  1.5763287544250488  and  1.2221964597702026\n",
      "total error:  3.8473325967788696\n",
      "training error:  1.0559802055358887  and  1.4724934101104736  and  1.2483330965042114\n",
      "total error:  3.7768067121505737\n",
      "training error:  1.04252028465271  and  1.523559331893921  and  1.2144594192504883\n",
      "total error:  3.780539035797119\n",
      "training error:  1.0480519533157349  and  1.5085440874099731  and  1.197258472442627\n",
      "total error:  3.753854513168335\n",
      "training error:  1.0597394704818726  and  1.6938457489013672  and  1.249525547027588\n",
      "total error:  4.003110766410828\n",
      "training error:  1.0552723407745361  and  1.8099603652954102  and  1.2445764541625977\n",
      "total error:  4.109809160232544\n",
      "training error:  1.051267385482788  and  1.7188907861709595  and  1.270329475402832\n",
      "total error:  4.04048764705658\n",
      "training error:  1.0622000694274902  and  1.5878174304962158  and  1.300248622894287\n",
      "total error:  3.950266122817993\n",
      "training error:  1.0438083410263062  and  1.4820959568023682  and  1.2087273597717285\n",
      "total error:  3.734631657600403\n",
      "training error:  1.0293571949005127  and  1.4638307094573975  and  1.2318402528762817\n",
      "total error:  3.725028157234192\n",
      "training error:  1.0306687355041504  and  1.4989203214645386  and  1.1977919340133667\n",
      "total error:  3.7273809909820557\n",
      "training error:  1.0503292083740234  and  1.46184504032135  and  1.2307261228561401\n",
      "total error:  3.7429003715515137\n",
      "training error:  1.061282992362976  and  1.5201348066329956  and  1.325308084487915\n",
      "total error:  3.9067258834838867\n",
      "training error:  1.0086312294006348  and  1.5427755117416382  and  1.2545770406723022\n",
      "total error:  3.805983781814575\n",
      "training error:  1.048534631729126  and  1.478158712387085  and  1.2636311054229736\n",
      "total error:  3.7903244495391846\n",
      "training error:  1.0527689456939697  and  1.499830722808838  and  1.2337777614593506\n",
      "total error:  3.786377429962158\n",
      "training error:  1.039102554321289  and  1.493422031402588  and  1.2262825965881348\n",
      "total error:  3.7588071823120117\n",
      "training error:  1.0413870811462402  and  1.4614909887313843  and  1.2494990825653076\n",
      "total error:  3.752377152442932\n",
      "training error:  1.0490736961364746  and  1.528725504875183  and  1.2338060140609741\n",
      "total error:  3.811605215072632\n",
      "training error:  1.0502244234085083  and  1.4830379486083984  and  1.302840232849121\n",
      "total error:  3.836102604866028\n",
      "training error:  1.0349345207214355  and  1.6858198642730713  and  1.210178017616272\n",
      "total error:  3.930932402610779\n",
      "training error:  1.0448143482208252  and  1.6660661697387695  and  1.2084505558013916\n",
      "total error:  3.9193310737609863\n",
      "training error:  1.06247079372406  and  1.4894983768463135  and  1.2383923530578613\n",
      "total error:  3.790361523628235\n",
      "training error:  1.0451085567474365  and  1.5053715705871582  and  1.1712672710418701\n",
      "total error:  3.721747398376465\n",
      "training error:  1.0548028945922852  and  1.541232943534851  and  1.2169556617736816\n",
      "total error:  3.812991499900818\n",
      "training error:  1.0696892738342285  and  1.8861067295074463  and  1.3122972249984741\n",
      "total error:  4.268093228340149\n",
      "training error:  1.0315062999725342  and  1.4596952199935913  and  1.2112923860549927\n",
      "total error:  3.702493906021118\n",
      "training error:  1.0165935754776  and  1.4991250038146973  and  1.2011407613754272\n",
      "total error:  3.7168593406677246\n",
      "training error:  1.0086734294891357  and  1.6537163257598877  and  1.305860161781311\n",
      "total error:  3.9682499170303345\n",
      "training error:  1.0480711460113525  and  1.546626091003418  and  1.26936936378479\n",
      "total error:  3.8640666007995605\n",
      "training error:  1.0148766040802002  and  1.4508819580078125  and  1.228437900543213\n",
      "total error:  3.6941964626312256\n",
      "training error:  1.0125854015350342  and  1.5043201446533203  and  1.2603645324707031\n",
      "total error:  3.7772700786590576\n",
      "training error:  1.035693883895874  and  1.5249621868133545  and  1.2392642498016357\n",
      "total error:  3.7999203205108643\n",
      "training error:  1.0480417013168335  and  1.523006558418274  and  1.2280136346817017\n",
      "total error:  3.799061894416809\n",
      "training error:  1.0448708534240723  and  1.5222865343093872  and  1.2138954401016235\n",
      "total error:  3.781052827835083\n",
      "training error:  1.0454728603363037  and  1.5073148012161255  and  1.1869196891784668\n",
      "total error:  3.739707350730896\n",
      "training error:  1.0269813537597656  and  1.4608895778656006  and  1.22324538230896\n",
      "total error:  3.711116313934326\n",
      "training error:  1.0389759540557861  and  1.5797960758209229  and  1.277113437652588\n",
      "total error:  3.895885467529297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.012222170829773  and  1.4677387475967407  and  1.2191723585128784\n",
      "total error:  3.699133276939392\n",
      "training error:  1.0270413160324097  and  1.5093976259231567  and  1.2333152294158936\n",
      "total error:  3.76975417137146\n",
      "training error:  1.0307295322418213  and  1.6513617038726807  and  1.1922566890716553\n",
      "total error:  3.8743479251861572\n",
      "training error:  1.0569535493850708  and  1.4838511943817139  and  1.1748242378234863\n",
      "total error:  3.715628981590271\n",
      "training error:  1.0282586812973022  and  1.5228501558303833  and  1.1961021423339844\n",
      "total error:  3.74721097946167\n",
      "training error:  1.0325318574905396  and  1.5252397060394287  and  1.21171236038208\n",
      "total error:  3.7694839239120483\n",
      "training error:  1.0529303550720215  and  1.5137537717819214  and  1.1812458038330078\n",
      "total error:  3.7479299306869507\n",
      "training error:  1.032060980796814  and  1.5032000541687012  and  1.2183351516723633\n",
      "total error:  3.7535961866378784\n",
      "training error:  1.0354633331298828  and  1.5218603610992432  and  1.2798352241516113\n",
      "total error:  3.8371589183807373\n",
      "training error:  1.0591862201690674  and  1.6224825382232666  and  1.2121903896331787\n",
      "total error:  3.8938591480255127\n",
      "training error:  1.018589973449707  and  1.5232913494110107  and  1.2403218746185303\n",
      "total error:  3.782203197479248\n",
      "training error:  1.025526762008667  and  1.448695182800293  and  1.2031757831573486\n",
      "total error:  3.6773977279663086\n",
      "training error:  1.0091958045959473  and  1.4251766204833984  and  1.1875910758972168\n",
      "total error:  3.6219635009765625\n",
      "training error:  1.005437970161438  and  1.4103119373321533  and  1.1929681301116943\n",
      "total error:  3.6087180376052856\n",
      "training error:  1.0471712350845337  and  1.5260541439056396  and  1.3050243854522705\n",
      "total error:  3.878249764442444\n",
      "training error:  1.038339614868164  and  1.6755247116088867  and  1.2550982236862183\n",
      "total error:  3.968962550163269\n",
      "training error:  1.0456897020339966  and  1.4885671138763428  and  1.2026761770248413\n",
      "total error:  3.7369329929351807\n",
      "training error:  1.0079951286315918  and  1.5006717443466187  and  1.2076538801193237\n",
      "total error:  3.716320753097534\n",
      "training error:  1.023493766784668  and  1.4693176746368408  and  1.212472677230835\n",
      "total error:  3.7052841186523438\n",
      "training error:  1.01902174949646  and  1.5574949979782104  and  1.2705808877944946\n",
      "total error:  3.847097635269165\n",
      "training error:  1.0160388946533203  and  1.560050368309021  and  1.1875102519989014\n",
      "total error:  3.7635995149612427\n",
      "training error:  1.03966224193573  and  1.7165225744247437  and  1.2923702001571655\n",
      "total error:  4.048555016517639\n",
      "training error:  0.990435004234314  and  1.427983283996582  and  1.2330214977264404\n",
      "total error:  3.6514397859573364\n",
      "training error:  1.0276646614074707  and  1.5168745517730713  and  1.185081124305725\n",
      "total error:  3.729620337486267\n",
      "training error:  1.0385007858276367  and  1.453042984008789  and  1.2084044218063354\n",
      "total error:  3.6999481916427612\n",
      "training error:  1.0336195230484009  and  1.4715044498443604  and  1.1798876523971558\n",
      "total error:  3.685011625289917\n",
      "training error:  1.0094811916351318  and  1.4523683786392212  and  1.216598629951477\n",
      "total error:  3.67844820022583\n",
      "training error:  1.0197219848632812  and  1.4171299934387207  and  1.2187217473983765\n",
      "total error:  3.6555737257003784\n",
      "training error:  1.0175635814666748  and  1.4498236179351807  and  1.2093174457550049\n",
      "total error:  3.6767046451568604\n",
      "training error:  1.0174565315246582  and  1.4558316469192505  and  1.1921019554138184\n",
      "total error:  3.665390133857727\n",
      "training error:  1.0303287506103516  and  1.4459702968597412  and  1.172844648361206\n",
      "total error:  3.649143695831299\n",
      "training error:  0.9926429986953735  and  1.5258629322052002  and  1.2725396156311035\n",
      "total error:  3.7910455465316772\n",
      "training error:  1.0378475189208984  and  1.4670135974884033  and  1.1639177799224854\n",
      "total error:  3.668778896331787\n",
      "training error:  0.9942668676376343  and  1.734973430633545  and  1.1824536323547363\n",
      "total error:  3.9116939306259155\n",
      "training error:  1.0230900049209595  and  1.6239640712738037  and  1.214687705039978\n",
      "total error:  3.861741781234741\n",
      "training error:  0.9965817928314209  and  1.4460633993148804  and  1.1849286556243896\n",
      "total error:  3.627573847770691\n",
      "training error:  1.0271004438400269  and  1.5112035274505615  and  1.224889874458313\n",
      "total error:  3.7631938457489014\n",
      "training error:  1.0021758079528809  and  1.4525103569030762  and  1.1812845468521118\n",
      "total error:  3.635970711708069\n",
      "training error:  1.0138041973114014  and  1.458016037940979  and  1.2038414478302002\n",
      "total error:  3.6756616830825806\n",
      "training error:  1.0374555587768555  and  1.6144647598266602  and  1.2614905834197998\n",
      "total error:  3.9134109020233154\n",
      "training error:  1.0203784704208374  and  1.8063364028930664  and  1.2507988214492798\n",
      "total error:  4.077513694763184\n",
      "training error:  1.0175570249557495  and  1.6397032737731934  and  1.330146312713623\n",
      "total error:  3.987406611442566\n",
      "training error:  1.008457064628601  and  1.4677129983901978  and  1.1718552112579346\n",
      "total error:  3.6480252742767334\n",
      "training error:  1.04496169090271  and  1.4870021343231201  and  1.2197154760360718\n",
      "total error:  3.751679301261902\n",
      "training error:  1.0123158693313599  and  1.429779291152954  and  1.1826200485229492\n",
      "total error:  3.624715209007263\n",
      "training error:  1.0272960662841797  and  1.4354174137115479  and  1.2001880407333374\n",
      "total error:  3.662901520729065\n",
      "training error:  1.0234436988830566  and  1.4463858604431152  and  1.2054997682571411\n",
      "total error:  3.675329327583313\n",
      "training error:  1.0068886280059814  and  1.406200885772705  and  1.1709102392196655\n",
      "total error:  3.583999752998352\n",
      "training error:  1.0060794353485107  and  1.4426813125610352  and  1.1774730682373047\n",
      "total error:  3.6262338161468506\n",
      "training error:  1.0133992433547974  and  1.4929628372192383  and  1.1822338104248047\n",
      "total error:  3.6885958909988403\n",
      "training error:  1.0276741981506348  and  1.4690788984298706  and  1.2286548614501953\n",
      "total error:  3.7254079580307007\n",
      "training error:  1.0319620370864868  and  1.668006181716919  and  1.1954351663589478\n",
      "total error:  3.8954033851623535\n",
      "training error:  1.0072894096374512  and  1.4472557306289673  and  1.1619160175323486\n",
      "total error:  3.616461157798767\n",
      "training error:  1.0152260065078735  and  1.493245244026184  and  1.1614744663238525\n",
      "total error:  3.66994571685791\n",
      "training error:  1.0154788494110107  and  1.494875192642212  and  1.220413088798523\n",
      "total error:  3.7307671308517456\n",
      "training error:  1.036224603652954  and  1.4709804058074951  and  1.2297325134277344\n",
      "total error:  3.7369375228881836\n",
      "training error:  1.0001230239868164  and  1.431380271911621  and  1.1942830085754395\n",
      "total error:  3.625786304473877\n",
      "training error:  0.9970862865447998  and  1.5860822200775146  and  1.1832115650177002\n",
      "total error:  3.7663800716400146\n",
      "training error:  1.0217489004135132  and  1.6866140365600586  and  1.269235372543335\n",
      "total error:  3.9775983095169067\n",
      "training error:  1.046296238899231  and  1.634705901145935  and  1.226151466369629\n",
      "total error:  3.907153606414795\n",
      "training error:  1.0161709785461426  and  1.4527193307876587  and  1.2006282806396484\n",
      "total error:  3.6695185899734497\n",
      "training error:  1.0004489421844482  and  1.430954098701477  and  1.173680067062378\n",
      "total error:  3.6050831079483032\n",
      "training error:  1.000528335571289  and  1.4520905017852783  and  1.1904921531677246\n",
      "total error:  3.643110990524292\n",
      "training error:  0.9898251295089722  and  1.4585024118423462  and  1.193359136581421\n",
      "total error:  3.6416866779327393\n",
      "training error:  1.0083167552947998  and  1.5452320575714111  and  1.1780685186386108\n",
      "total error:  3.7316173315048218\n",
      "training error:  1.028617262840271  and  1.464418649673462  and  1.2050540447235107\n",
      "total error:  3.6980899572372437\n",
      "training error:  1.0039057731628418  and  1.5034258365631104  and  1.1886906623840332\n",
      "total error:  3.6960222721099854\n",
      "training error:  0.9974603056907654  and  1.496445655822754  and  1.2234911918640137\n",
      "total error:  3.717397153377533\n",
      "training error:  0.9971085786819458  and  1.4203988313674927  and  1.1683493852615356\n",
      "total error:  3.585856795310974\n",
      "training error:  1.0333547592163086  and  1.4352197647094727  and  1.1814707517623901\n",
      "total error:  3.6500452756881714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.0148245096206665  and  1.4470826387405396  and  1.201918363571167\n",
      "total error:  3.663825511932373\n",
      "training error:  1.0097581148147583  and  1.505441427230835  and  1.148956298828125\n",
      "total error:  3.6641558408737183\n",
      "training error:  1.0109916925430298  and  1.4744303226470947  and  1.1986525058746338\n",
      "total error:  3.6840745210647583\n",
      "training error:  1.0165632963180542  and  1.4660077095031738  and  1.2052339315414429\n",
      "total error:  3.687804937362671\n",
      "training error:  0.9944055676460266  and  1.5638887882232666  and  1.1978814601898193\n",
      "total error:  3.7561758160591125\n",
      "training error:  1.019361972808838  and  1.4812407493591309  and  1.191866159439087\n",
      "total error:  3.6924688816070557\n",
      "training error:  1.016599178314209  and  1.5387153625488281  and  1.2539374828338623\n",
      "total error:  3.8092520236968994\n",
      "training error:  0.9859237670898438  and  1.4339845180511475  and  1.1941529512405396\n",
      "total error:  3.6140612363815308\n",
      "training error:  1.0287951231002808  and  1.4305529594421387  and  1.1824613809585571\n",
      "total error:  3.6418094635009766\n",
      "training error:  0.9920516014099121  and  1.4253873825073242  and  1.1480233669281006\n",
      "total error:  3.565462350845337\n",
      "training error:  1.0024147033691406  and  1.4897127151489258  and  1.1596813201904297\n",
      "total error:  3.651808738708496\n",
      "training error:  1.0161561965942383  and  1.4799165725708008  and  1.183261513710022\n",
      "total error:  3.679334282875061\n",
      "training error:  1.0291595458984375  and  1.4353435039520264  and  1.1887362003326416\n",
      "total error:  3.6532392501831055\n",
      "training error:  0.9912853837013245  and  1.445731282234192  and  1.2112891674041748\n",
      "total error:  3.648305833339691\n",
      "training error:  1.0199912786483765  and  1.4130051136016846  and  1.2358726263046265\n",
      "total error:  3.6688690185546875\n",
      "training error:  1.0163201093673706  and  1.4169871807098389  and  1.1818984746932983\n",
      "total error:  3.615205764770508\n",
      "training error:  0.9936634302139282  and  1.3927404880523682  and  1.1889731884002686\n",
      "total error:  3.575377106666565\n",
      "training error:  1.0093375444412231  and  1.4226975440979004  and  1.1549805402755737\n",
      "total error:  3.5870156288146973\n",
      "training error:  1.0046021938323975  and  1.4700485467910767  and  1.1527044773101807\n",
      "total error:  3.627355217933655\n",
      "training error:  0.9941407442092896  and  1.6397595405578613  and  1.2199091911315918\n",
      "total error:  3.8538094758987427\n",
      "training error:  0.9733016490936279  and  1.4506375789642334  and  1.186128854751587\n",
      "total error:  3.6100680828094482\n",
      "training error:  0.9857068061828613  and  1.517043113708496  and  1.2575719356536865\n",
      "total error:  3.760321855545044\n",
      "training error:  1.0226703882217407  and  1.5692787170410156  and  1.1908284425735474\n",
      "total error:  3.7827775478363037\n",
      "training error:  0.9687917232513428  and  1.4454774856567383  and  1.1667755842208862\n",
      "total error:  3.5810447931289673\n",
      "training error:  0.994725227355957  and  1.5405867099761963  and  1.1930830478668213\n",
      "total error:  3.7283949851989746\n",
      "training error:  0.9659359455108643  and  1.4825866222381592  and  1.1536335945129395\n",
      "total error:  3.602156162261963\n",
      "training error:  0.9979662299156189  and  1.5903022289276123  and  1.2411657571792603\n",
      "total error:  3.8294342160224915\n",
      "training error:  0.9847422242164612  and  1.5417652130126953  and  1.2037349939346313\n",
      "total error:  3.730242431163788\n",
      "training error:  0.9776249527931213  and  1.4837504625320435  and  1.2269885540008545\n",
      "total error:  3.6883639693260193\n",
      "training error:  0.9898185133934021  and  1.3991705179214478  and  1.1792547702789307\n",
      "total error:  3.5682438015937805\n",
      "training error:  0.9852447509765625  and  1.4438093900680542  and  1.1780786514282227\n",
      "total error:  3.6071327924728394\n",
      "training error:  1.0025666952133179  and  1.3817145824432373  and  1.153078317642212\n",
      "total error:  3.537359595298767\n",
      "training error:  0.9749403595924377  and  1.7389166355133057  and  1.1625566482543945\n",
      "total error:  3.876413643360138\n",
      "training error:  1.0103647708892822  and  1.4291496276855469  and  1.1781803369522095\n",
      "total error:  3.6176947355270386\n",
      "training error:  0.9672743678092957  and  1.428814172744751  and  1.169245958328247\n",
      "total error:  3.5653344988822937\n",
      "training error:  0.9963797330856323  and  1.4157108068466187  and  1.1924550533294678\n",
      "total error:  3.6045455932617188\n",
      "training error:  0.9876576066017151  and  1.4337270259857178  and  1.3155567646026611\n",
      "total error:  3.736941397190094\n",
      "training error:  0.9819135665893555  and  1.4237014055252075  and  1.1562683582305908\n",
      "total error:  3.561883330345154\n",
      "training error:  0.9910906553268433  and  1.4418909549713135  and  1.212268352508545\n",
      "total error:  3.6452499628067017\n",
      "training error:  1.020897626876831  and  1.520003080368042  and  1.1461021900177002\n",
      "total error:  3.6870028972625732\n",
      "training error:  0.9991552233695984  and  1.5284709930419922  and  1.1930443048477173\n",
      "total error:  3.720670521259308\n",
      "training error:  0.9940526485443115  and  1.7555104494094849  and  1.2445099353790283\n",
      "total error:  3.9940730333328247\n",
      "training error:  0.9779791831970215  and  1.4669866561889648  and  1.167264699935913\n",
      "total error:  3.6122305393218994\n",
      "training error:  1.0069010257720947  and  1.6833693981170654  and  1.3123222589492798\n",
      "total error:  4.00259268283844\n",
      "training error:  0.9763286709785461  and  1.6965669393539429  and  1.2384593486785889\n",
      "total error:  3.911354959011078\n",
      "training error:  0.9832894206047058  and  1.4330941438674927  and  1.1616039276123047\n",
      "total error:  3.577987492084503\n",
      "training error:  0.9759539365768433  and  1.4067198038101196  and  1.1673071384429932\n",
      "total error:  3.549980878829956\n",
      "training error:  0.96757572889328  and  1.4903863668441772  and  1.1976943016052246\n",
      "total error:  3.655656397342682\n",
      "training error:  0.979371964931488  and  1.4106416702270508  and  1.1759281158447266\n",
      "total error:  3.5659417510032654\n",
      "training error:  0.9825543165206909  and  1.4247490167617798  and  1.1742024421691895\n",
      "total error:  3.58150577545166\n",
      "training error:  0.9771618247032166  and  1.5030921697616577  and  1.1785483360290527\n",
      "total error:  3.658802330493927\n",
      "training error:  0.9750427007675171  and  1.4323796033859253  and  1.189652681350708\n",
      "total error:  3.5970749855041504\n",
      "training error:  0.9997352361679077  and  1.4206252098083496  and  1.179008960723877\n",
      "total error:  3.5993694067001343\n",
      "training error:  0.9990952014923096  and  1.4762442111968994  and  1.1624785661697388\n",
      "total error:  3.6378179788589478\n",
      "training error:  0.970809817314148  and  1.4535558223724365  and  1.1715192794799805\n",
      "total error:  3.595884919166565\n",
      "training error:  0.9770676493644714  and  1.4190747737884521  and  1.1548168659210205\n",
      "total error:  3.550959289073944\n",
      "training error:  0.9880236983299255  and  1.3998433351516724  and  1.1414105892181396\n",
      "total error:  3.5292776226997375\n",
      "training error:  0.9580451250076294  and  1.4355895519256592  and  1.1882983446121216\n",
      "total error:  3.58193302154541\n",
      "training error:  0.9935510754585266  and  1.5095374584197998  and  1.175918459892273\n",
      "total error:  3.6790069937705994\n",
      "training error:  0.9835017919540405  and  1.4827221632003784  and  1.1489535570144653\n",
      "total error:  3.6151775121688843\n",
      "training error:  0.9716060161590576  and  1.4309473037719727  and  1.1857030391693115\n",
      "total error:  3.588256359100342\n",
      "training error:  0.9825964570045471  and  1.3943555355072021  and  1.1623554229736328\n",
      "total error:  3.539307415485382\n",
      "training error:  0.9814400672912598  and  1.4686287641525269  and  1.2058229446411133\n",
      "total error:  3.6558917760849\n",
      "training error:  0.989867627620697  and  1.4029780626296997  and  1.1912903785705566\n",
      "total error:  3.5841360688209534\n",
      "training error:  0.9693251848220825  and  1.4411157369613647  and  1.1718595027923584\n",
      "total error:  3.5823004245758057\n",
      "training error:  0.963577926158905  and  1.4169909954071045  and  1.2124152183532715\n",
      "total error:  3.592984139919281\n",
      "training error:  0.9700616598129272  and  1.5470006465911865  and  1.2371515035629272\n",
      "total error:  3.754213809967041\n",
      "training error:  0.9843394160270691  and  1.5030648708343506  and  1.1723052263259888\n",
      "total error:  3.6597095131874084\n",
      "training error:  0.9782560467720032  and  1.4162523746490479  and  1.1849614381790161\n",
      "total error:  3.579469859600067\n",
      "training error:  0.9735078811645508  and  1.4041869640350342  and  1.1556007862091064\n",
      "total error:  3.5332956314086914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.9589096903800964  and  1.4166338443756104  and  1.1772722005844116\n",
      "total error:  3.5528157353401184\n",
      "training error:  0.9674456119537354  and  1.4316189289093018  and  1.1514384746551514\n",
      "total error:  3.5505030155181885\n",
      "training error:  0.9869232177734375  and  1.4312430620193481  and  1.1386833190917969\n",
      "total error:  3.5568495988845825\n",
      "training error:  0.9816244840621948  and  1.3975629806518555  and  1.2302182912826538\n",
      "total error:  3.609405755996704\n",
      "training error:  0.9758148193359375  and  1.4352954626083374  and  1.1779088973999023\n",
      "total error:  3.5890191793441772\n",
      "training error:  0.9913684129714966  and  1.4439455270767212  and  1.2276835441589355\n",
      "total error:  3.6629974842071533\n",
      "training error:  0.9576026201248169  and  1.5395710468292236  and  1.1876964569091797\n",
      "total error:  3.68487012386322\n",
      "training error:  0.963894248008728  and  1.5276610851287842  and  1.1850167512893677\n",
      "total error:  3.67657208442688\n",
      "training error:  1.0142972469329834  and  1.576329231262207  and  1.210472583770752\n",
      "total error:  3.8010990619659424\n",
      "training error:  0.96718430519104  and  1.495105504989624  and  1.1850477457046509\n",
      "total error:  3.647337555885315\n",
      "training error:  0.967197597026825  and  1.4479049444198608  and  1.1809592247009277\n",
      "total error:  3.5960617661476135\n",
      "training error:  0.9425207376480103  and  1.3847209215164185  and  1.143257737159729\n",
      "total error:  3.4704993963241577\n",
      "training error:  0.9660131931304932  and  1.4065725803375244  and  1.1614489555358887\n",
      "total error:  3.5340347290039062\n",
      "training error:  0.9699621200561523  and  1.5075054168701172  and  1.1751017570495605\n",
      "total error:  3.65256929397583\n",
      "training error:  0.9772534370422363  and  1.4473061561584473  and  1.1812729835510254\n",
      "total error:  3.605832576751709\n",
      "training error:  0.977739691734314  and  1.4547111988067627  and  1.165367841720581\n",
      "total error:  3.5978187322616577\n",
      "training error:  0.9617083668708801  and  1.409897804260254  and  1.1457595825195312\n",
      "total error:  3.5173657536506653\n",
      "training error:  0.976969838142395  and  1.4257405996322632  and  1.1975396871566772\n",
      "total error:  3.6002501249313354\n",
      "training error:  1.0063552856445312  and  1.3924342393875122  and  1.1631078720092773\n",
      "total error:  3.561897397041321\n",
      "training error:  0.9743359088897705  and  1.4216630458831787  and  1.1850976943969727\n",
      "total error:  3.581096649169922\n",
      "training error:  1.002297043800354  and  1.4189205169677734  and  1.1610095500946045\n",
      "total error:  3.582227110862732\n",
      "training error:  0.9560762643814087  and  1.5093507766723633  and  1.2088119983673096\n",
      "total error:  3.6742390394210815\n",
      "training error:  0.9741562604904175  and  1.4471430778503418  and  1.172746181488037\n",
      "total error:  3.5940455198287964\n",
      "training error:  0.9605612754821777  and  1.3929111957550049  and  1.2325007915496826\n",
      "total error:  3.5859732627868652\n",
      "training error:  0.9671599268913269  and  1.831390142440796  and  1.20099675655365\n",
      "total error:  3.9995468258857727\n",
      "training error:  0.9840410947799683  and  1.625257968902588  and  1.1944546699523926\n",
      "total error:  3.8037537336349487\n",
      "training error:  0.973099946975708  and  1.4118932485580444  and  1.1918914318084717\n",
      "total error:  3.576884627342224\n",
      "training error:  0.9313530325889587  and  1.3909832239151  and  1.139695644378662\n",
      "total error:  3.462031900882721\n",
      "training error:  0.9657328128814697  and  1.4085392951965332  and  1.1600196361541748\n",
      "total error:  3.5342917442321777\n",
      "training error:  0.9779353141784668  and  1.4414901733398438  and  1.172945261001587\n",
      "total error:  3.5923707485198975\n",
      "training error:  0.9717414379119873  and  1.4138084650039673  and  1.1413488388061523\n",
      "total error:  3.526898741722107\n",
      "training error:  0.9689608812332153  and  1.4016520977020264  and  1.1297274827957153\n",
      "total error:  3.500340461730957\n",
      "training error:  0.9666523933410645  and  1.4639745950698853  and  1.1608633995056152\n",
      "total error:  3.591490387916565\n",
      "training error:  0.9610233306884766  and  1.4800132513046265  and  1.1749368906021118\n",
      "total error:  3.615973472595215\n",
      "training error:  0.9442382454872131  and  1.3935482501983643  and  1.153770923614502\n",
      "total error:  3.4915574193000793\n",
      "training error:  0.9709608554840088  and  1.3805360794067383  and  1.1707367897033691\n",
      "total error:  3.522233724594116\n",
      "training error:  0.9429919123649597  and  1.442360758781433  and  1.1983568668365479\n",
      "total error:  3.5837095379829407\n",
      "training error:  1.0091171264648438  and  1.4281690120697021  and  1.193486213684082\n",
      "total error:  3.630772352218628\n",
      "training error:  0.9723361730575562  and  1.4349756240844727  and  1.1930463314056396\n",
      "total error:  3.6003581285476685\n",
      "training error:  0.9453822374343872  and  1.6694003343582153  and  1.2481374740600586\n",
      "total error:  3.862920045852661\n",
      "training error:  0.9647839665412903  and  1.4209997653961182  and  1.1962666511535645\n",
      "total error:  3.582050383090973\n",
      "training error:  0.968092679977417  and  1.3791608810424805  and  1.2025642395019531\n",
      "total error:  3.5498178005218506\n",
      "training error:  0.9409488439559937  and  1.421865701675415  and  1.1963499784469604\n",
      "total error:  3.559164524078369\n",
      "training error:  0.953586220741272  and  1.4564871788024902  and  1.1951448917388916\n",
      "total error:  3.605218291282654\n",
      "training error:  0.9403333067893982  and  1.4178961515426636  and  1.121350646018982\n",
      "total error:  3.4795801043510437\n",
      "training error:  0.9829099178314209  and  1.6859904527664185  and  1.2035365104675293\n",
      "total error:  3.8724368810653687\n",
      "training error:  0.946038007736206  and  1.3713096380233765  and  1.1763628721237183\n",
      "total error:  3.493710517883301\n",
      "training error:  0.9397019147872925  and  1.3628175258636475  and  1.130039095878601\n",
      "total error:  3.432558536529541\n",
      "training error:  0.9489767551422119  and  1.3828675746917725  and  1.1391292810440063\n",
      "total error:  3.4709736108779907\n",
      "training error:  0.9513499140739441  and  1.493969202041626  and  1.15397310256958\n",
      "total error:  3.59929221868515\n",
      "training error:  0.9499533772468567  and  1.6697282791137695  and  1.361210823059082\n",
      "total error:  3.9808924794197083\n",
      "training error:  0.9507812261581421  and  1.346327304840088  and  1.181018352508545\n",
      "total error:  3.478126883506775\n",
      "training error:  0.9791625738143921  and  1.406113624572754  and  1.137599229812622\n",
      "total error:  3.522875428199768\n",
      "training error:  0.9506410956382751  and  1.4163119792938232  and  1.255525827407837\n",
      "total error:  3.6224789023399353\n",
      "training error:  0.9420356750488281  and  1.3898828029632568  and  1.1877983808517456\n",
      "total error:  3.5197168588638306\n",
      "training error:  0.9605919122695923  and  1.5937163829803467  and  1.1627777814865112\n",
      "total error:  3.71708607673645\n",
      "training error:  0.9387834072113037  and  1.414966344833374  and  1.2568351030349731\n",
      "total error:  3.610584855079651\n",
      "training error:  0.9551987648010254  and  1.4318759441375732  and  1.124509572982788\n",
      "total error:  3.5115842819213867\n",
      "training error:  0.9478745460510254  and  1.4212565422058105  and  1.234593391418457\n",
      "total error:  3.603724479675293\n",
      "training error:  0.9441382884979248  and  1.3972355127334595  and  1.186159610748291\n",
      "total error:  3.5275334119796753\n",
      "training error:  0.943250834941864  and  1.3665746450424194  and  1.2176661491394043\n",
      "total error:  3.5274916291236877\n",
      "training error:  0.9430888891220093  and  1.3680297136306763  and  1.113241195678711\n",
      "total error:  3.4243597984313965\n",
      "training error:  0.956865668296814  and  1.3642101287841797  and  1.1340627670288086\n",
      "total error:  3.4551385641098022\n",
      "training error:  0.9422665238380432  and  1.3930668830871582  and  1.1635451316833496\n",
      "total error:  3.498878538608551\n",
      "training error:  0.9447928667068481  and  1.5454318523406982  and  1.1540639400482178\n",
      "total error:  3.644288659095764\n",
      "training error:  0.9740163683891296  and  1.5417895317077637  and  1.1448673009872437\n",
      "total error:  3.660673201084137\n",
      "training error:  0.9443714618682861  and  1.410735845565796  and  1.137317180633545\n",
      "total error:  3.492424488067627\n",
      "training error:  0.9535796642303467  and  1.406064748764038  and  1.1742088794708252\n",
      "total error:  3.53385329246521\n",
      "training error:  0.9594319462776184  and  1.3676834106445312  and  1.116811752319336\n",
      "total error:  3.4439271092414856\n",
      "training error:  0.9325606822967529  and  1.3484041690826416  and  1.1184253692626953\n",
      "total error:  3.39939022064209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.9668834209442139  and  1.4016822576522827  and  1.1753065586090088\n",
      "total error:  3.5438722372055054\n",
      "training error:  0.9846470355987549  and  1.5263291597366333  and  1.1425807476043701\n",
      "total error:  3.6535569429397583\n",
      "training error:  0.9453944563865662  and  1.4156566858291626  and  1.1049774885177612\n",
      "total error:  3.46602863073349\n",
      "training error:  0.9497134685516357  and  1.3718472719192505  and  1.1910600662231445\n",
      "total error:  3.5126208066940308\n",
      "training error:  0.9278929233551025  and  1.3996753692626953  and  1.1936371326446533\n",
      "total error:  3.521205425262451\n",
      "training error:  0.9538551568984985  and  1.4407165050506592  and  1.117540717124939\n",
      "total error:  3.5121123790740967\n",
      "training error:  0.9637132287025452  and  1.645376205444336  and  1.1971129179000854\n",
      "total error:  3.8062023520469666\n",
      "training error:  0.9519397020339966  and  1.4571940898895264  and  1.1587458848953247\n",
      "total error:  3.5678796768188477\n",
      "training error:  0.9455035328865051  and  1.4896491765975952  and  1.1698579788208008\n",
      "total error:  3.605010688304901\n",
      "training error:  0.9756699204444885  and  1.350982427597046  and  1.1421421766281128\n",
      "total error:  3.468794524669647\n",
      "training error:  0.9576938152313232  and  1.3752391338348389  and  1.191670298576355\n",
      "total error:  3.524603247642517\n",
      "training error:  0.9248673319816589  and  1.4301095008850098  and  1.1539416313171387\n",
      "total error:  3.5089184641838074\n",
      "training error:  0.9771783351898193  and  1.433917760848999  and  1.2143237590789795\n",
      "total error:  3.625419855117798\n",
      "training error:  0.9470503926277161  and  1.4383633136749268  and  1.2459845542907715\n",
      "total error:  3.6313982605934143\n",
      "training error:  0.9352750182151794  and  1.4630595445632935  and  1.2019245624542236\n",
      "total error:  3.6002591252326965\n",
      "training error:  0.9518991708755493  and  1.428280234336853  and  1.1543874740600586\n",
      "total error:  3.534566879272461\n",
      "training error:  0.9513259530067444  and  1.4899877309799194  and  1.1449015140533447\n",
      "total error:  3.5862151980400085\n",
      "training error:  0.945907711982727  and  1.4044744968414307  and  1.1531939506530762\n",
      "total error:  3.503576159477234\n",
      "training error:  0.9539600610733032  and  1.4022811651229858  and  1.1540460586547852\n",
      "total error:  3.510287284851074\n",
      "training error:  0.9393953084945679  and  1.3837212324142456  and  1.1437351703643799\n",
      "total error:  3.4668517112731934\n",
      "training error:  0.9417424201965332  and  1.3652225732803345  and  1.110992431640625\n",
      "total error:  3.4179574251174927\n",
      "training error:  0.9525083303451538  and  1.4397825002670288  and  1.1468652486801147\n",
      "total error:  3.5391560792922974\n",
      "training error:  0.9341551065444946  and  1.45615816116333  and  1.1432385444641113\n",
      "total error:  3.533551812171936\n",
      "training error:  0.9321038126945496  and  1.3515278100967407  and  1.1671900749206543\n",
      "total error:  3.4508216977119446\n",
      "training error:  0.9583189487457275  and  1.3892858028411865  and  1.1257195472717285\n",
      "total error:  3.4733242988586426\n",
      "training error:  0.9395194053649902  and  1.5203874111175537  and  1.241973638534546\n",
      "total error:  3.70188045501709\n",
      "training error:  0.9458573460578918  and  1.540524959564209  and  1.1903631687164307\n",
      "total error:  3.6767454743385315\n",
      "training error:  0.9222017526626587  and  1.4506604671478271  and  1.122157096862793\n",
      "total error:  3.495019316673279\n",
      "training error:  0.9449855089187622  and  1.3972638845443726  and  1.2592835426330566\n",
      "total error:  3.6015329360961914\n",
      "training error:  0.936316728591919  and  1.3290281295776367  and  1.112569808959961\n",
      "total error:  3.3779146671295166\n",
      "training error:  0.9679678678512573  and  1.760740041732788  and  1.3073861598968506\n",
      "total error:  4.036094069480896\n",
      "training error:  0.9169732928276062  and  1.3480151891708374  and  1.1453943252563477\n",
      "total error:  3.4103828072547913\n",
      "training error:  0.9348763823509216  and  1.3636746406555176  and  1.1357433795928955\n",
      "total error:  3.4342944025993347\n",
      "training error:  0.9344439506530762  and  1.3908228874206543  and  1.1398448944091797\n",
      "total error:  3.46511173248291\n",
      "training error:  0.9603680372238159  and  1.387507438659668  and  1.1653965711593628\n",
      "total error:  3.5132720470428467\n",
      "training error:  0.9268300533294678  and  1.392918348312378  and  1.1220630407333374\n",
      "total error:  3.441811442375183\n",
      "training error:  0.9577343463897705  and  1.3983819484710693  and  1.1923593282699585\n",
      "total error:  3.5484756231307983\n",
      "training error:  0.9284717440605164  and  1.3584144115447998  and  1.1506378650665283\n",
      "total error:  3.4375240206718445\n",
      "training error:  0.9290481805801392  and  1.5296940803527832  and  1.166957139968872\n",
      "total error:  3.6256994009017944\n",
      "training error:  0.9503395557403564  and  1.4552632570266724  and  1.1909033060073853\n",
      "total error:  3.596506118774414\n",
      "training error:  0.9458439350128174  and  1.5556180477142334  and  1.2757148742675781\n",
      "total error:  3.777176856994629\n",
      "training error:  0.928115963935852  and  1.393038034439087  and  1.1239486932754517\n",
      "total error:  3.4451026916503906\n",
      "training error:  0.9155348539352417  and  1.4101786613464355  and  1.100116491317749\n",
      "total error:  3.4258300065994263\n",
      "training error:  0.9255532026290894  and  1.3709560632705688  and  1.1603343486785889\n",
      "total error:  3.456843614578247\n",
      "training error:  0.9341453313827515  and  1.4190136194229126  and  1.1614611148834229\n",
      "total error:  3.514620065689087\n",
      "training error:  0.916282594203949  and  1.3566803932189941  and  1.189659595489502\n",
      "total error:  3.462622582912445\n",
      "training error:  0.9344754219055176  and  1.3591350317001343  and  1.1332545280456543\n",
      "total error:  3.426864981651306\n",
      "training error:  0.9447557926177979  and  1.4015707969665527  and  1.1780381202697754\n",
      "total error:  3.524364709854126\n",
      "training error:  0.9173018336296082  and  1.359066367149353  and  1.1253279447555542\n",
      "total error:  3.4016961455345154\n",
      "training error:  0.910231351852417  and  1.5093142986297607  and  1.1408394575119019\n",
      "total error:  3.5603851079940796\n",
      "training error:  0.9178255200386047  and  1.4447541236877441  and  1.1172332763671875\n",
      "total error:  3.4798129200935364\n",
      "training error:  0.91985684633255  and  1.3647546768188477  and  1.1723496913909912\n",
      "total error:  3.456961214542389\n",
      "training error:  0.9005422592163086  and  1.334635853767395  and  1.114145278930664\n",
      "total error:  3.3493233919143677\n",
      "training error:  0.9460359215736389  and  1.4355287551879883  and  1.125220775604248\n",
      "total error:  3.5067854523658752\n",
      "training error:  0.9342765808105469  and  1.4045056104660034  and  1.1728930473327637\n",
      "total error:  3.511675238609314\n",
      "training error:  0.9247941374778748  and  1.5001263618469238  and  1.1422722339630127\n",
      "total error:  3.5671927332878113\n",
      "training error:  0.9425752758979797  and  1.6758815050125122  and  1.1772047281265259\n",
      "total error:  3.795661509037018\n",
      "training error:  0.9106072187423706  and  1.3819596767425537  and  1.1229881048202515\n",
      "total error:  3.415555000305176\n",
      "training error:  0.9218165874481201  and  1.35956871509552  and  1.12272047996521\n",
      "total error:  3.40410578250885\n",
      "training error:  0.8964089751243591  and  1.3743702173233032  and  1.149537205696106\n",
      "total error:  3.4203163981437683\n",
      "training error:  0.8946617841720581  and  1.3664549589157104  and  1.1310107707977295\n",
      "total error:  3.392127513885498\n",
      "training error:  0.9187110066413879  and  1.3611834049224854  and  1.1294206380844116\n",
      "total error:  3.409315049648285\n",
      "training error:  0.9106731414794922  and  1.6053712368011475  and  1.2197458744049072\n",
      "total error:  3.735790252685547\n",
      "training error:  0.921431839466095  and  1.4758126735687256  and  1.1996855735778809\n",
      "total error:  3.5969300866127014\n",
      "training error:  0.9133861660957336  and  1.3183778524398804  and  1.0882532596588135\n",
      "total error:  3.3200172781944275\n",
      "training error:  0.902491569519043  and  1.3677279949188232  and  1.1018342971801758\n",
      "total error:  3.372053861618042\n",
      "training error:  0.9096968173980713  and  1.3539886474609375  and  1.1362559795379639\n",
      "total error:  3.3999414443969727\n",
      "training error:  0.890902042388916  and  1.4834160804748535  and  1.1371493339538574\n",
      "total error:  3.511467456817627\n",
      "training error:  0.9442710280418396  and  1.5434060096740723  and  1.1795389652252197\n",
      "total error:  3.6672160029411316\n",
      "training error:  0.9086710214614868  and  1.394014596939087  and  1.092810034751892\n",
      "total error:  3.395495653152466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.9029238224029541  and  1.5822498798370361  and  1.167367696762085\n",
      "total error:  3.652541399002075\n",
      "training error:  0.9144394397735596  and  1.4793682098388672  and  1.18202543258667\n",
      "total error:  3.5758330821990967\n",
      "training error:  0.9162555932998657  and  1.3381714820861816  and  1.1455118656158447\n",
      "total error:  3.399938941001892\n",
      "training error:  0.9210065603256226  and  1.4602292776107788  and  1.1437582969665527\n",
      "total error:  3.524994134902954\n",
      "training error:  0.9142799377441406  and  1.3954466581344604  and  1.2142126560211182\n",
      "total error:  3.5239392518997192\n",
      "training error:  0.9186516404151917  and  1.5287641286849976  and  1.4116857051849365\n",
      "total error:  3.8591014742851257\n",
      "training error:  0.9084105491638184  and  1.5992493629455566  and  1.1824543476104736\n",
      "total error:  3.6901142597198486\n",
      "training error:  0.9036098718643188  and  1.5086698532104492  and  1.1216332912445068\n",
      "total error:  3.533913016319275\n",
      "training error:  0.9177913069725037  and  1.5244706869125366  and  1.1806080341339111\n",
      "total error:  3.6228700280189514\n",
      "training error:  0.9001167416572571  and  1.3314380645751953  and  1.0993199348449707\n",
      "total error:  3.330874741077423\n",
      "training error:  0.9012601375579834  and  1.366685152053833  and  1.1962225437164307\n",
      "total error:  3.464167833328247\n",
      "training error:  0.9163077473640442  and  1.3840768337249756  and  1.135798692703247\n",
      "total error:  3.436183273792267\n",
      "training error:  0.9200289845466614  and  1.3613817691802979  and  1.1155686378479004\n",
      "total error:  3.3969793915748596\n",
      "training error:  0.8980206847190857  and  1.465261459350586  and  1.17941415309906\n",
      "total error:  3.5426962971687317\n",
      "training error:  0.9059194922447205  and  1.334938406944275  and  1.1392457485198975\n",
      "total error:  3.380103647708893\n",
      "training error:  0.9223569631576538  and  1.3363230228424072  and  1.12754225730896\n",
      "total error:  3.386222243309021\n",
      "training error:  0.927787184715271  and  1.340930461883545  and  1.1318341493606567\n",
      "total error:  3.4005517959594727\n",
      "training error:  0.9021879434585571  and  1.41038179397583  and  1.1221462488174438\n",
      "total error:  3.434715986251831\n",
      "training error:  0.9203853607177734  and  1.4229613542556763  and  1.118145227432251\n",
      "total error:  3.4614919424057007\n",
      "training error:  0.9021854400634766  and  1.3641719818115234  and  1.1852760314941406\n",
      "total error:  3.4516334533691406\n",
      "training error:  0.9101083874702454  and  1.3310277462005615  and  1.095927119255066\n",
      "total error:  3.337063252925873\n",
      "training error:  0.9154880046844482  and  1.3994905948638916  and  1.126198649406433\n",
      "total error:  3.441177248954773\n",
      "training error:  0.897818922996521  and  1.3196890354156494  and  1.1361100673675537\n",
      "total error:  3.353618025779724\n",
      "training error:  0.8986539840698242  and  1.3728199005126953  and  1.1525360345840454\n",
      "total error:  3.424009919166565\n",
      "training error:  0.9072533845901489  and  1.9398303031921387  and  1.1417224407196045\n",
      "total error:  3.988806128501892\n",
      "training error:  0.9176929593086243  and  1.4070907831192017  and  1.1931428909301758\n",
      "total error:  3.5179266333580017\n",
      "training error:  0.9173123240470886  and  1.412989616394043  and  1.1065747737884521\n",
      "total error:  3.4368767142295837\n",
      "training error:  0.9025667905807495  and  1.3418341875076294  and  1.0972471237182617\n",
      "total error:  3.3416481018066406\n",
      "training error:  0.9010918736457825  and  1.376852035522461  and  1.123485803604126\n",
      "total error:  3.4014297127723694\n",
      "training error:  0.8982201218605042  and  1.483104944229126  and  1.1868765354156494\n",
      "total error:  3.5682016015052795\n",
      "training error:  0.8855131268501282  and  1.4032570123672485  and  1.1378296613693237\n",
      "total error:  3.4265998005867004\n",
      "training error:  0.9128004908561707  and  1.370774507522583  and  1.1343587636947632\n",
      "total error:  3.417933762073517\n",
      "training error:  0.89886075258255  and  1.4356670379638672  and  1.1677995920181274\n",
      "total error:  3.5023273825645447\n",
      "training error:  0.8886057138442993  and  1.4634811878204346  and  1.1378319263458252\n",
      "total error:  3.489918828010559\n",
      "training error:  0.9302963018417358  and  1.6789014339447021  and  1.24628484249115\n",
      "total error:  3.855482578277588\n",
      "training error:  0.8998783230781555  and  1.3354341983795166  and  1.1066462993621826\n",
      "total error:  3.3419588208198547\n",
      "training error:  0.932239294052124  and  1.411872386932373  and  1.1150333881378174\n",
      "total error:  3.4591450691223145\n",
      "training error:  0.9066227674484253  and  1.3740429878234863  and  1.1530909538269043\n",
      "total error:  3.433756709098816\n",
      "training error:  0.8915746212005615  and  1.3123078346252441  and  1.1081395149230957\n",
      "total error:  3.3120219707489014\n",
      "training error:  0.8987467885017395  and  1.3548774719238281  and  1.1225193738937378\n",
      "total error:  3.3761436343193054\n",
      "training error:  0.8906489014625549  and  1.3182740211486816  and  1.149337649345398\n",
      "total error:  3.3582605719566345\n",
      "training error:  0.9134265184402466  and  1.394587755203247  and  1.1246839761734009\n",
      "total error:  3.4326982498168945\n",
      "training error:  0.8964324593544006  and  1.3384287357330322  and  1.1083351373672485\n",
      "total error:  3.3431963324546814\n",
      "training error:  0.9033017158508301  and  1.3526387214660645  and  1.1419308185577393\n",
      "total error:  3.397871255874634\n",
      "training error:  0.883150577545166  and  1.3828953504562378  and  1.0733911991119385\n",
      "total error:  3.3394371271133423\n",
      "training error:  0.9082543849945068  and  1.3815889358520508  and  1.1479682922363281\n",
      "total error:  3.4378116130828857\n",
      "training error:  0.8946914076805115  and  1.380553960800171  and  1.1181848049163818\n",
      "total error:  3.393430173397064\n",
      "training error:  0.8889271020889282  and  1.3223767280578613  and  1.1493480205535889\n",
      "total error:  3.3606518507003784\n",
      "training error:  0.8950917720794678  and  1.317855954170227  and  1.1496539115905762\n",
      "total error:  3.362601637840271\n",
      "training error:  0.9029837250709534  and  1.3521578311920166  and  1.0825257301330566\n",
      "total error:  3.3376672863960266\n",
      "training error:  0.9140864014625549  and  1.4460861682891846  and  1.1571675539016724\n",
      "total error:  3.517340123653412\n",
      "training error:  0.8986823558807373  and  1.331230878829956  and  1.1014444828033447\n",
      "total error:  3.331357717514038\n",
      "training error:  0.8957521915435791  and  1.3175855875015259  and  1.0848461389541626\n",
      "total error:  3.2981839179992676\n",
      "training error:  0.9134466052055359  and  1.497057557106018  and  1.2029855251312256\n",
      "total error:  3.6134896874427795\n",
      "training error:  0.8837225437164307  and  1.396714210510254  and  1.1109423637390137\n",
      "total error:  3.3913791179656982\n",
      "training error:  0.9174082279205322  and  1.604668378829956  and  1.1487799882888794\n",
      "total error:  3.6708565950393677\n",
      "training error:  0.8838551044464111  and  1.3359227180480957  and  1.1275110244750977\n",
      "total error:  3.3472888469696045\n",
      "training error:  0.8987741470336914  and  1.3379138708114624  and  1.1365435123443604\n",
      "total error:  3.373231530189514\n",
      "training error:  0.9106841087341309  and  1.327309250831604  and  1.1073639392852783\n",
      "total error:  3.345357298851013\n",
      "training error:  0.869041919708252  and  1.4876295328140259  and  1.1392300128936768\n",
      "total error:  3.4959014654159546\n",
      "training error:  0.9048698544502258  and  1.3763821125030518  and  1.0971354246139526\n",
      "total error:  3.3783873915672302\n",
      "training error:  0.8970992565155029  and  1.3243050575256348  and  1.1717195510864258\n",
      "total error:  3.3931238651275635\n",
      "training error:  0.9040447473526001  and  1.3239320516586304  and  1.1026861667633057\n",
      "total error:  3.330662965774536\n",
      "training error:  0.8816728591918945  and  1.3744292259216309  and  1.13835608959198\n",
      "total error:  3.3944581747055054\n",
      "training error:  0.8805921673774719  and  1.3572380542755127  and  1.0939369201660156\n",
      "total error:  3.3317671418190002\n",
      "training error:  0.9010682106018066  and  1.3425828218460083  and  1.1137783527374268\n",
      "total error:  3.3574293851852417\n",
      "training error:  0.882827639579773  and  1.363632321357727  and  1.1352708339691162\n",
      "total error:  3.381730794906616\n",
      "training error:  0.8843349814414978  and  1.3386415243148804  and  1.1083418130874634\n",
      "total error:  3.3313183188438416\n",
      "training error:  0.9017214775085449  and  1.4351038932800293  and  1.1406357288360596\n",
      "total error:  3.477461099624634\n",
      "training error:  0.89224773645401  and  1.3439656496047974  and  1.0981719493865967\n",
      "total error:  3.334385335445404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.909878134727478  and  1.521561861038208  and  1.1022142171859741\n",
      "total error:  3.53365421295166\n",
      "training error:  0.899621844291687  and  1.5044662952423096  and  1.134099006652832\n",
      "total error:  3.5381871461868286\n",
      "training error:  0.8764908313751221  and  1.2975294589996338  and  1.1571788787841797\n",
      "total error:  3.3311991691589355\n",
      "training error:  0.8895023465156555  and  1.3837974071502686  and  1.1069821119308472\n",
      "total error:  3.3802818655967712\n",
      "training error:  0.8854333162307739  and  1.3541781902313232  and  1.0785034894943237\n",
      "total error:  3.318114995956421\n",
      "training error:  0.879891037940979  and  1.2989654541015625  and  1.1173169612884521\n",
      "total error:  3.2961734533309937\n",
      "training error:  0.8993346691131592  and  1.4476220607757568  and  1.1306941509246826\n",
      "total error:  3.4776508808135986\n",
      "training error:  0.8838123679161072  and  1.3831844329833984  and  1.0870845317840576\n",
      "total error:  3.3540813326835632\n",
      "training error:  0.8771039247512817  and  1.371590495109558  and  1.0978732109069824\n",
      "total error:  3.3465676307678223\n",
      "training error:  0.8724153637886047  and  1.3275275230407715  and  1.0899641513824463\n",
      "total error:  3.2899070382118225\n",
      "training error:  0.8960077166557312  and  1.3009300231933594  and  1.0783097743988037\n",
      "total error:  3.2752475142478943\n",
      "training error:  0.914116621017456  and  1.6809477806091309  and  1.1610616445541382\n",
      "total error:  3.756126046180725\n",
      "training error:  0.8808367252349854  and  1.2895185947418213  and  1.0883007049560547\n",
      "total error:  3.2586560249328613\n",
      "training error:  0.8600870370864868  and  1.4030007123947144  and  1.0866206884384155\n",
      "total error:  3.3497084379196167\n",
      "training error:  0.8723841905593872  and  1.3869688510894775  and  1.2083477973937988\n",
      "total error:  3.4677008390426636\n",
      "training error:  0.902510404586792  and  1.4230494499206543  and  1.1980371475219727\n",
      "total error:  3.523597002029419\n",
      "training error:  0.868942141532898  and  1.3744070529937744  and  1.1187348365783691\n",
      "total error:  3.3620840311050415\n",
      "training error:  0.8926643133163452  and  1.369605541229248  and  1.1837806701660156\n",
      "total error:  3.446050524711609\n",
      "training error:  0.8868119716644287  and  1.3397213220596313  and  1.160078763961792\n",
      "total error:  3.386612057685852\n",
      "training error:  0.8707065582275391  and  1.2925236225128174  and  1.0889029502868652\n",
      "total error:  3.2521331310272217\n",
      "training error:  0.891305685043335  and  1.295417070388794  and  1.0866533517837524\n",
      "total error:  3.2733761072158813\n",
      "training error:  0.8804676532745361  and  1.3072377443313599  and  1.1832144260406494\n",
      "total error:  3.3709198236465454\n",
      "training error:  0.8725687265396118  and  1.3718993663787842  and  1.1026802062988281\n",
      "total error:  3.347148299217224\n",
      "training error:  0.8898582458496094  and  1.3439292907714844  and  1.1230225563049316\n",
      "total error:  3.3568100929260254\n",
      "training error:  0.869631290435791  and  1.3477238416671753  and  1.0843470096588135\n",
      "total error:  3.30170214176178\n",
      "training error:  0.8936392068862915  and  1.4049625396728516  and  1.1989669799804688\n",
      "total error:  3.497568726539612\n",
      "training error:  0.8941290378570557  and  1.5282204151153564  and  1.139305591583252\n",
      "total error:  3.561655044555664\n",
      "training error:  0.8669852614402771  and  1.3418976068496704  and  1.1304991245269775\n",
      "total error:  3.339381992816925\n",
      "training error:  0.8995167016983032  and  1.3185186386108398  and  1.0612926483154297\n",
      "total error:  3.2793279886245728\n",
      "training error:  0.8970131874084473  and  1.499037504196167  and  1.1800628900527954\n",
      "total error:  3.5761135816574097\n",
      "training error:  0.8920258283615112  and  1.322624921798706  and  1.0822696685791016\n",
      "total error:  3.296920418739319\n",
      "training error:  0.9058762788772583  and  1.3562896251678467  and  1.213539719581604\n",
      "total error:  3.475705623626709\n",
      "training error:  0.8885544538497925  and  1.3982610702514648  and  1.188888430595398\n",
      "total error:  3.4757039546966553\n",
      "training error:  0.878890872001648  and  1.4265326261520386  and  1.1231461763381958\n",
      "total error:  3.4285696744918823\n",
      "training error:  0.8821377158164978  and  1.3589959144592285  and  1.0895380973815918\n",
      "total error:  3.330671727657318\n",
      "training error:  0.8676193952560425  and  1.3578319549560547  and  1.1630949974060059\n",
      "total error:  3.388546347618103\n",
      "training error:  0.8595669269561768  and  1.3804731369018555  and  1.211834192276001\n",
      "total error:  3.451874256134033\n",
      "training error:  0.8883823156356812  and  1.7712491750717163  and  1.2443745136260986\n",
      "total error:  3.904006004333496\n",
      "training error:  0.8553025126457214  and  1.342178225517273  and  1.1871607303619385\n",
      "total error:  3.384641468524933\n",
      "training error:  0.8696351051330566  and  1.3566007614135742  and  1.1161482334136963\n",
      "total error:  3.342384099960327\n",
      "training error:  0.8766699433326721  and  1.4020907878875732  and  1.1190640926361084\n",
      "total error:  3.3978248238563538\n",
      "training error:  0.8652965426445007  and  1.432359218597412  and  1.071690559387207\n",
      "total error:  3.36934632062912\n",
      "training error:  0.851432204246521  and  1.3904378414154053  and  1.1145179271697998\n",
      "total error:  3.356387972831726\n",
      "training error:  0.8749858140945435  and  1.2998557090759277  and  1.1336649656295776\n",
      "total error:  3.308506488800049\n",
      "training error:  0.8540675640106201  and  1.3012531995773315  and  1.0451748371124268\n",
      "total error:  3.2004956007003784\n",
      "training error:  0.8651431798934937  and  1.3125604391098022  and  1.1601169109344482\n",
      "total error:  3.337820529937744\n",
      "training error:  0.8691315650939941  and  1.313129186630249  and  1.0738526582717896\n",
      "total error:  3.2561134099960327\n",
      "training error:  0.8760759234428406  and  1.4235941171646118  and  1.080639362335205\n",
      "total error:  3.3803094029426575\n",
      "training error:  0.8821229338645935  and  1.3374340534210205  and  1.117738127708435\n",
      "total error:  3.337295114994049\n",
      "training error:  0.8648191094398499  and  1.3415615558624268  and  1.112790822982788\n",
      "total error:  3.3191714882850647\n",
      "training error:  0.8679774403572083  and  1.3802485466003418  and  1.0709211826324463\n",
      "total error:  3.3191471695899963\n",
      "training error:  0.8723543882369995  and  1.5173976421356201  and  1.1293692588806152\n",
      "total error:  3.519121289253235\n",
      "training error:  0.8503516912460327  and  1.2839115858078003  and  1.0968637466430664\n",
      "total error:  3.2311270236968994\n",
      "training error:  0.8472334146499634  and  1.2862143516540527  and  1.0923823118209839\n",
      "total error:  3.225830078125\n",
      "training error:  0.8805910348892212  and  1.4210652112960815  and  1.0884056091308594\n",
      "total error:  3.390061855316162\n",
      "training error:  0.9131934642791748  and  1.409055471420288  and  1.065286636352539\n",
      "total error:  3.387535572052002\n",
      "training error:  0.8636513948440552  and  1.3230946063995361  and  1.0787591934204102\n",
      "total error:  3.2655051946640015\n",
      "training error:  0.8726509213447571  and  1.2942357063293457  and  1.106323003768921\n",
      "total error:  3.2732096314430237\n",
      "training error:  0.8506711721420288  and  1.3123382329940796  and  1.1215484142303467\n",
      "total error:  3.284557819366455\n",
      "training error:  0.859946608543396  and  1.4044597148895264  and  1.0922266244888306\n",
      "total error:  3.356632947921753\n",
      "training error:  0.8656682372093201  and  1.363677740097046  and  1.1518199443817139\n",
      "total error:  3.38116592168808\n",
      "training error:  0.8563451170921326  and  1.3179852962493896  and  1.0920196771621704\n",
      "total error:  3.2663500905036926\n",
      "training error:  0.8522316217422485  and  1.324218511581421  and  1.1599643230438232\n",
      "total error:  3.3364144563674927\n",
      "training error:  0.8584014773368835  and  1.474471092224121  and  1.0666285753250122\n",
      "total error:  3.399501144886017\n",
      "training error:  0.8755050897598267  and  1.316371202468872  and  1.108795404434204\n",
      "total error:  3.300671696662903\n",
      "training error:  0.854706346988678  and  1.3686541318893433  and  1.0936925411224365\n",
      "total error:  3.3170530200004578\n",
      "training error:  0.8639712929725647  and  1.3335621356964111  and  1.0965988636016846\n",
      "total error:  3.2941322922706604\n",
      "training error:  0.8817859888076782  and  1.3445020914077759  and  1.0732142925262451\n",
      "total error:  3.299502372741699\n",
      "training error:  0.8726229667663574  and  1.3650809526443481  and  1.094689965248108\n",
      "total error:  3.3323938846588135\n",
      "training error:  0.8712721467018127  and  1.306524634361267  and  1.0712361335754395\n",
      "total error:  3.2490329146385193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.855925977230072  and  1.2947547435760498  and  1.099731206893921\n",
      "total error:  3.2504119277000427\n",
      "training error:  0.8585669994354248  and  1.3287866115570068  and  1.0876233577728271\n",
      "total error:  3.274976968765259\n",
      "training error:  0.8573178052902222  and  1.3453702926635742  and  1.1730711460113525\n",
      "total error:  3.375759243965149\n",
      "training error:  0.8658091425895691  and  1.6196496486663818  and  1.1077666282653809\n",
      "total error:  3.593225419521332\n",
      "training error:  0.8720710873603821  and  1.3185548782348633  and  1.122924566268921\n",
      "total error:  3.3135505318641663\n",
      "training error:  0.8911619782447815  and  1.3267661333084106  and  1.0904691219329834\n",
      "total error:  3.3083972334861755\n",
      "training error:  0.8947200179100037  and  1.431880235671997  and  1.2162885665893555\n",
      "total error:  3.542888820171356\n",
      "training error:  0.8526381254196167  and  1.2992026805877686  and  1.0858688354492188\n",
      "total error:  3.237709641456604\n",
      "training error:  0.8456621766090393  and  1.325099229812622  and  1.0627915859222412\n",
      "total error:  3.2335529923439026\n",
      "training error:  0.8569285869598389  and  1.3517045974731445  and  1.0859156847000122\n",
      "total error:  3.2945488691329956\n",
      "training error:  0.8729179501533508  and  1.37953519821167  and  1.078822374343872\n",
      "total error:  3.331275522708893\n",
      "training error:  0.8869607448577881  and  1.3739327192306519  and  1.1659064292907715\n",
      "total error:  3.4267998933792114\n",
      "training error:  0.8647701740264893  and  1.3278346061706543  and  1.1726958751678467\n",
      "total error:  3.3653006553649902\n",
      "training error:  0.8366953134536743  and  1.289697289466858  and  1.0692980289459229\n",
      "total error:  3.195690631866455\n",
      "training error:  0.8531830906867981  and  1.324471116065979  and  1.1310005187988281\n",
      "total error:  3.3086547255516052\n",
      "training error:  0.8739656209945679  and  1.3842368125915527  and  1.0695117712020874\n",
      "total error:  3.327714204788208\n",
      "training error:  0.8838150501251221  and  1.337360143661499  and  1.0823378562927246\n",
      "total error:  3.3035130500793457\n",
      "training error:  0.8463419675827026  and  1.380211591720581  and  1.1627970933914185\n",
      "total error:  3.389350652694702\n",
      "training error:  0.8306989669799805  and  1.2985649108886719  and  1.0580074787139893\n",
      "total error:  3.1872713565826416\n",
      "training error:  0.8586012721061707  and  1.3762662410736084  and  1.0670890808105469\n",
      "total error:  3.301956593990326\n",
      "training error:  0.8571334481239319  and  1.431196928024292  and  1.0821118354797363\n",
      "total error:  3.37044221162796\n",
      "training error:  0.869212806224823  and  1.3925544023513794  and  1.1051607131958008\n",
      "total error:  3.366927921772003\n",
      "training error:  0.8587242364883423  and  1.5078096389770508  and  1.3142926692962646\n",
      "total error:  3.6808265447616577\n",
      "training error:  0.8449877500534058  and  1.4901573657989502  and  1.1421692371368408\n",
      "total error:  3.4773143529891968\n",
      "training error:  0.8440431952476501  and  1.4391456842422485  and  1.1565721035003662\n",
      "total error:  3.439760982990265\n",
      "training error:  0.889212429523468  and  1.4158005714416504  and  1.1310218572616577\n",
      "total error:  3.436034858226776\n",
      "training error:  0.8436211943626404  and  1.2969943284988403  and  1.0474708080291748\n",
      "total error:  3.1880863308906555\n",
      "training error:  0.8640269637107849  and  1.308153748512268  and  1.1208226680755615\n",
      "total error:  3.2930033802986145\n",
      "training error:  0.8513915538787842  and  1.344504714012146  and  1.122389793395996\n",
      "total error:  3.3182860612869263\n",
      "training error:  0.8716421127319336  and  1.290431022644043  and  1.0536057949066162\n",
      "total error:  3.2156789302825928\n",
      "training error:  0.8480653762817383  and  1.3828647136688232  and  1.144667148590088\n",
      "total error:  3.3755972385406494\n",
      "training error:  0.8404972553253174  and  1.296384572982788  and  1.1123441457748413\n",
      "total error:  3.2492259740829468\n",
      "training error:  0.8550490140914917  and  1.3492515087127686  and  1.1066877841949463\n",
      "total error:  3.3109883069992065\n",
      "training error:  0.844192385673523  and  1.371030569076538  and  1.095630407333374\n",
      "total error:  3.310853362083435\n",
      "training error:  0.8449093103408813  and  1.4290529489517212  and  1.1885063648223877\n",
      "total error:  3.4624686241149902\n",
      "training error:  0.8580915927886963  and  1.3459138870239258  and  1.0899361371994019\n",
      "total error:  3.293941617012024\n",
      "training error:  0.8568968176841736  and  1.3025541305541992  and  1.0766913890838623\n",
      "total error:  3.236142337322235\n",
      "training error:  0.8578554391860962  and  1.288766622543335  and  1.1832042932510376\n",
      "total error:  3.3298263549804688\n",
      "training error:  0.8219349384307861  and  1.365088939666748  and  1.0373529195785522\n",
      "total error:  3.2243767976760864\n",
      "training error:  0.861110270023346  and  1.2703733444213867  and  1.042201042175293\n",
      "total error:  3.1736846566200256\n",
      "training error:  0.851165771484375  and  1.2595280408859253  and  1.0950291156768799\n",
      "total error:  3.20572292804718\n",
      "training error:  0.8519219160079956  and  1.3056375980377197  and  1.0863618850708008\n",
      "total error:  3.243921399116516\n",
      "training error:  0.8253628611564636  and  1.4062435626983643  and  1.082053303718567\n",
      "total error:  3.3136597275733948\n",
      "training error:  0.8392190933227539  and  1.2644308805465698  and  1.0941661596298218\n",
      "total error:  3.1978161334991455\n",
      "training error:  0.8399302363395691  and  1.3253397941589355  and  1.0567169189453125\n",
      "total error:  3.221986949443817\n",
      "training error:  0.832037627696991  and  1.3711119890213013  and  1.1120293140411377\n",
      "total error:  3.31517893075943\n",
      "training error:  0.8461916446685791  and  1.2818936109542847  and  1.047347903251648\n",
      "total error:  3.1754331588745117\n",
      "training error:  0.8604138493537903  and  1.2994568347930908  and  1.1112958192825317\n",
      "total error:  3.271166503429413\n",
      "training error:  0.8475943803787231  and  1.2720563411712646  and  1.0520808696746826\n",
      "total error:  3.1717315912246704\n",
      "training error:  0.8372825384140015  and  1.3267608880996704  and  1.076080083847046\n",
      "total error:  3.2401235103607178\n",
      "training error:  0.8409966826438904  and  1.3422516584396362  and  1.0761853456497192\n",
      "total error:  3.259433686733246\n",
      "training error:  0.8417795896530151  and  1.3347465991973877  and  1.1310908794403076\n",
      "total error:  3.3076170682907104\n",
      "training error:  0.829583466053009  and  1.3341243267059326  and  1.1107721328735352\n",
      "total error:  3.274479925632477\n",
      "training error:  0.8283342719078064  and  1.3561242818832397  and  1.1525124311447144\n",
      "total error:  3.3369709849357605\n",
      "training error:  0.8400709629058838  and  1.3599729537963867  and  1.0822184085845947\n",
      "total error:  3.2822623252868652\n",
      "training error:  0.842200756072998  and  1.2764413356781006  and  1.0899471044540405\n",
      "total error:  3.208589196205139\n",
      "training error:  0.8697537183761597  and  1.2944815158843994  and  1.075265884399414\n",
      "total error:  3.239501118659973\n",
      "training error:  0.8474733829498291  and  1.281380295753479  and  1.0842503309249878\n",
      "total error:  3.213104009628296\n",
      "training error:  0.8377400040626526  and  1.2839362621307373  and  1.1020970344543457\n",
      "total error:  3.2237733006477356\n",
      "training error:  0.8306165933609009  and  1.32180655002594  and  1.0578687191009521\n",
      "total error:  3.210291862487793\n",
      "training error:  0.841751217842102  and  1.4565863609313965  and  1.048830509185791\n",
      "total error:  3.3471680879592896\n",
      "training error:  0.8280466198921204  and  1.2695878744125366  and  1.0765482187271118\n",
      "total error:  3.174182713031769\n",
      "training error:  0.8759452700614929  and  1.718429684638977  and  1.1847624778747559\n",
      "total error:  3.779137432575226\n",
      "training error:  0.8459011912345886  and  1.2992761135101318  and  1.0766891241073608\n",
      "total error:  3.2218664288520813\n",
      "training error:  0.8336377143859863  and  1.5631293058395386  and  1.141018271446228\n",
      "total error:  3.537785291671753\n",
      "training error:  0.8309925198554993  and  1.2994236946105957  and  1.05264413356781\n",
      "total error:  3.183060348033905\n",
      "training error:  0.847713828086853  and  1.3250173330307007  and  1.090566873550415\n",
      "total error:  3.2632980346679688\n",
      "training error:  0.8312845826148987  and  1.3075151443481445  and  1.0803067684173584\n",
      "total error:  3.2191064953804016\n",
      "training error:  0.830585241317749  and  1.284480333328247  and  1.094369888305664\n",
      "total error:  3.20943546295166\n",
      "training error:  0.8370807766914368  and  1.3461246490478516  and  1.024254322052002\n",
      "total error:  3.2074597477912903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.8747125864028931  and  1.5494056940078735  and  1.0913243293762207\n",
      "total error:  3.5154426097869873\n",
      "training error:  0.8833714723587036  and  1.3803167343139648  and  1.094362497329712\n",
      "total error:  3.3580507040023804\n",
      "training error:  0.8332467675209045  and  1.3432462215423584  and  1.0363333225250244\n",
      "total error:  3.2128263115882874\n",
      "training error:  0.8203883171081543  and  1.3921444416046143  and  1.0895135402679443\n",
      "total error:  3.302046298980713\n",
      "training error:  0.869623064994812  and  1.3294974565505981  and  1.0972356796264648\n",
      "total error:  3.296356201171875\n",
      "training error:  0.8322729468345642  and  1.30181884765625  and  1.111310362815857\n",
      "total error:  3.245402157306671\n",
      "training error:  0.8331079483032227  and  1.3097400665283203  and  1.0663138628005981\n",
      "total error:  3.209161877632141\n",
      "training error:  0.8163937330245972  and  1.3706691265106201  and  1.1020909547805786\n",
      "total error:  3.289153814315796\n",
      "training error:  0.8212243318557739  and  1.3124696016311646  and  1.101459264755249\n",
      "total error:  3.2351531982421875\n",
      "training error:  0.8367392420768738  and  1.448094367980957  and  1.1283049583435059\n",
      "total error:  3.4131385684013367\n",
      "training error:  0.8260974287986755  and  1.2765703201293945  and  1.0853967666625977\n",
      "total error:  3.1880645155906677\n",
      "training error:  0.8197876811027527  and  1.2497742176055908  and  1.1065254211425781\n",
      "total error:  3.1760873198509216\n",
      "training error:  0.8246239423751831  and  1.2667407989501953  and  1.0544474124908447\n",
      "total error:  3.145812153816223\n",
      "training error:  0.8510132431983948  and  1.3441314697265625  and  1.163699984550476\n",
      "total error:  3.3588446974754333\n",
      "training error:  0.8519808650016785  and  1.282532811164856  and  1.0943524837493896\n",
      "total error:  3.228866159915924\n",
      "training error:  0.8234553933143616  and  1.255176305770874  and  1.0546815395355225\n",
      "total error:  3.133313238620758\n",
      "training error:  0.8588340878486633  and  1.2724356651306152  and  1.0698952674865723\n",
      "total error:  3.201165020465851\n",
      "training error:  0.8439165353775024  and  1.291083574295044  and  1.1164014339447021\n",
      "total error:  3.2514015436172485\n",
      "training error:  0.8177769780158997  and  1.2872966527938843  and  1.071110725402832\n",
      "total error:  3.176184356212616\n",
      "training error:  0.823974072933197  and  1.3195686340332031  and  1.0956186056137085\n",
      "total error:  3.2391613125801086\n",
      "training error:  0.8411159515380859  and  1.2384339570999146  and  1.0312867164611816\n",
      "total error:  3.110836625099182\n",
      "training error:  0.8451071977615356  and  1.4070130586624146  and  1.173959493637085\n",
      "total error:  3.426079750061035\n",
      "training error:  0.876993715763092  and  1.345909595489502  and  1.1477439403533936\n",
      "total error:  3.3706472516059875\n",
      "training error:  0.8293313980102539  and  1.2787219285964966  and  1.0721687078475952\n",
      "total error:  3.1802220344543457\n",
      "training error:  0.8411608338356018  and  1.2225048542022705  and  1.096279501914978\n",
      "total error:  3.1599451899528503\n",
      "training error:  0.8293720483779907  and  1.4087599515914917  and  1.10850989818573\n",
      "total error:  3.3466418981552124\n",
      "training error:  0.8237518072128296  and  1.514662504196167  and  1.0868772268295288\n",
      "total error:  3.4252915382385254\n",
      "training error:  0.8268135786056519  and  1.306377649307251  and  1.0899550914764404\n",
      "total error:  3.2231463193893433\n",
      "training error:  0.8092235326766968  and  1.3704502582550049  and  1.0692120790481567\n",
      "total error:  3.2488858699798584\n",
      "training error:  0.8313770294189453  and  1.3086917400360107  and  1.1398013830184937\n",
      "total error:  3.2798701524734497\n",
      "training error:  0.8362699151039124  and  1.283307433128357  and  1.0719168186187744\n",
      "total error:  3.1914941668510437\n",
      "training error:  0.8510203957557678  and  1.288618803024292  and  1.0776731967926025\n",
      "total error:  3.2173123955726624\n",
      "training error:  0.8337461948394775  and  1.2582809925079346  and  1.1575138568878174\n",
      "total error:  3.2495410442352295\n",
      "training error:  0.8411011695861816  and  1.2850782871246338  and  1.0899959802627563\n",
      "total error:  3.2161754369735718\n",
      "training error:  0.8079735040664673  and  1.2470470666885376  and  1.0336434841156006\n",
      "total error:  3.0886640548706055\n",
      "training error:  0.8187755942344666  and  1.2840017080307007  and  1.044874906539917\n",
      "total error:  3.1476522088050842\n",
      "training error:  0.8093628287315369  and  1.269054651260376  and  1.078752040863037\n",
      "total error:  3.15716952085495\n",
      "training error:  0.842872679233551  and  1.285398006439209  and  1.074585199356079\n",
      "total error:  3.202855885028839\n",
      "training error:  0.8485330939292908  and  1.4239588975906372  and  1.0979773998260498\n",
      "total error:  3.370469391345978\n",
      "training error:  0.8198354244232178  and  1.280923843383789  and  1.0872362852096558\n",
      "total error:  3.1879955530166626\n",
      "training error:  0.8176162838935852  and  1.3220531940460205  and  1.134503722190857\n",
      "total error:  3.2741732001304626\n",
      "training error:  0.8170590400695801  and  1.313098669052124  and  1.0712814331054688\n",
      "total error:  3.201439142227173\n",
      "training error:  0.8370238542556763  and  1.250596046447754  and  1.1332886219024658\n",
      "total error:  3.220908522605896\n",
      "training error:  0.8193926215171814  and  1.2877506017684937  and  1.0586130619049072\n",
      "total error:  3.1657562851905823\n",
      "training error:  0.8241108655929565  and  1.2841798067092896  and  1.1212188005447388\n",
      "total error:  3.229509472846985\n",
      "training error:  0.829238772392273  and  1.2519162893295288  and  1.0639050006866455\n",
      "total error:  3.1450600624084473\n",
      "training error:  0.8108624219894409  and  1.3180984258651733  and  1.0498030185699463\n",
      "total error:  3.1787638664245605\n",
      "training error:  0.8287473917007446  and  1.3111779689788818  and  1.0389106273651123\n",
      "total error:  3.1788359880447388\n",
      "training error:  0.8029770851135254  and  1.2722223997116089  and  1.0575553178787231\n",
      "total error:  3.1327548027038574\n",
      "training error:  0.8150677680969238  and  1.2684212923049927  and  1.0739892721176147\n",
      "total error:  3.1574783325195312\n",
      "training error:  0.8505845665931702  and  1.3071866035461426  and  1.1212449073791504\n",
      "total error:  3.279016077518463\n",
      "training error:  0.8204331398010254  and  1.4181616306304932  and  1.1513628959655762\n",
      "total error:  3.3899576663970947\n",
      "training error:  0.8091990947723389  and  1.2774665355682373  and  1.0599660873413086\n",
      "total error:  3.1466317176818848\n",
      "training error:  0.8125066757202148  and  1.2294023036956787  and  1.033109188079834\n",
      "total error:  3.0750181674957275\n",
      "training error:  0.8108983635902405  and  1.414675235748291  and  1.0887371301651\n",
      "total error:  3.3143107295036316\n",
      "training error:  0.8113774061203003  and  1.2529696226119995  and  1.05843186378479\n",
      "total error:  3.12277889251709\n",
      "training error:  0.8427808880805969  and  1.3570711612701416  and  1.0447711944580078\n",
      "total error:  3.2446232438087463\n",
      "training error:  0.8349192142486572  and  1.2537786960601807  and  1.0461511611938477\n",
      "total error:  3.1348490715026855\n",
      "training error:  0.8181654214859009  and  1.3470306396484375  and  1.0936061143875122\n",
      "total error:  3.2588021755218506\n",
      "training error:  0.8314164280891418  and  1.2920799255371094  and  1.0859581232070923\n",
      "total error:  3.2094544768333435\n",
      "training error:  0.8557055592536926  and  1.5839685201644897  and  1.0886437892913818\n",
      "total error:  3.528317868709564\n",
      "training error:  0.7999958992004395  and  1.3106895685195923  and  1.0961241722106934\n",
      "total error:  3.206809639930725\n",
      "training error:  0.8227362632751465  and  1.2913845777511597  and  1.1201945543289185\n",
      "total error:  3.2343153953552246\n",
      "training error:  0.8333848714828491  and  1.26902437210083  and  1.0833748579025269\n",
      "total error:  3.185784101486206\n",
      "training error:  0.803846001625061  and  1.2184581756591797  and  1.0526269674301147\n",
      "total error:  3.0749311447143555\n",
      "training error:  0.8180243372917175  and  1.256910800933838  and  1.0592857599258423\n",
      "total error:  3.1342208981513977\n",
      "training error:  0.8225849866867065  and  1.2703725099563599  and  1.0842196941375732\n",
      "total error:  3.1771771907806396\n",
      "training error:  0.8134423494338989  and  1.2685546875  and  1.0825871229171753\n",
      "total error:  3.164584159851074\n",
      "training error:  0.824045717716217  and  1.2614312171936035  and  1.0922642946243286\n",
      "total error:  3.177741229534149\n",
      "training error:  0.8253064155578613  and  1.3525209426879883  and  1.0682228803634644\n",
      "total error:  3.246050238609314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.8131216764450073  and  1.3053971529006958  and  1.0718374252319336\n",
      "total error:  3.1903562545776367\n",
      "training error:  0.8043358325958252  and  1.2524833679199219  and  1.0625298023223877\n",
      "total error:  3.1193490028381348\n",
      "training error:  0.8337961435317993  and  1.336566686630249  and  1.1344645023345947\n",
      "total error:  3.304827332496643\n",
      "training error:  0.8177706003189087  and  1.2614351511001587  and  1.0733141899108887\n",
      "total error:  3.152519941329956\n",
      "training error:  0.8168355226516724  and  1.260274887084961  and  1.0960193872451782\n",
      "total error:  3.1731297969818115\n",
      "training error:  0.8044244050979614  and  1.2340567111968994  and  1.0984965562820435\n",
      "total error:  3.1369776725769043\n",
      "training error:  0.8088728785514832  and  1.2652037143707275  and  1.0357060432434082\n",
      "total error:  3.109782636165619\n",
      "training error:  0.8046869039535522  and  1.251209020614624  and  1.0425567626953125\n",
      "total error:  3.0984526872634888\n",
      "training error:  0.8147050142288208  and  1.3091504573822021  and  1.070236086845398\n",
      "total error:  3.194091558456421\n",
      "training error:  0.817600429058075  and  1.2674839496612549  and  1.025884747505188\n",
      "total error:  3.110969126224518\n",
      "training error:  0.8138379454612732  and  1.3158695697784424  and  1.1230618953704834\n",
      "total error:  3.252769410610199\n",
      "training error:  0.8186660408973694  and  1.335566759109497  and  1.0442205667495728\n",
      "total error:  3.198453366756439\n",
      "training error:  0.8379430770874023  and  1.433485746383667  and  1.046053171157837\n",
      "total error:  3.3174819946289062\n",
      "training error:  0.8279457092285156  and  1.2611223459243774  and  1.0774483680725098\n",
      "total error:  3.166516423225403\n",
      "training error:  0.8243848085403442  and  1.2469758987426758  and  1.0674476623535156\n",
      "total error:  3.1388083696365356\n",
      "training error:  0.7969710826873779  and  1.2406328916549683  and  1.111495852470398\n",
      "total error:  3.149099826812744\n",
      "training error:  0.8083003759384155  and  1.2890191078186035  and  1.0813872814178467\n",
      "total error:  3.1787067651748657\n",
      "training error:  0.821379542350769  and  1.4108681678771973  and  1.1030397415161133\n",
      "total error:  3.3352874517440796\n",
      "training error:  0.7829717397689819  and  1.2376313209533691  and  1.0828170776367188\n",
      "total error:  3.10342013835907\n",
      "training error:  0.8363475799560547  and  1.3942615985870361  and  1.1420905590057373\n",
      "total error:  3.372699737548828\n",
      "training error:  0.7993183135986328  and  1.295742154121399  and  1.0703201293945312\n",
      "total error:  3.165380597114563\n",
      "training error:  0.8049045205116272  and  1.3168582916259766  and  1.0486235618591309\n",
      "total error:  3.1703863739967346\n",
      "training error:  0.8316105008125305  and  1.3698501586914062  and  1.024058222770691\n",
      "total error:  3.2255188822746277\n",
      "training error:  0.8196777105331421  and  1.2610447406768799  and  1.0628018379211426\n",
      "total error:  3.1435242891311646\n",
      "training error:  0.7933151125907898  and  1.3512611389160156  and  1.1931445598602295\n",
      "total error:  3.337720811367035\n",
      "training error:  0.8152859807014465  and  1.2446813583374023  and  1.252953290939331\n",
      "total error:  3.31292062997818\n",
      "training error:  0.7827466726303101  and  1.2470823526382446  and  1.0691473484039307\n",
      "total error:  3.0989763736724854\n",
      "training error:  0.8054611682891846  and  1.3029820919036865  and  1.0936734676361084\n",
      "total error:  3.2021167278289795\n",
      "training error:  0.8219021558761597  and  1.287906289100647  and  1.050709843635559\n",
      "total error:  3.1605182886123657\n",
      "training error:  0.812964141368866  and  1.359052300453186  and  1.189720869064331\n",
      "total error:  3.361737310886383\n",
      "training error:  0.7834283709526062  and  1.2611992359161377  and  1.0839673280715942\n",
      "total error:  3.128594934940338\n",
      "training error:  0.7949889898300171  and  1.2414398193359375  and  1.0259590148925781\n",
      "total error:  3.0623878240585327\n",
      "training error:  0.779962420463562  and  1.247689962387085  and  1.099120855331421\n",
      "total error:  3.126773238182068\n",
      "training error:  0.832822859287262  and  1.2624930143356323  and  1.0379446744918823\n",
      "total error:  3.1332605481147766\n",
      "training error:  0.8087543845176697  and  1.280536413192749  and  1.1055562496185303\n",
      "total error:  3.194847047328949\n",
      "training error:  0.8153319358825684  and  1.2922158241271973  and  1.080388069152832\n",
      "total error:  3.1879358291625977\n",
      "training error:  0.8026372194290161  and  1.2604997158050537  and  1.0081404447555542\n",
      "total error:  3.071277379989624\n",
      "training error:  0.788606584072113  and  1.2614299058914185  and  1.1139333248138428\n",
      "total error:  3.1639698147773743\n",
      "training error:  0.8279218077659607  and  1.2646788358688354  and  1.089279294013977\n",
      "total error:  3.181879937648773\n",
      "training error:  0.8294723629951477  and  1.3687876462936401  and  1.0451109409332275\n",
      "total error:  3.2433709502220154\n",
      "training error:  0.7959833741188049  and  1.2364263534545898  and  1.06685209274292\n",
      "total error:  3.0992618203163147\n",
      "training error:  0.8174425363540649  and  1.244385004043579  and  1.0228772163391113\n",
      "total error:  3.0847047567367554\n",
      "training error:  0.8216484785079956  and  1.501422643661499  and  1.1020500659942627\n",
      "total error:  3.4251211881637573\n",
      "training error:  0.8226197361946106  and  1.3291006088256836  and  1.0794084072113037\n",
      "total error:  3.231128752231598\n",
      "training error:  0.8251329064369202  and  1.2484190464019775  and  1.0754554271697998\n",
      "total error:  3.1490073800086975\n",
      "training error:  0.8103336095809937  and  1.230966567993164  and  1.0398956537246704\n",
      "total error:  3.081195831298828\n",
      "training error:  0.7915840744972229  and  1.2547705173492432  and  1.0695240497589111\n",
      "total error:  3.115878641605377\n",
      "training error:  0.7798174023628235  and  1.3106029033660889  and  1.1108754873275757\n",
      "total error:  3.201295793056488\n",
      "training error:  0.8194214701652527  and  1.2607078552246094  and  1.0486726760864258\n",
      "total error:  3.128802001476288\n",
      "training error:  0.7897679805755615  and  1.2357046604156494  and  1.0361504554748535\n",
      "total error:  3.0616230964660645\n",
      "training error:  0.8255637884140015  and  1.7332888841629028  and  1.1229687929153442\n",
      "total error:  3.6818214654922485\n",
      "training error:  0.8137255907058716  and  1.2439473867416382  and  1.1003583669662476\n",
      "total error:  3.1580313444137573\n",
      "training error:  0.8111610412597656  and  1.2699428796768188  and  1.0961675643920898\n",
      "total error:  3.1772714853286743\n",
      "training error:  0.8008215427398682  and  1.2431182861328125  and  1.1113405227661133\n",
      "total error:  3.155280351638794\n",
      "training error:  0.8011410236358643  and  1.2845923900604248  and  1.0457305908203125\n",
      "total error:  3.1314640045166016\n",
      "training error:  0.7800343036651611  and  1.261521339416504  and  1.1298238039016724\n",
      "total error:  3.1713794469833374\n",
      "training error:  0.8189809322357178  and  1.228786826133728  and  1.0196176767349243\n",
      "total error:  3.06738543510437\n",
      "training error:  0.7895604968070984  and  1.2876865863800049  and  1.1055901050567627\n",
      "total error:  3.182837188243866\n",
      "training error:  0.7990463972091675  and  1.247507929801941  and  1.0337146520614624\n",
      "total error:  3.080268979072571\n",
      "training error:  0.7917492389678955  and  1.2033393383026123  and  1.0199291706085205\n",
      "total error:  3.0150177478790283\n",
      "training error:  0.8003393411636353  and  1.3538199663162231  and  1.057920217514038\n",
      "total error:  3.2120795249938965\n",
      "training error:  0.8095522522926331  and  1.3796942234039307  and  1.0329936742782593\n",
      "total error:  3.222240149974823\n",
      "training error:  0.797924816608429  and  1.249756932258606  and  1.0918971300125122\n",
      "total error:  3.139578878879547\n",
      "training error:  0.7675633430480957  and  1.2463711500167847  and  1.0585994720458984\n",
      "total error:  3.072533965110779\n",
      "training error:  0.7852891683578491  and  1.2365707159042358  and  1.044534683227539\n",
      "total error:  3.066394567489624\n",
      "training error:  0.7973148226737976  and  1.2786293029785156  and  1.1541528701782227\n",
      "total error:  3.230096995830536\n",
      "training error:  0.8029887676239014  and  1.2344775199890137  and  1.111771821975708\n",
      "total error:  3.149238109588623\n",
      "training error:  0.768977701663971  and  1.2431073188781738  and  1.1446889638900757\n",
      "total error:  3.1567739844322205\n",
      "training error:  0.7808501124382019  and  1.2690703868865967  and  1.017613172531128\n",
      "total error:  3.0675336718559265\n",
      "training error:  0.797569990158081  and  1.2654380798339844  and  1.0726220607757568\n",
      "total error:  3.1356301307678223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7766038179397583  and  1.213533878326416  and  1.0024137496948242\n",
      "total error:  2.9925514459609985\n",
      "training error:  0.7953242659568787  and  1.233389973640442  and  1.096744179725647\n",
      "total error:  3.1254584193229675\n",
      "training error:  0.7921582460403442  and  1.2207938432693481  and  1.0142107009887695\n",
      "total error:  3.027162790298462\n",
      "training error:  0.8104466199874878  and  1.2331931591033936  and  1.0302066802978516\n",
      "total error:  3.073846459388733\n",
      "training error:  0.8004802465438843  and  1.2384365797042847  and  1.0633448362350464\n",
      "total error:  3.1022616624832153\n",
      "training error:  0.8052924871444702  and  1.2809654474258423  and  1.0219964981079102\n",
      "total error:  3.1082544326782227\n",
      "training error:  0.8086881041526794  and  1.2566397190093994  and  1.0370454788208008\n",
      "total error:  3.1023733019828796\n",
      "training error:  0.8252558708190918  and  1.368607759475708  and  1.0196770429611206\n",
      "total error:  3.2135406732559204\n",
      "training error:  0.7632501721382141  and  1.2936196327209473  and  1.1434110403060913\n",
      "total error:  3.2002808451652527\n",
      "training error:  0.8222010731697083  and  1.2403538227081299  and  1.0095932483673096\n",
      "total error:  3.0721481442451477\n",
      "training error:  0.804608941078186  and  1.2371292114257812  and  1.0139280557632446\n",
      "total error:  3.055666208267212\n",
      "training error:  0.7888776659965515  and  1.249224305152893  and  1.061754822731018\n",
      "total error:  3.0998567938804626\n",
      "training error:  0.7733323574066162  and  1.3186123371124268  and  1.0625627040863037\n",
      "total error:  3.1545073986053467\n",
      "training error:  0.7999383211135864  and  1.247513771057129  and  1.0627632141113281\n",
      "total error:  3.1102153062820435\n",
      "training error:  0.789862871170044  and  1.2402464151382446  and  1.0881457328796387\n",
      "total error:  3.1182550191879272\n",
      "training error:  0.7745991945266724  and  1.3705710172653198  and  1.1995759010314941\n",
      "total error:  3.3447461128234863\n",
      "training error:  0.7976828813552856  and  1.220175862312317  and  1.0164380073547363\n",
      "total error:  3.034296751022339\n",
      "training error:  0.8224448561668396  and  1.2884148359298706  and  1.1696242094039917\n",
      "total error:  3.280483901500702\n",
      "training error:  0.8351635932922363  and  1.3794023990631104  and  1.108385682106018\n",
      "total error:  3.3229516744613647\n",
      "training error:  0.798335075378418  and  1.5320415496826172  and  1.1393027305603027\n",
      "total error:  3.469679355621338\n",
      "training error:  0.8400143384933472  and  2.012375593185425  and  1.1749387979507446\n",
      "total error:  4.027328729629517\n",
      "training error:  0.8077151775360107  and  1.266282320022583  and  1.0490875244140625\n",
      "total error:  3.1230850219726562\n",
      "training error:  0.7760879993438721  and  1.266660213470459  and  1.0057132244110107\n",
      "total error:  3.048461437225342\n",
      "training error:  0.7800229787826538  and  1.2297298908233643  and  1.0578782558441162\n",
      "total error:  3.0676311254501343\n",
      "training error:  0.7948936223983765  and  1.2412574291229248  and  1.126686692237854\n",
      "total error:  3.1628377437591553\n",
      "training error:  0.7985936403274536  and  1.2408478260040283  and  1.061046838760376\n",
      "total error:  3.100488305091858\n",
      "training error:  0.7990044951438904  and  1.258518099784851  and  1.0869139432907104\n",
      "total error:  3.144436538219452\n",
      "training error:  0.8015480041503906  and  1.2159289121627808  and  1.0964012145996094\n",
      "total error:  3.1138781309127808\n",
      "training error:  0.7899285554885864  and  1.237959384918213  and  1.0426249504089355\n",
      "total error:  3.070512890815735\n",
      "training error:  0.7959831953048706  and  1.2903072834014893  and  1.139946699142456\n",
      "total error:  3.226237177848816\n",
      "training error:  0.7983976602554321  and  1.2608668804168701  and  1.049060583114624\n",
      "total error:  3.1083251237869263\n",
      "training error:  0.761474072933197  and  1.2707505226135254  and  1.039190649986267\n",
      "total error:  3.0714152455329895\n",
      "training error:  0.7992711067199707  and  1.2547814846038818  and  1.0611646175384521\n",
      "total error:  3.1152172088623047\n",
      "training error:  0.7755737900733948  and  1.263714075088501  and  1.096020221710205\n",
      "total error:  3.135308086872101\n",
      "training error:  0.8093879222869873  and  1.3326483964920044  and  1.07700777053833\n",
      "total error:  3.2190440893173218\n",
      "training error:  0.7983939051628113  and  1.2696092128753662  and  1.0090434551239014\n",
      "total error:  3.077046573162079\n",
      "training error:  0.7848784923553467  and  1.3200862407684326  and  1.121614933013916\n",
      "total error:  3.2265796661376953\n",
      "training error:  0.7734930515289307  and  1.3072669506072998  and  1.0297648906707764\n",
      "total error:  3.110524892807007\n",
      "training error:  0.8406001329421997  and  1.2762742042541504  and  1.0855019092559814\n",
      "total error:  3.2023762464523315\n",
      "training error:  0.8035581111907959  and  1.2550437450408936  and  1.0735807418823242\n",
      "total error:  3.1321825981140137\n",
      "training error:  0.8031416535377502  and  1.2705285549163818  and  1.0305122137069702\n",
      "total error:  3.1041824221611023\n",
      "training error:  0.7895928025245667  and  1.21051824092865  and  1.062896966934204\n",
      "total error:  3.0630080103874207\n",
      "training error:  0.7806769609451294  and  1.4225516319274902  and  1.032438039779663\n",
      "total error:  3.2356666326522827\n",
      "training error:  0.7676524519920349  and  1.2170727252960205  and  1.0231989622116089\n",
      "total error:  3.0079241394996643\n",
      "training error:  0.799870491027832  and  1.2046763896942139  and  1.0366601943969727\n",
      "total error:  3.0412070751190186\n",
      "training error:  0.7753936648368835  and  1.2126939296722412  and  1.0510293245315552\n",
      "total error:  3.03911691904068\n",
      "training error:  0.763309121131897  and  1.407004952430725  and  1.071332335472107\n",
      "total error:  3.241646409034729\n",
      "training error:  0.7708920240402222  and  1.2312747240066528  and  1.0536973476409912\n",
      "total error:  3.055864095687866\n",
      "training error:  0.785912275314331  and  1.409820318222046  and  1.0798404216766357\n",
      "total error:  3.2755730152130127\n",
      "training error:  0.7702062129974365  and  1.2786860466003418  and  1.1003986597061157\n",
      "total error:  3.149290919303894\n",
      "training error:  0.7771971821784973  and  1.277584433555603  and  1.0307021141052246\n",
      "total error:  3.085483729839325\n",
      "training error:  0.7764554023742676  and  1.2353591918945312  and  1.0017125606536865\n",
      "total error:  3.0135271549224854\n",
      "training error:  0.7762424945831299  and  1.223024845123291  and  0.9971349835395813\n",
      "total error:  2.996402323246002\n",
      "training error:  0.776006817817688  and  1.2101032733917236  and  0.9751028418540955\n",
      "total error:  2.961212933063507\n",
      "training error:  0.8121713399887085  and  1.2909071445465088  and  1.0343068838119507\n",
      "total error:  3.137385368347168\n",
      "training error:  0.774630069732666  and  1.2592105865478516  and  1.0478053092956543\n",
      "total error:  3.081645965576172\n",
      "training error:  0.781933605670929  and  1.2348438501358032  and  1.046227216720581\n",
      "total error:  3.0630046725273132\n",
      "training error:  0.8090388178825378  and  1.2756538391113281  and  1.0491005182266235\n",
      "total error:  3.1337931752204895\n",
      "training error:  0.7728483080863953  and  1.2069776058197021  and  1.0122300386428833\n",
      "total error:  2.9920559525489807\n",
      "training error:  0.7589740753173828  and  1.2510294914245605  and  1.0288254022598267\n",
      "total error:  3.03882896900177\n",
      "training error:  0.7820320129394531  and  1.2739572525024414  and  1.0522264242172241\n",
      "total error:  3.1082156896591187\n",
      "training error:  0.7802011966705322  and  1.4136881828308105  and  1.0456807613372803\n",
      "total error:  3.239570140838623\n",
      "training error:  0.7733623385429382  and  1.3456828594207764  and  1.0479650497436523\n",
      "total error:  3.167010247707367\n",
      "training error:  0.7943987846374512  and  1.2678606510162354  and  0.9968627691268921\n",
      "total error:  3.0591222047805786\n",
      "training error:  0.8083755373954773  and  1.3907009363174438  and  1.1237008571624756\n",
      "total error:  3.3227773308753967\n",
      "training error:  0.7653217315673828  and  1.233778953552246  and  1.0504698753356934\n",
      "total error:  3.0495705604553223\n",
      "training error:  0.8241297006607056  and  1.2237732410430908  and  1.108148455619812\n",
      "total error:  3.1560513973236084\n",
      "training error:  0.7665132284164429  and  1.256152868270874  and  1.0828015804290771\n",
      "total error:  3.105467677116394\n",
      "training error:  0.7728203535079956  and  1.3233973979949951  and  1.0107088088989258\n",
      "total error:  3.1069265604019165\n",
      "training error:  0.7718954086303711  and  1.2601193189620972  and  1.0412917137145996\n",
      "total error:  3.073306441307068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7741859555244446  and  1.3445786237716675  and  1.0155394077301025\n",
      "total error:  3.1343039870262146\n",
      "training error:  0.7610698938369751  and  1.2547032833099365  and  1.0742056369781494\n",
      "total error:  3.089978814125061\n",
      "training error:  0.7809051871299744  and  1.3873664140701294  and  1.0944468975067139\n",
      "total error:  3.2627184987068176\n",
      "training error:  0.8062238693237305  and  1.2964341640472412  and  0.9967019557952881\n",
      "total error:  3.0993599891662598\n",
      "training error:  0.738538384437561  and  1.2583330869674683  and  1.047328233718872\n",
      "total error:  3.0441997051239014\n",
      "training error:  0.7765416502952576  and  1.2318896055221558  and  1.0537350177764893\n",
      "total error:  3.0621662735939026\n",
      "training error:  0.7734037637710571  and  1.2290090322494507  and  1.0855075120925903\n",
      "total error:  3.087920308113098\n",
      "training error:  0.7423722743988037  and  1.247420072555542  and  1.0297085046768188\n",
      "total error:  3.0195008516311646\n",
      "training error:  0.789944052696228  and  1.2723222970962524  and  1.1228244304656982\n",
      "total error:  3.1850907802581787\n",
      "training error:  0.751788318157196  and  1.2163156270980835  and  1.0249074697494507\n",
      "total error:  2.9930114150047302\n",
      "training error:  0.8058331608772278  and  1.2957723140716553  and  1.1154223680496216\n",
      "total error:  3.2170278429985046\n",
      "training error:  0.773152232170105  and  1.3088260889053345  and  1.0355205535888672\n",
      "total error:  3.1174988746643066\n",
      "training error:  0.7436728477478027  and  1.218268632888794  and  1.0229803323745728\n",
      "total error:  2.9849218130111694\n",
      "training error:  0.7594786882400513  and  1.2780377864837646  and  1.0511474609375\n",
      "total error:  3.088663935661316\n",
      "training error:  0.7498537302017212  and  1.198050856590271  and  1.002516508102417\n",
      "total error:  2.950421094894409\n",
      "training error:  0.8026847839355469  and  1.379242181777954  and  1.198071002960205\n",
      "total error:  3.379997968673706\n",
      "training error:  0.7892211079597473  and  1.268517017364502  and  1.0147764682769775\n",
      "total error:  3.072514593601227\n",
      "training error:  0.745245099067688  and  1.3283319473266602  and  1.0173695087432861\n",
      "total error:  3.0909465551376343\n",
      "training error:  0.7970552444458008  and  1.2344276905059814  and  1.1276580095291138\n",
      "total error:  3.159140944480896\n",
      "training error:  0.771274209022522  and  1.249086856842041  and  1.1397826671600342\n",
      "total error:  3.160143733024597\n",
      "training error:  0.8107372522354126  and  1.507188320159912  and  1.1656550168991089\n",
      "total error:  3.4835805892944336\n",
      "training error:  0.7566201686859131  and  1.171689510345459  and  0.9904125332832336\n",
      "total error:  2.9187222123146057\n",
      "training error:  0.7786011695861816  and  1.2949331998825073  and  1.0406183004379272\n",
      "total error:  3.114152669906616\n",
      "training error:  0.7738621234893799  and  1.258225679397583  and  1.0615893602371216\n",
      "total error:  3.0936771631240845\n",
      "training error:  0.7636904120445251  and  1.23647141456604  and  1.0049962997436523\n",
      "total error:  3.0051581263542175\n",
      "training error:  0.7526561617851257  and  1.2195481061935425  and  1.0093727111816406\n",
      "total error:  2.981576979160309\n",
      "training error:  0.7493793964385986  and  1.239654302597046  and  1.0237162113189697\n",
      "total error:  3.0127499103546143\n",
      "training error:  0.7830830812454224  and  1.2921316623687744  and  1.0659633874893188\n",
      "total error:  3.1411781311035156\n",
      "training error:  0.7735198736190796  and  1.2149195671081543  and  1.0121124982833862\n",
      "total error:  3.00055193901062\n",
      "training error:  0.7607442140579224  and  1.2680153846740723  and  1.0388766527175903\n",
      "total error:  3.067636251449585\n",
      "training error:  0.7898921370506287  and  1.2400325536727905  and  1.125934362411499\n",
      "total error:  3.155859053134918\n",
      "training error:  0.7548569440841675  and  1.2174263000488281  and  1.0514148473739624\n",
      "total error:  3.023698091506958\n",
      "training error:  0.7807035446166992  and  1.2274842262268066  and  1.0057382583618164\n",
      "total error:  3.0139260292053223\n",
      "training error:  0.7624020576477051  and  1.2356759309768677  and  1.0206838846206665\n",
      "total error:  3.0187618732452393\n",
      "training error:  0.7778707146644592  and  1.2901197671890259  and  1.045469880104065\n",
      "total error:  3.11346036195755\n",
      "training error:  0.7673379182815552  and  1.2359280586242676  and  1.0192656517028809\n",
      "total error:  3.0225316286087036\n",
      "training error:  0.7767326831817627  and  1.2706412076950073  and  1.1600680351257324\n",
      "total error:  3.2074419260025024\n",
      "training error:  0.7760276198387146  and  1.2329413890838623  and  1.0484657287597656\n",
      "total error:  3.0574347376823425\n",
      "training error:  0.7393287420272827  and  1.205207109451294  and  1.005518913269043\n",
      "total error:  2.9500547647476196\n",
      "training error:  0.7439436316490173  and  1.2306153774261475  and  0.9869372248649597\n",
      "total error:  2.9614962339401245\n",
      "training error:  0.759750247001648  and  1.2516660690307617  and  1.0883617401123047\n",
      "total error:  3.0997780561447144\n",
      "training error:  0.7703363299369812  and  1.2627817392349243  and  1.0497057437896729\n",
      "total error:  3.0828238129615784\n",
      "training error:  0.7587867975234985  and  1.2213037014007568  and  1.0327394008636475\n",
      "total error:  3.012829899787903\n",
      "training error:  0.7548918724060059  and  1.3904074430465698  and  1.1887764930725098\n",
      "total error:  3.3340758085250854\n",
      "training error:  0.7494181394577026  and  1.2296638488769531  and  1.0251951217651367\n",
      "total error:  3.0042771100997925\n",
      "training error:  0.7632592916488647  and  1.231698751449585  and  1.0124839544296265\n",
      "total error:  3.007441997528076\n",
      "training error:  0.7651444673538208  and  1.1957701444625854  and  1.0233347415924072\n",
      "total error:  2.9842493534088135\n",
      "training error:  0.7674506902694702  and  1.2322827577590942  and  1.004760503768921\n",
      "total error:  3.0044939517974854\n",
      "training error:  0.7584201693534851  and  1.2316601276397705  and  1.0189028978347778\n",
      "total error:  3.0089831948280334\n",
      "training error:  0.789400577545166  and  1.242537498474121  and  1.0362179279327393\n",
      "total error:  3.0681560039520264\n",
      "training error:  0.7478963136672974  and  1.2170495986938477  and  1.0162404775619507\n",
      "total error:  2.9811863899230957\n",
      "training error:  0.7877965569496155  and  1.2801010608673096  and  1.0951027870178223\n",
      "total error:  3.1630004048347473\n",
      "training error:  0.7499260902404785  and  1.2285462617874146  and  1.0613971948623657\n",
      "total error:  3.039869546890259\n",
      "training error:  0.7856408357620239  and  1.1925787925720215  and  0.9667863845825195\n",
      "total error:  2.945006012916565\n",
      "training error:  0.7527899742126465  and  1.189197301864624  and  1.0329523086547852\n",
      "total error:  2.9749395847320557\n",
      "training error:  0.7446954846382141  and  1.2193132638931274  and  1.0883898735046387\n",
      "total error:  3.0523986220359802\n",
      "training error:  0.7563279867172241  and  1.2181862592697144  and  1.056323528289795\n",
      "total error:  3.0308377742767334\n",
      "training error:  0.7614881992340088  and  1.1800258159637451  and  0.9964475631713867\n",
      "total error:  2.9379615783691406\n",
      "training error:  0.7525410056114197  and  1.2031909227371216  and  0.9946730136871338\n",
      "total error:  2.950404942035675\n",
      "training error:  0.7544291615486145  and  1.2979276180267334  and  1.031812071800232\n",
      "total error:  3.08416885137558\n",
      "training error:  0.7719906568527222  and  1.288018822669983  and  1.0292952060699463\n",
      "total error:  3.0893046855926514\n",
      "training error:  0.7625669240951538  and  1.2244913578033447  and  1.089546799659729\n",
      "total error:  3.0766050815582275\n",
      "training error:  0.7353297472000122  and  1.2286875247955322  and  1.0234804153442383\n",
      "total error:  2.9874976873397827\n",
      "training error:  0.7613410353660583  and  1.42167067527771  and  1.2596938610076904\n",
      "total error:  3.4427055716514587\n",
      "training error:  0.7469867467880249  and  1.2387845516204834  and  0.999976396560669\n",
      "total error:  2.9857476949691772\n",
      "training error:  0.7704164981842041  and  1.398850917816162  and  1.0958442687988281\n",
      "total error:  3.2651116847991943\n",
      "training error:  0.762549877166748  and  1.2196924686431885  and  1.0661914348602295\n",
      "total error:  3.048433780670166\n",
      "training error:  0.7362504005432129  and  1.1983823776245117  and  1.0474705696105957\n",
      "total error:  2.9821033477783203\n",
      "training error:  0.7759465575218201  and  1.2445018291473389  and  1.0373239517211914\n",
      "total error:  3.0577723383903503\n",
      "training error:  0.7437259554862976  and  1.1935358047485352  and  1.0396811962127686\n",
      "total error:  2.9769429564476013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7572708129882812  and  1.1842132806777954  and  1.0089011192321777\n",
      "total error:  2.9503852128982544\n",
      "training error:  0.8024380803108215  and  1.4513037204742432  and  0.9971833229064941\n",
      "total error:  3.250925123691559\n",
      "training error:  0.7563798427581787  and  1.223292350769043  and  1.0359197854995728\n",
      "total error:  3.0155919790267944\n",
      "training error:  0.7622705698013306  and  1.2323957681655884  and  1.0083191394805908\n",
      "total error:  3.0029854774475098\n",
      "training error:  0.7806466817855835  and  1.2200120687484741  and  1.0368375778198242\n",
      "total error:  3.037496328353882\n",
      "training error:  0.7468177676200867  and  1.2267014980316162  and  1.1091997623443604\n",
      "total error:  3.0827190279960632\n",
      "training error:  0.7421633005142212  and  1.2419328689575195  and  1.0515265464782715\n",
      "total error:  3.035622715950012\n",
      "training error:  0.7526463270187378  and  1.3444262742996216  and  1.0743911266326904\n",
      "total error:  3.17146372795105\n",
      "training error:  0.7715246677398682  and  1.321595311164856  and  0.9995216131210327\n",
      "total error:  3.092641592025757\n",
      "training error:  0.7551231384277344  and  1.1882096529006958  and  1.0671576261520386\n",
      "total error:  3.0104904174804688\n",
      "training error:  0.7618518471717834  and  1.2388547658920288  and  0.9882732629776001\n",
      "total error:  2.9889798760414124\n",
      "training error:  0.740037739276886  and  1.2393912076950073  and  1.0517723560333252\n",
      "total error:  3.0312013030052185\n",
      "training error:  0.7384274005889893  and  1.1741660833358765  and  0.9754002690315247\n",
      "total error:  2.8879937529563904\n",
      "training error:  0.7350074052810669  and  1.2109272480010986  and  0.9572374820709229\n",
      "total error:  2.9031721353530884\n",
      "training error:  0.7671584486961365  and  1.3538098335266113  and  1.032578945159912\n",
      "total error:  3.15354722738266\n",
      "training error:  0.7728469371795654  and  1.1785202026367188  and  1.008368968963623\n",
      "total error:  2.9597361087799072\n",
      "training error:  0.7761695384979248  and  1.2718913555145264  and  1.0299533605575562\n",
      "total error:  3.0780142545700073\n",
      "training error:  0.7419750690460205  and  1.4146592617034912  and  1.0322349071502686\n",
      "total error:  3.1888692378997803\n",
      "training error:  0.768779993057251  and  1.2001526355743408  and  0.9677979946136475\n",
      "total error:  2.9367306232452393\n",
      "training error:  0.731743335723877  and  1.2595120668411255  and  0.9787106513977051\n",
      "total error:  2.9699660539627075\n",
      "training error:  0.7220392227172852  and  1.19248366355896  and  1.0876455307006836\n",
      "total error:  3.0021684169769287\n",
      "training error:  0.7439507246017456  and  1.2547836303710938  and  1.065932035446167\n",
      "total error:  3.0646663904190063\n",
      "training error:  0.7420521378517151  and  1.2207345962524414  and  0.9958524703979492\n",
      "total error:  2.9586392045021057\n",
      "training error:  0.7434687614440918  and  1.1688240766525269  and  1.082602620124817\n",
      "total error:  2.9948954582214355\n",
      "training error:  0.7339538931846619  and  1.207921028137207  and  1.0072643756866455\n",
      "total error:  2.9491392970085144\n",
      "training error:  0.7368611097335815  and  1.16302490234375  and  1.0040736198425293\n",
      "total error:  2.903959631919861\n",
      "training error:  0.7255910634994507  and  1.2128723859786987  and  0.9943807721138\n",
      "total error:  2.9328442215919495\n",
      "training error:  0.7260303497314453  and  1.2102450132369995  and  1.0817515850067139\n",
      "total error:  3.0180269479751587\n",
      "training error:  0.7712103128433228  and  1.2705821990966797  and  1.107362151145935\n",
      "total error:  3.1491546630859375\n",
      "training error:  0.7646399140357971  and  1.2282452583312988  and  1.0361082553863525\n",
      "total error:  3.0289934277534485\n",
      "training error:  0.7358725070953369  and  1.2126203775405884  and  0.9945857524871826\n",
      "total error:  2.943078637123108\n",
      "training error:  0.7575994729995728  and  1.2027523517608643  and  1.0459353923797607\n",
      "total error:  3.0062872171401978\n",
      "training error:  0.7754589319229126  and  1.2150745391845703  and  1.0209213495254517\n",
      "total error:  3.0114548206329346\n",
      "training error:  0.7386552095413208  and  1.1674113273620605  and  0.9988687038421631\n",
      "total error:  2.9049352407455444\n",
      "training error:  0.7289631962776184  and  1.1883411407470703  and  0.9963197708129883\n",
      "total error:  2.913624107837677\n",
      "training error:  0.7168681621551514  and  1.208446979522705  and  1.0052920579910278\n",
      "total error:  2.9306071996688843\n",
      "training error:  0.7311055660247803  and  1.163019061088562  and  0.9956139922142029\n",
      "total error:  2.889738619327545\n",
      "training error:  0.7416698932647705  and  1.2480989694595337  and  1.0422134399414062\n",
      "total error:  3.0319823026657104\n",
      "training error:  0.7619493007659912  and  1.2250697612762451  and  1.0247390270233154\n",
      "total error:  3.0117580890655518\n",
      "training error:  0.7839257717132568  and  1.2978358268737793  and  1.0690038204193115\n",
      "total error:  3.1507654190063477\n",
      "training error:  0.7408900260925293  and  1.2450222969055176  and  1.016234278678894\n",
      "total error:  3.002146601676941\n",
      "training error:  0.7464409470558167  and  1.2309632301330566  and  1.1280670166015625\n",
      "total error:  3.105471193790436\n",
      "training error:  0.7442210912704468  and  1.2080626487731934  and  1.010786771774292\n",
      "total error:  2.963070511817932\n",
      "training error:  0.7450695037841797  and  1.1966177225112915  and  0.98210209608078\n",
      "total error:  2.923789322376251\n",
      "training error:  0.7542283535003662  and  1.230050802230835  and  1.0249781608581543\n",
      "total error:  3.0092573165893555\n",
      "training error:  0.7571128606796265  and  1.1931123733520508  and  1.0028862953186035\n",
      "total error:  2.9531115293502808\n",
      "training error:  0.7361847162246704  and  1.2131133079528809  and  1.039576530456543\n",
      "total error:  2.9888745546340942\n",
      "training error:  0.7525191307067871  and  1.2118422985076904  and  0.9798428416252136\n",
      "total error:  2.944204270839691\n",
      "training error:  0.7666647434234619  and  1.243944764137268  and  1.0624001026153564\n",
      "total error:  3.0730096101760864\n",
      "training error:  0.7510720491409302  and  1.1540734767913818  and  1.0100305080413818\n",
      "total error:  2.915176033973694\n",
      "training error:  0.7324110865592957  and  1.2370351552963257  and  1.0429024696350098\n",
      "total error:  3.012348711490631\n",
      "training error:  0.7394447326660156  and  1.1971583366394043  and  1.0033384561538696\n",
      "total error:  2.9399415254592896\n",
      "training error:  0.741788387298584  and  1.2628291845321655  and  1.08997642993927\n",
      "total error:  3.0945940017700195\n",
      "training error:  0.7370161414146423  and  1.211101770401001  and  1.0352885723114014\n",
      "total error:  2.9834064841270447\n",
      "training error:  0.7653354406356812  and  1.372860074043274  and  1.1201815605163574\n",
      "total error:  3.2583770751953125\n",
      "training error:  0.7554638385772705  and  1.226765513420105  and  1.066161036491394\n",
      "total error:  3.0483903884887695\n",
      "training error:  0.7300037145614624  and  1.2377070188522339  and  1.0089762210845947\n",
      "total error:  2.976686954498291\n",
      "training error:  0.7267538905143738  and  1.230311632156372  and  1.0904290676116943\n",
      "total error:  3.04749459028244\n",
      "training error:  0.7484190464019775  and  1.2088465690612793  and  1.0005384683609009\n",
      "total error:  2.9578040838241577\n",
      "training error:  0.7228056192398071  and  1.1835014820098877  and  0.9905837774276733\n",
      "total error:  2.896890878677368\n",
      "training error:  0.7315016388893127  and  1.1452516317367554  and  0.9838241934776306\n",
      "total error:  2.8605774641036987\n",
      "training error:  0.756263256072998  and  1.2096410989761353  and  1.0124390125274658\n",
      "total error:  2.978343367576599\n",
      "training error:  0.7556806802749634  and  1.188387155532837  and  0.947235643863678\n",
      "total error:  2.8913034796714783\n",
      "training error:  0.7231290340423584  and  1.235379695892334  and  1.0100165605545044\n",
      "total error:  2.9685252904891968\n",
      "training error:  0.7672594785690308  and  1.1924002170562744  and  0.9761925935745239\n",
      "total error:  2.935852289199829\n",
      "training error:  0.7286034822463989  and  1.4115798473358154  and  1.0508438348770142\n",
      "total error:  3.1910271644592285\n",
      "training error:  0.7406630516052246  and  1.2988903522491455  and  1.0161491632461548\n",
      "total error:  3.055702567100525\n",
      "training error:  0.7399153709411621  and  1.172827124595642  and  1.0380247831344604\n",
      "total error:  2.9507672786712646\n",
      "training error:  0.733310341835022  and  1.180952548980713  and  1.0759440660476685\n",
      "total error:  2.9902069568634033\n",
      "training error:  0.7176737785339355  and  1.1438770294189453  and  0.9462252259254456\n",
      "total error:  2.8077760338783264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7438594102859497  and  1.2121875286102295  and  1.026821494102478\n",
      "total error:  2.9828684329986572\n",
      "training error:  0.7304329872131348  and  1.1586649417877197  and  0.9730952382087708\n",
      "total error:  2.8621931672096252\n",
      "training error:  0.7272278070449829  and  1.2213084697723389  and  1.0414700508117676\n",
      "total error:  2.9900063276290894\n",
      "training error:  0.7256774306297302  and  1.1587879657745361  and  1.0246621370315552\n",
      "total error:  2.9091275334358215\n",
      "training error:  0.7344782948493958  and  1.2076518535614014  and  1.0328872203826904\n",
      "total error:  2.9750173687934875\n",
      "training error:  0.7326930165290833  and  1.3814903497695923  and  1.1198265552520752\n",
      "total error:  3.2340099215507507\n",
      "training error:  0.7736482620239258  and  1.2200469970703125  and  0.9886006116867065\n",
      "total error:  2.982295870780945\n",
      "training error:  0.7292425632476807  and  1.2961684465408325  and  0.9719806909561157\n",
      "total error:  2.997391700744629\n",
      "training error:  0.7135334610939026  and  1.1651252508163452  and  1.0071176290512085\n",
      "total error:  2.8857763409614563\n",
      "training error:  0.7464155554771423  and  1.2000463008880615  and  0.9832187294960022\n",
      "total error:  2.929680585861206\n",
      "training error:  0.7343851327896118  and  1.2167062759399414  and  1.0134929418563843\n",
      "total error:  2.9645843505859375\n",
      "training error:  0.7389270663261414  and  1.414375901222229  and  1.0432608127593994\n",
      "total error:  3.1965637803077698\n",
      "training error:  0.7321063280105591  and  1.1754655838012695  and  0.9775064587593079\n",
      "total error:  2.8850783705711365\n",
      "training error:  0.7106931209564209  and  1.2015330791473389  and  1.0462228059768677\n",
      "total error:  2.9584490060806274\n",
      "training error:  0.741955041885376  and  1.2071201801300049  and  1.0092413425445557\n",
      "total error:  2.9583165645599365\n",
      "training error:  0.7303646206855774  and  1.2283557653427124  and  1.0106825828552246\n",
      "total error:  2.9694029688835144\n",
      "training error:  0.7434860467910767  and  1.182607889175415  and  0.9967522025108337\n",
      "total error:  2.9228461384773254\n",
      "training error:  0.7261104583740234  and  1.1966023445129395  and  1.0894606113433838\n",
      "total error:  3.0121734142303467\n",
      "training error:  0.7272948622703552  and  1.1461551189422607  and  0.9972806572914124\n",
      "total error:  2.8707306385040283\n",
      "training error:  0.7427459955215454  and  1.1882805824279785  and  0.9733010530471802\n",
      "total error:  2.904327630996704\n",
      "training error:  0.7104419469833374  and  1.1504511833190918  and  1.0596061944961548\n",
      "total error:  2.920499324798584\n",
      "training error:  0.7478506565093994  and  1.1970767974853516  and  1.007470726966858\n",
      "total error:  2.952398180961609\n",
      "training error:  0.7520982027053833  and  1.3608819246292114  and  1.0947597026824951\n",
      "total error:  3.20773983001709\n",
      "training error:  0.7227049469947815  and  1.153531551361084  and  0.9764763712882996\n",
      "total error:  2.852712869644165\n",
      "training error:  0.7224411964416504  and  1.1781930923461914  and  0.937886118888855\n",
      "total error:  2.8385204076766968\n",
      "training error:  0.722069263458252  and  1.1975393295288086  and  1.0142078399658203\n",
      "total error:  2.933816432952881\n",
      "training error:  0.7217673063278198  and  1.1817071437835693  and  1.0157644748687744\n",
      "total error:  2.9192389249801636\n",
      "training error:  0.7449527978897095  and  1.3601971864700317  and  0.9852412939071655\n",
      "total error:  3.0903912782669067\n",
      "training error:  0.7198491096496582  and  1.1822823286056519  and  1.013714075088501\n",
      "total error:  2.915845513343811\n",
      "training error:  0.7409595251083374  and  1.1765425205230713  and  1.0027527809143066\n",
      "total error:  2.9202548265457153\n",
      "training error:  0.7348017692565918  and  1.5003900527954102  and  0.9935257434844971\n",
      "total error:  3.228717565536499\n",
      "training error:  0.7389920949935913  and  1.167812466621399  and  0.9913265705108643\n",
      "total error:  2.8981311321258545\n",
      "training error:  0.7151951789855957  and  1.1862647533416748  and  1.0333681106567383\n",
      "total error:  2.934828042984009\n",
      "training error:  0.7546470165252686  and  1.2074172496795654  and  1.1090352535247803\n",
      "total error:  3.0710995197296143\n",
      "training error:  0.7389737367630005  and  1.1984608173370361  and  1.066158413887024\n",
      "total error:  3.0035929679870605\n",
      "training error:  0.7224817872047424  and  1.2210185527801514  and  0.991783857345581\n",
      "total error:  2.935284197330475\n",
      "training error:  0.7269043922424316  and  1.2385224103927612  and  1.0057861804962158\n",
      "total error:  2.9712129831314087\n",
      "training error:  0.699192464351654  and  1.1705399751663208  and  0.9946122169494629\n",
      "total error:  2.8643446564674377\n",
      "training error:  0.758576512336731  and  1.2308361530303955  and  1.0090982913970947\n",
      "total error:  2.998510956764221\n",
      "training error:  0.7397646903991699  and  1.2220170497894287  and  1.015392780303955\n",
      "total error:  2.9771745204925537\n",
      "training error:  0.7254892587661743  and  1.1765693426132202  and  1.0104026794433594\n",
      "total error:  2.912461280822754\n",
      "training error:  0.7356566190719604  and  1.1811325550079346  and  1.018926739692688\n",
      "total error:  2.935715913772583\n",
      "training error:  0.7136732339859009  and  1.1865830421447754  and  0.9662591218948364\n",
      "total error:  2.8665153980255127\n",
      "training error:  0.7273081541061401  and  1.216322422027588  and  0.962186336517334\n",
      "total error:  2.905816912651062\n",
      "training error:  0.7649649977684021  and  1.314911127090454  and  0.9504074454307556\n",
      "total error:  3.030283570289612\n",
      "training error:  0.724648118019104  and  1.2100391387939453  and  0.9421517848968506\n",
      "total error:  2.8768390417099\n",
      "training error:  0.7354212999343872  and  1.1861519813537598  and  1.0242836475372314\n",
      "total error:  2.9458569288253784\n",
      "training error:  0.6951349377632141  and  1.166318416595459  and  0.9534371495246887\n",
      "total error:  2.814890503883362\n",
      "training error:  0.7214493155479431  and  1.1908694505691528  and  0.9858096837997437\n",
      "total error:  2.8981284499168396\n",
      "training error:  0.7313214540481567  and  1.222327470779419  and  1.0381284952163696\n",
      "total error:  2.9917774200439453\n",
      "training error:  0.7296075820922852  and  1.1931841373443604  and  1.0526220798492432\n",
      "total error:  2.9754137992858887\n",
      "training error:  0.6998869180679321  and  1.1409790515899658  and  0.9931341409683228\n",
      "total error:  2.8340001106262207\n",
      "training error:  0.7318421006202698  and  1.2178261280059814  and  0.980270266532898\n",
      "total error:  2.929938495159149\n",
      "training error:  0.7201159000396729  and  1.161024808883667  and  1.0187588930130005\n",
      "total error:  2.8998996019363403\n",
      "training error:  0.7478100061416626  and  1.3583216667175293  and  1.085668683052063\n",
      "total error:  3.191800355911255\n",
      "training error:  0.7053444385528564  and  1.163105845451355  and  0.9918749928474426\n",
      "total error:  2.860325276851654\n",
      "training error:  0.7160840034484863  and  1.2000083923339844  and  1.0139963626861572\n",
      "total error:  2.930088758468628\n",
      "training error:  0.7143370509147644  and  1.285443663597107  and  1.0841732025146484\n",
      "total error:  3.0839539170265198\n",
      "training error:  0.7083396315574646  and  1.1667208671569824  and  0.9756588339805603\n",
      "total error:  2.8507193326950073\n",
      "training error:  0.7014904618263245  and  1.19427490234375  and  0.9580700993537903\n",
      "total error:  2.8538354635238647\n",
      "training error:  0.7115210294723511  and  1.2441420555114746  and  1.0191147327423096\n",
      "total error:  2.9747778177261353\n",
      "training error:  0.7146834135055542  and  1.1767578125  and  1.001842737197876\n",
      "total error:  2.89328396320343\n",
      "training error:  0.7260040044784546  and  1.1771124601364136  and  1.0576958656311035\n",
      "total error:  2.9608123302459717\n",
      "training error:  0.7199187278747559  and  1.3485078811645508  and  1.0214574337005615\n",
      "total error:  3.089884042739868\n",
      "training error:  0.7225938439369202  and  1.259502649307251  and  1.0141698122024536\n",
      "total error:  2.9962663054466248\n",
      "training error:  0.7246685028076172  and  1.1341502666473389  and  0.978652834892273\n",
      "total error:  2.837471604347229\n",
      "training error:  0.7295550107955933  and  1.1823188066482544  and  1.033712387084961\n",
      "total error:  2.9455862045288086\n",
      "training error:  0.7114171981811523  and  1.198581337928772  and  1.0449516773223877\n",
      "total error:  2.954950213432312\n",
      "training error:  0.7013975381851196  and  1.1817377805709839  and  0.9857685565948486\n",
      "total error:  2.868903875350952\n",
      "training error:  0.7180405259132385  and  1.1700845956802368  and  1.0489510297775269\n",
      "total error:  2.937076151371002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6946917772293091  and  1.2223851680755615  and  1.0250508785247803\n",
      "total error:  2.942127823829651\n",
      "training error:  0.7258210778236389  and  1.2276211977005005  and  0.9990689158439636\n",
      "total error:  2.952511191368103\n",
      "training error:  0.7118434906005859  and  1.209136962890625  and  1.0192358493804932\n",
      "total error:  2.940216302871704\n",
      "training error:  0.7030588388442993  and  1.1840662956237793  and  1.0550652742385864\n",
      "total error:  2.942190408706665\n",
      "training error:  0.7270798683166504  and  1.3827831745147705  and  1.0190868377685547\n",
      "total error:  3.1289498805999756\n",
      "training error:  0.7056884765625  and  1.1433446407318115  and  1.008189082145691\n",
      "total error:  2.8572221994400024\n",
      "training error:  0.7047346830368042  and  1.1921842098236084  and  1.1119589805603027\n",
      "total error:  3.0088778734207153\n",
      "training error:  0.7222132086753845  and  1.1842172145843506  and  1.0006983280181885\n",
      "total error:  2.9071287512779236\n",
      "training error:  0.6972702145576477  and  1.1579103469848633  and  0.9844914674758911\n",
      "total error:  2.839672029018402\n",
      "training error:  0.6936821341514587  and  1.173275351524353  and  0.9465262293815613\n",
      "total error:  2.813483715057373\n",
      "training error:  0.6873378753662109  and  1.1466758251190186  and  1.0039234161376953\n",
      "total error:  2.837937116622925\n",
      "training error:  0.7336865663528442  and  1.4602423906326294  and  0.9895872473716736\n",
      "total error:  3.183516204357147\n",
      "training error:  0.7171369791030884  and  1.1865816116333008  and  1.0480625629425049\n",
      "total error:  2.951781153678894\n",
      "training error:  0.7745953798294067  and  1.340941309928894  and  1.2122474908828735\n",
      "total error:  3.3277841806411743\n",
      "training error:  0.6767396926879883  and  1.2012664079666138  and  1.0241811275482178\n",
      "total error:  2.90218722820282\n",
      "training error:  0.7058139443397522  and  1.243391752243042  and  1.039153814315796\n",
      "total error:  2.98835951089859\n",
      "training error:  0.7245433330535889  and  1.1926825046539307  and  1.0610415935516357\n",
      "total error:  2.9782674312591553\n",
      "training error:  0.73507159948349  and  1.1740028858184814  and  0.9591902494430542\n",
      "total error:  2.8682647347450256\n",
      "training error:  0.7094055414199829  and  1.233826994895935  and  0.9694151878356934\n",
      "total error:  2.9126477241516113\n",
      "training error:  0.7041802406311035  and  1.1646573543548584  and  0.9550137519836426\n",
      "total error:  2.8238513469696045\n",
      "training error:  0.7181100249290466  and  1.2119755744934082  and  0.9562506675720215\n",
      "total error:  2.8863362669944763\n",
      "training error:  0.6915099620819092  and  1.2495315074920654  and  0.9916555285453796\n",
      "total error:  2.9326969981193542\n",
      "training error:  0.7013424038887024  and  1.174372911453247  and  0.9527978897094727\n",
      "total error:  2.828513205051422\n",
      "training error:  0.7428771257400513  and  1.3642675876617432  and  0.9624924659729004\n",
      "total error:  3.069637179374695\n",
      "training error:  0.6988502740859985  and  1.1426913738250732  and  0.9581603407859802\n",
      "total error:  2.799701988697052\n",
      "training error:  0.7247561812400818  and  1.1775161027908325  and  1.0324374437332153\n",
      "total error:  2.9347097277641296\n",
      "training error:  0.712715744972229  and  1.1551167964935303  and  0.9953317642211914\n",
      "total error:  2.8631643056869507\n",
      "training error:  0.694168210029602  and  1.1818578243255615  and  1.0242693424224854\n",
      "total error:  2.900295376777649\n",
      "training error:  0.7268022298812866  and  1.256371259689331  and  1.0204379558563232\n",
      "total error:  3.003611445426941\n",
      "training error:  0.6951457858085632  and  1.2180991172790527  and  0.9823350310325623\n",
      "total error:  2.8955799341201782\n",
      "training error:  0.726953387260437  and  1.2097790241241455  and  1.035783052444458\n",
      "total error:  2.9725154638290405\n",
      "training error:  0.7243294715881348  and  1.207094669342041  and  0.9475995898246765\n",
      "total error:  2.8790237307548523\n",
      "training error:  0.7682967782020569  and  1.2004621028900146  and  1.0104612112045288\n",
      "total error:  2.9792200922966003\n",
      "training error:  0.6982988119125366  and  1.1510791778564453  and  1.0363054275512695\n",
      "total error:  2.8856834173202515\n",
      "training error:  0.6943950653076172  and  1.2197160720825195  and  1.0384225845336914\n",
      "total error:  2.952533721923828\n",
      "training error:  0.721556544303894  and  1.1760475635528564  and  0.9657595753669739\n",
      "total error:  2.8633636832237244\n",
      "training error:  0.7173922061920166  and  1.219097375869751  and  0.9912720918655396\n",
      "total error:  2.927761673927307\n",
      "training error:  0.6963268518447876  and  1.155412197113037  and  1.060669183731079\n",
      "total error:  2.912408232688904\n",
      "training error:  0.7273795008659363  and  1.2184181213378906  and  0.9955484867095947\n",
      "total error:  2.9413461089134216\n",
      "training error:  0.7047615051269531  and  1.137520432472229  and  0.9422921538352966\n",
      "total error:  2.7845740914344788\n",
      "training error:  0.7040461301803589  and  1.1861833333969116  and  1.023932933807373\n",
      "total error:  2.9141623973846436\n",
      "training error:  0.7016286849975586  and  1.1665688753128052  and  1.0305304527282715\n",
      "total error:  2.8987280130386353\n",
      "training error:  0.6920744180679321  and  1.1541295051574707  and  0.9975049495697021\n",
      "total error:  2.843708872795105\n",
      "training error:  0.6964457035064697  and  1.1629718542099  and  1.0058722496032715\n",
      "total error:  2.865289807319641\n",
      "training error:  0.6835444569587708  and  1.1651780605316162  and  0.9935997724533081\n",
      "total error:  2.842322289943695\n",
      "training error:  0.6959589719772339  and  1.1608102321624756  and  0.9645082950592041\n",
      "total error:  2.8212774991989136\n",
      "training error:  0.7154331803321838  and  1.169312834739685  and  1.002906084060669\n",
      "total error:  2.887652099132538\n",
      "training error:  0.6933594942092896  and  1.1593586206436157  and  1.0030829906463623\n",
      "total error:  2.8558011054992676\n",
      "training error:  0.7194530367851257  and  1.2527413368225098  and  0.9999895691871643\n",
      "total error:  2.9721839427948\n",
      "training error:  0.7078826427459717  and  1.1882998943328857  and  1.1425979137420654\n",
      "total error:  3.038780450820923\n",
      "training error:  0.6931570768356323  and  1.2649002075195312  and  1.0357614755630493\n",
      "total error:  2.993818759918213\n",
      "training error:  0.6976674795150757  and  1.2112228870391846  and  0.9774042963981628\n",
      "total error:  2.886294662952423\n",
      "training error:  0.7187072038650513  and  1.2256499528884888  and  1.1383408308029175\n",
      "total error:  3.0826979875564575\n",
      "training error:  0.7237818837165833  and  1.179335117340088  and  1.0350966453552246\n",
      "total error:  2.9382136464118958\n",
      "training error:  0.731735110282898  and  1.2691876888275146  and  1.0435187816619873\n",
      "total error:  3.0444415807724\n",
      "training error:  0.7063481211662292  and  1.3187423944473267  and  0.9833545684814453\n",
      "total error:  3.008445084095001\n",
      "training error:  0.6927971839904785  and  1.1478207111358643  and  1.016660451889038\n",
      "total error:  2.857278347015381\n",
      "training error:  0.6800627112388611  and  1.164450764656067  and  0.9406517148017883\n",
      "total error:  2.7851651906967163\n",
      "training error:  0.6723450422286987  and  1.168421983718872  and  1.0160045623779297\n",
      "total error:  2.8567715883255005\n",
      "training error:  0.7039977312088013  and  1.140548586845398  and  0.9878909587860107\n",
      "total error:  2.83243727684021\n",
      "training error:  0.7150330543518066  and  1.4229052066802979  and  1.0009913444519043\n",
      "total error:  3.138929605484009\n",
      "training error:  0.7120816707611084  and  1.1913775205612183  and  1.080923080444336\n",
      "total error:  2.9843822717666626\n",
      "training error:  0.6939835548400879  and  1.1364556550979614  and  1.0149571895599365\n",
      "total error:  2.845396399497986\n",
      "training error:  0.6796554327011108  and  1.14473295211792  and  1.026909589767456\n",
      "total error:  2.851297974586487\n",
      "training error:  0.6797376871109009  and  1.1431585550308228  and  0.9724645614624023\n",
      "total error:  2.795360803604126\n",
      "training error:  0.6958001852035522  and  1.16909921169281  and  0.9883402585983276\n",
      "total error:  2.85323965549469\n",
      "training error:  0.7106072306632996  and  1.2506370544433594  and  0.9965763092041016\n",
      "total error:  2.9578205943107605\n",
      "training error:  0.6819310188293457  and  1.142603874206543  and  1.0307807922363281\n",
      "total error:  2.855315685272217\n",
      "training error:  0.7367686033248901  and  1.3266286849975586  and  0.9549275040626526\n",
      "total error:  3.0183247923851013\n",
      "training error:  0.7095047235488892  and  1.1680024862289429  and  1.0148173570632935\n",
      "total error:  2.8923245668411255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.688023567199707  and  1.1463347673416138  and  0.9778316020965576\n",
      "total error:  2.8121899366378784\n",
      "training error:  0.7300137281417847  and  1.4611005783081055  and  1.0242273807525635\n",
      "total error:  3.2153416872024536\n",
      "training error:  0.6996015310287476  and  1.3157672882080078  and  0.988591730594635\n",
      "total error:  3.0039605498313904\n",
      "training error:  0.6768760085105896  and  1.1426864862442017  and  0.9800878167152405\n",
      "total error:  2.7996503114700317\n",
      "training error:  0.7215583324432373  and  1.2063705921173096  and  0.9987204670906067\n",
      "total error:  2.9266493916511536\n",
      "training error:  0.6894087791442871  and  1.1336733102798462  and  1.015217661857605\n",
      "total error:  2.8382997512817383\n",
      "training error:  0.6783287525177002  and  1.1728240251541138  and  1.02507483959198\n",
      "total error:  2.876227617263794\n",
      "training error:  0.716657280921936  and  1.195887565612793  and  1.0222958326339722\n",
      "total error:  2.934840679168701\n",
      "training error:  0.7094314098358154  and  1.2019894123077393  and  0.999038577079773\n",
      "total error:  2.9104593992233276\n",
      "training error:  0.7060117721557617  and  1.1468613147735596  and  1.001328706741333\n",
      "total error:  2.8542017936706543\n",
      "training error:  0.7011579871177673  and  1.1456036567687988  and  0.9606566429138184\n",
      "total error:  2.8074182868003845\n",
      "training error:  0.696839451789856  and  1.1839215755462646  and  0.9832532405853271\n",
      "total error:  2.8640142679214478\n",
      "training error:  0.7034573554992676  and  1.1638432741165161  and  0.9682416915893555\n",
      "total error:  2.835542321205139\n",
      "training error:  0.6924794912338257  and  1.1664855480194092  and  1.0755568742752075\n",
      "total error:  2.9345219135284424\n",
      "training error:  0.6939219236373901  and  1.2097346782684326  and  0.9658297300338745\n",
      "total error:  2.8694863319396973\n",
      "training error:  0.6779456734657288  and  1.1631510257720947  and  0.945239782333374\n",
      "total error:  2.7863364815711975\n",
      "training error:  0.704355001449585  and  1.2073991298675537  and  0.9984275102615356\n",
      "total error:  2.9101816415786743\n",
      "training error:  0.6864092946052551  and  1.1707336902618408  and  1.0127577781677246\n",
      "total error:  2.8699007630348206\n",
      "training error:  0.7026581764221191  and  1.1656336784362793  and  0.9387005567550659\n",
      "total error:  2.8069924116134644\n",
      "training error:  0.6863613128662109  and  1.1371827125549316  and  1.012552261352539\n",
      "total error:  2.8360962867736816\n",
      "training error:  0.6902480125427246  and  1.312455654144287  and  1.1044056415557861\n",
      "total error:  3.107109308242798\n",
      "training error:  0.6890212297439575  and  1.144300937652588  and  0.9607275724411011\n",
      "total error:  2.7940497398376465\n",
      "training error:  0.6817033290863037  and  1.1254888772964478  and  0.9260440468788147\n",
      "total error:  2.733236253261566\n",
      "training error:  0.6798995733261108  and  1.1889121532440186  and  0.9976774454116821\n",
      "total error:  2.8664891719818115\n",
      "training error:  0.703926682472229  and  1.294161081314087  and  0.9814349412918091\n",
      "total error:  2.979522705078125\n",
      "training error:  0.676318347454071  and  1.1411185264587402  and  0.9675372838973999\n",
      "total error:  2.784974157810211\n",
      "training error:  0.7002174854278564  and  1.159155011177063  and  0.9987547397613525\n",
      "total error:  2.858127236366272\n",
      "training error:  0.6875452995300293  and  1.2057682275772095  and  1.0183087587356567\n",
      "total error:  2.9116222858428955\n",
      "training error:  0.6747278571128845  and  1.1605819463729858  and  0.9577341675758362\n",
      "total error:  2.7930439710617065\n",
      "training error:  0.6762856841087341  and  1.1592018604278564  and  1.0035375356674194\n",
      "total error:  2.83902508020401\n",
      "training error:  0.7154331803321838  and  1.2156106233596802  and  0.9797142744064331\n",
      "total error:  2.910758078098297\n",
      "training error:  0.7114273905754089  and  1.1317874193191528  and  0.9830424785614014\n",
      "total error:  2.826257288455963\n",
      "training error:  0.69737309217453  and  1.1672608852386475  and  0.960453987121582\n",
      "total error:  2.8250879645347595\n",
      "training error:  0.689118504524231  and  1.2581872940063477  and  0.9802395105361938\n",
      "total error:  2.9275453090667725\n",
      "training error:  0.695663332939148  and  1.1296195983886719  and  0.9540285468101501\n",
      "total error:  2.77931147813797\n",
      "training error:  0.6738728284835815  and  1.1579430103302002  and  0.9854458570480347\n",
      "total error:  2.8172616958618164\n",
      "training error:  0.6842491030693054  and  1.1411166191101074  and  0.9730628132820129\n",
      "total error:  2.798428535461426\n",
      "training error:  0.7093747854232788  and  1.294118881225586  and  1.1466248035430908\n",
      "total error:  3.1501184701919556\n",
      "training error:  0.6690664887428284  and  1.1698153018951416  and  0.9761933088302612\n",
      "total error:  2.815075099468231\n",
      "training error:  0.7270286083221436  and  1.2320036888122559  and  1.0510532855987549\n",
      "total error:  3.0100855827331543\n",
      "training error:  0.6811090707778931  and  1.2653225660324097  and  0.9709002375602722\n",
      "total error:  2.917331874370575\n",
      "training error:  0.687021017074585  and  1.1367424726486206  and  0.9567605257034302\n",
      "total error:  2.7805240154266357\n",
      "training error:  0.7302905321121216  and  1.3943703174591064  and  1.0129203796386719\n",
      "total error:  3.1375812292099\n",
      "training error:  0.6895660161972046  and  1.2323379516601562  and  0.9687498807907104\n",
      "total error:  2.8906538486480713\n",
      "training error:  0.7122685313224792  and  1.1933093070983887  and  0.9512125253677368\n",
      "total error:  2.8567903637886047\n",
      "training error:  0.6969960331916809  and  1.1831679344177246  and  1.0257753133773804\n",
      "total error:  2.905939280986786\n",
      "training error:  0.6867733001708984  and  1.2086198329925537  and  1.0749690532684326\n",
      "total error:  2.9703621864318848\n",
      "training error:  0.7516763806343079  and  1.2309465408325195  and  0.9845385551452637\n",
      "total error:  2.967161476612091\n",
      "training error:  0.6908214688301086  and  1.2615679502487183  and  1.0805622339248657\n",
      "total error:  3.0329516530036926\n",
      "training error:  0.7035881876945496  and  1.1578948497772217  and  0.9645496606826782\n",
      "total error:  2.8260326981544495\n",
      "training error:  0.7034993171691895  and  1.206557035446167  and  0.9631102085113525\n",
      "total error:  2.873166561126709\n",
      "training error:  0.6816378831863403  and  1.1570839881896973  and  0.9984031319618225\n",
      "total error:  2.83712500333786\n",
      "training error:  0.6723461151123047  and  1.1730139255523682  and  0.993311882019043\n",
      "total error:  2.838671922683716\n",
      "training error:  0.6907029151916504  and  1.3111355304718018  and  0.9566340446472168\n",
      "total error:  2.958472490310669\n",
      "training error:  0.6647272109985352  and  1.1260037422180176  and  0.9347517490386963\n",
      "total error:  2.725482702255249\n",
      "training error:  0.7562950849533081  and  1.2839457988739014  and  0.9979366660118103\n",
      "total error:  3.0381775498390198\n",
      "training error:  0.6828195452690125  and  1.1756925582885742  and  0.9288615584373474\n",
      "total error:  2.787373661994934\n",
      "training error:  0.6780357360839844  and  1.1878236532211304  and  1.0004689693450928\n",
      "total error:  2.8663283586502075\n",
      "training error:  0.6893967390060425  and  1.2377824783325195  and  1.0145865678787231\n",
      "total error:  2.941765785217285\n",
      "training error:  0.6876620054244995  and  1.1762146949768066  and  0.9835171103477478\n",
      "total error:  2.847393810749054\n",
      "training error:  0.6730692386627197  and  1.1170052289962769  and  0.9429367184638977\n",
      "total error:  2.7330111861228943\n",
      "training error:  0.6919126510620117  and  1.1364169120788574  and  0.9632354974746704\n",
      "total error:  2.7915650606155396\n",
      "training error:  0.6821106672286987  and  1.143874168395996  and  0.9716378450393677\n",
      "total error:  2.7976226806640625\n",
      "training error:  0.673676609992981  and  1.2453142404556274  and  0.9501715898513794\n",
      "total error:  2.869162440299988\n",
      "training error:  0.6838827133178711  and  1.155799150466919  and  0.9796151518821716\n",
      "total error:  2.8192970156669617\n",
      "training error:  0.6677772998809814  and  1.15596342086792  and  0.9620542526245117\n",
      "total error:  2.785794973373413\n",
      "training error:  0.6690638065338135  and  1.174028754234314  and  0.9928492307662964\n",
      "total error:  2.835941791534424\n",
      "training error:  0.6918034553527832  and  1.2884862422943115  and  1.088196039199829\n",
      "total error:  3.068485736846924\n",
      "training error:  0.6892910599708557  and  1.1772973537445068  and  1.1162642240524292\n",
      "total error:  2.9828526377677917\n",
      "training error:  0.6726218461990356  and  1.2686371803283691  and  0.9358637928962708\n",
      "total error:  2.8771228194236755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6770259141921997  and  1.1733289957046509  and  0.9903952479362488\n",
      "total error:  2.8407501578330994\n",
      "training error:  0.6920128464698792  and  1.1642067432403564  and  1.0188324451446533\n",
      "total error:  2.875052034854889\n",
      "training error:  0.688077986240387  and  1.159777045249939  and  0.9933687448501587\n",
      "total error:  2.8412237763404846\n",
      "training error:  0.6845200061798096  and  1.1717907190322876  and  0.9424605369567871\n",
      "total error:  2.7987712621688843\n",
      "training error:  0.671747088432312  and  1.17996084690094  and  0.9688952565193176\n",
      "total error:  2.8206031918525696\n",
      "training error:  0.6733415722846985  and  1.1936962604522705  and  0.9949229955673218\n",
      "total error:  2.8619608283042908\n",
      "training error:  0.6694693565368652  and  1.12046480178833  and  0.998383641242981\n",
      "total error:  2.7883177995681763\n",
      "training error:  0.6945583820343018  and  1.0972956418991089  and  0.9239950180053711\n",
      "total error:  2.7158490419387817\n",
      "training error:  0.6748124361038208  and  1.2062102556228638  and  0.9843548536300659\n",
      "total error:  2.8653775453567505\n",
      "training error:  0.6664679050445557  and  1.1334517002105713  and  0.9093802571296692\n",
      "total error:  2.709299862384796\n",
      "training error:  0.6635473966598511  and  1.200986385345459  and  0.9778596758842468\n",
      "total error:  2.842393457889557\n",
      "training error:  0.6634212136268616  and  1.2243950366973877  and  1.0495525598526\n",
      "total error:  2.9373688101768494\n",
      "training error:  0.6793584823608398  and  1.1277621984481812  and  0.960253119468689\n",
      "total error:  2.76737380027771\n",
      "training error:  0.7007877230644226  and  1.1891778707504272  and  0.9986602067947388\n",
      "total error:  2.8886258006095886\n",
      "training error:  0.6934143304824829  and  1.1954984664916992  and  0.978395938873291\n",
      "total error:  2.867308735847473\n",
      "training error:  0.6757000088691711  and  1.158664345741272  and  0.9999304413795471\n",
      "total error:  2.8342947959899902\n",
      "training error:  0.6637811660766602  and  1.153602123260498  and  0.9544837474822998\n",
      "total error:  2.771867036819458\n",
      "training error:  0.658500075340271  and  1.1131553649902344  and  0.9232089519500732\n",
      "total error:  2.6948643922805786\n",
      "training error:  0.6630736589431763  and  1.2231839895248413  and  0.996052086353302\n",
      "total error:  2.8823097348213196\n",
      "training error:  0.6794805526733398  and  1.1289745569229126  and  0.9103652238845825\n",
      "total error:  2.718820333480835\n",
      "training error:  0.6959125399589539  and  1.3229361772537231  and  0.9898972511291504\n",
      "total error:  3.0087459683418274\n",
      "training error:  0.6942604780197144  and  1.1777896881103516  and  0.9388219714164734\n",
      "total error:  2.8108721375465393\n",
      "training error:  0.6816293001174927  and  1.179819941520691  and  0.9215625524520874\n",
      "total error:  2.783011794090271\n",
      "training error:  0.6787633895874023  and  1.1787889003753662  and  0.9710549712181091\n",
      "total error:  2.8286072611808777\n",
      "training error:  0.7106961011886597  and  1.27593195438385  and  1.0940477848052979\n",
      "total error:  3.0806758403778076\n",
      "training error:  0.6893350481987  and  1.1556869745254517  and  0.949283242225647\n",
      "total error:  2.7943052649497986\n",
      "training error:  0.6660718321800232  and  1.2649730443954468  and  1.0531922578811646\n",
      "total error:  2.9842371344566345\n",
      "training error:  0.6840859651565552  and  1.1928761005401611  and  0.92479407787323\n",
      "total error:  2.8017561435699463\n",
      "training error:  0.712472140789032  and  1.1801023483276367  and  0.9389506578445435\n",
      "total error:  2.831525146961212\n",
      "training error:  0.6856447458267212  and  1.1698954105377197  and  0.9632040858268738\n",
      "total error:  2.8187442421913147\n",
      "training error:  0.6682316064834595  and  1.2057793140411377  and  0.9763768911361694\n",
      "total error:  2.8503878116607666\n",
      "training error:  0.6858984231948853  and  1.168835163116455  and  1.0229896306991577\n",
      "total error:  2.877723217010498\n",
      "training error:  0.7136892080307007  and  1.2648993730545044  and  0.9885544776916504\n",
      "total error:  2.9671430587768555\n",
      "training error:  0.7292835116386414  and  1.1475234031677246  and  0.9341288208961487\n",
      "total error:  2.8109357357025146\n",
      "training error:  0.6963609457015991  and  1.124753713607788  and  0.892094612121582\n",
      "total error:  2.7132092714309692\n",
      "training error:  0.6594386696815491  and  1.0973219871520996  and  0.9061251878738403\n",
      "total error:  2.662885844707489\n",
      "training error:  0.6617820858955383  and  1.199432134628296  and  0.9145774841308594\n",
      "total error:  2.7757917046546936\n",
      "training error:  0.6753722429275513  and  1.1498719453811646  and  0.9782810211181641\n",
      "total error:  2.80352520942688\n",
      "training error:  0.6629770994186401  and  1.2033475637435913  and  1.0395941734313965\n",
      "total error:  2.905918836593628\n",
      "training error:  0.6674118638038635  and  1.1614718437194824  and  0.9350210428237915\n",
      "total error:  2.7639047503471375\n",
      "training error:  0.6920362710952759  and  1.1985032558441162  and  1.056828260421753\n",
      "total error:  2.947367787361145\n",
      "training error:  0.6750760078430176  and  1.1786651611328125  and  0.9746493101119995\n",
      "total error:  2.8283904790878296\n",
      "training error:  0.6887677907943726  and  1.248222827911377  and  0.9681562185287476\n",
      "total error:  2.905146837234497\n",
      "training error:  0.665951132774353  and  1.1908234357833862  and  1.0069975852966309\n",
      "total error:  2.86377215385437\n",
      "training error:  0.7157520055770874  and  1.229038953781128  and  1.1379122734069824\n",
      "total error:  3.0827032327651978\n",
      "training error:  0.6609855890274048  and  1.3871160745620728  and  1.000278353691101\n",
      "total error:  3.0483800172805786\n",
      "training error:  0.6737619042396545  and  1.1391830444335938  and  0.9614762663841248\n",
      "total error:  2.774421215057373\n",
      "training error:  0.6399609446525574  and  1.1848700046539307  and  0.9101533889770508\n",
      "total error:  2.734984338283539\n",
      "training error:  0.6893310546875  and  1.1630152463912964  and  1.0097713470458984\n",
      "total error:  2.862117648124695\n",
      "training error:  0.7837088108062744  and  1.4990580081939697  and  1.1484967470169067\n",
      "total error:  3.431263566017151\n",
      "training error:  0.6969915628433228  and  1.2544790506362915  and  0.94963538646698\n",
      "total error:  2.9011059999465942\n",
      "training error:  0.6625082492828369  and  1.1535896062850952  and  0.9325881004333496\n",
      "total error:  2.7486859560012817\n",
      "training error:  0.6803432703018188  and  1.1300904750823975  and  0.9365154504776001\n",
      "total error:  2.7469491958618164\n",
      "training error:  0.6698125004768372  and  1.146888256072998  and  0.9563859105110168\n",
      "total error:  2.773086667060852\n",
      "training error:  0.71571946144104  and  1.209642767906189  and  1.0189932584762573\n",
      "total error:  2.9443554878234863\n",
      "training error:  0.6913697123527527  and  1.2706302404403687  and  1.0138137340545654\n",
      "total error:  2.9758136868476868\n",
      "training error:  0.6725686192512512  and  1.2747441530227661  and  0.9918458461761475\n",
      "total error:  2.939158618450165\n",
      "training error:  0.6653506755828857  and  1.148817539215088  and  0.9755614995956421\n",
      "total error:  2.7897297143936157\n",
      "training error:  0.6622416973114014  and  1.1299755573272705  and  0.9557362794876099\n",
      "total error:  2.7479535341262817\n",
      "training error:  0.6831400394439697  and  1.15602707862854  and  0.9581761360168457\n",
      "total error:  2.7973432540893555\n",
      "training error:  0.6795881986618042  and  1.1564055681228638  and  0.9438341856002808\n",
      "total error:  2.7798279523849487\n",
      "training error:  0.6590303182601929  and  1.1316901445388794  and  0.9378681182861328\n",
      "total error:  2.728588581085205\n",
      "training error:  0.6652136445045471  and  1.1104791164398193  and  0.9857558608055115\n",
      "total error:  2.761448621749878\n",
      "training error:  0.6876448392868042  and  1.1525492668151855  and  0.9683545231819153\n",
      "total error:  2.808548629283905\n",
      "training error:  0.6568491458892822  and  1.2754569053649902  and  0.9873968362808228\n",
      "total error:  2.919702887535095\n",
      "training error:  0.6879675984382629  and  1.1877249479293823  and  0.9957916736602783\n",
      "total error:  2.8714842200279236\n",
      "training error:  0.7010288238525391  and  1.4464941024780273  and  1.0003069639205933\n",
      "total error:  3.1478298902511597\n",
      "training error:  0.6555138826370239  and  1.1395022869110107  and  0.9855344891548157\n",
      "total error:  2.7805506587028503\n",
      "training error:  0.672299325466156  and  1.1966888904571533  and  1.0802760124206543\n",
      "total error:  2.9492642283439636\n",
      "training error:  0.647146463394165  and  1.147920846939087  and  0.949435830116272\n",
      "total error:  2.744503140449524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6513776183128357  and  1.128810167312622  and  0.9866204261779785\n",
      "total error:  2.7668082118034363\n",
      "training error:  0.664846658706665  and  1.1464349031448364  and  1.006248950958252\n",
      "total error:  2.8175305128097534\n",
      "training error:  0.6776829361915588  and  1.1242761611938477  and  0.9663394689559937\n",
      "total error:  2.7682985663414\n",
      "training error:  0.6752476096153259  and  1.0993489027023315  and  0.9691086411476135\n",
      "total error:  2.743705153465271\n",
      "training error:  0.6510549783706665  and  1.1217964887619019  and  0.9356532096862793\n",
      "total error:  2.7085046768188477\n",
      "training error:  0.6794750690460205  and  1.227914571762085  and  0.9610147476196289\n",
      "total error:  2.8684043884277344\n",
      "training error:  0.6897066831588745  and  1.1801632642745972  and  0.9516423344612122\n",
      "total error:  2.821512281894684\n",
      "training error:  0.7245989441871643  and  1.1578965187072754  and  1.0359957218170166\n",
      "total error:  2.9184911847114563\n",
      "training error:  0.6518348455429077  and  1.1722500324249268  and  0.9332752823829651\n",
      "total error:  2.7573601603507996\n",
      "training error:  0.650553286075592  and  1.1424005031585693  and  0.9537433385848999\n",
      "total error:  2.7466971278190613\n",
      "training error:  0.6734977960586548  and  1.1403979063034058  and  0.9358600974082947\n",
      "total error:  2.7497557997703552\n",
      "training error:  0.6523422598838806  and  1.2665477991104126  and  0.9912734031677246\n",
      "total error:  2.910163462162018\n",
      "training error:  0.647152304649353  and  1.2608165740966797  and  0.991517186164856\n",
      "total error:  2.8994860649108887\n",
      "training error:  0.666441798210144  and  1.3343219757080078  and  1.1475708484649658\n",
      "total error:  3.1483346223831177\n",
      "training error:  0.6373263597488403  and  1.1296659708023071  and  0.8991746306419373\n",
      "total error:  2.6661669611930847\n",
      "training error:  0.6545662879943848  and  1.1435799598693848  and  0.9006646275520325\n",
      "total error:  2.698810875415802\n",
      "training error:  0.6742970943450928  and  1.1552270650863647  and  1.0085742473602295\n",
      "total error:  2.838098406791687\n",
      "training error:  0.649643063545227  and  1.1153221130371094  and  0.937720537185669\n",
      "total error:  2.7026857137680054\n",
      "training error:  0.6856862306594849  and  1.1589381694793701  and  0.9739524126052856\n",
      "total error:  2.8185768127441406\n",
      "training error:  0.665531575679779  and  1.120652198791504  and  0.9656858444213867\n",
      "total error:  2.7518696188926697\n",
      "training error:  0.6472203135490417  and  1.157250165939331  and  0.9693582057952881\n",
      "total error:  2.773828685283661\n",
      "training error:  0.6546636819839478  and  1.2690112590789795  and  1.0401655435562134\n",
      "total error:  2.9638404846191406\n",
      "training error:  0.671109676361084  and  1.143639326095581  and  0.9160975813865662\n",
      "total error:  2.730846583843231\n",
      "training error:  0.6333751678466797  and  1.184563159942627  and  1.0521025657653809\n",
      "total error:  2.8700408935546875\n",
      "training error:  0.6465999484062195  and  1.1509358882904053  and  0.9760924577713013\n",
      "total error:  2.773628294467926\n",
      "training error:  0.6728662252426147  and  1.1769380569458008  and  0.9532343149185181\n",
      "total error:  2.8030385971069336\n",
      "training error:  0.6834216117858887  and  1.1679054498672485  and  1.0123059749603271\n",
      "total error:  2.8636330366134644\n",
      "training error:  0.6796319484710693  and  1.1215986013412476  and  0.913842499256134\n",
      "total error:  2.715073049068451\n",
      "training error:  0.6499063968658447  and  1.1464923620224  and  0.9911325573921204\n",
      "total error:  2.787531316280365\n",
      "training error:  0.6390805244445801  and  1.1173579692840576  and  0.9703845977783203\n",
      "total error:  2.726823091506958\n",
      "training error:  0.6411762237548828  and  1.1504114866256714  and  1.0342698097229004\n",
      "total error:  2.8258575201034546\n",
      "training error:  0.6745080947875977  and  1.1311652660369873  and  0.9335756897926331\n",
      "total error:  2.739249050617218\n",
      "training error:  0.6894813776016235  and  1.211045265197754  and  0.9564122557640076\n",
      "total error:  2.856938898563385\n",
      "training error:  0.6554760932922363  and  1.121179223060608  and  0.9536057710647583\n",
      "total error:  2.7302610874176025\n",
      "training error:  0.6701458096504211  and  1.1418414115905762  and  0.9927407503128052\n",
      "total error:  2.8047279715538025\n",
      "training error:  0.653226375579834  and  1.209488034248352  and  0.9624704122543335\n",
      "total error:  2.8251848220825195\n",
      "training error:  0.6971874833106995  and  1.23932945728302  and  1.0219051837921143\n",
      "total error:  2.9584221243858337\n",
      "training error:  0.6616930961608887  and  1.1016111373901367  and  0.8845516443252563\n",
      "total error:  2.6478558778762817\n",
      "training error:  0.65931636095047  and  1.1163873672485352  and  0.9304264783859253\n",
      "total error:  2.7061302065849304\n",
      "training error:  0.6464840173721313  and  1.12176513671875  and  0.91972815990448\n",
      "total error:  2.6879773139953613\n",
      "training error:  0.673820972442627  and  1.0972657203674316  and  0.9674524068832397\n",
      "total error:  2.7385390996932983\n",
      "training error:  0.6744973063468933  and  1.2005884647369385  and  0.9458153247833252\n",
      "total error:  2.820901095867157\n",
      "training error:  0.6623461246490479  and  1.177626371383667  and  0.9088284969329834\n",
      "total error:  2.7488009929656982\n",
      "training error:  0.6522966623306274  and  1.2178694009780884  and  0.9487481713294983\n",
      "total error:  2.818914234638214\n",
      "training error:  0.6628080606460571  and  1.2207157611846924  and  0.9456149935722351\n",
      "total error:  2.8291388154029846\n",
      "training error:  0.6411864757537842  and  1.1092147827148438  and  0.9599407315254211\n",
      "total error:  2.710341989994049\n",
      "training error:  0.656198263168335  and  1.172981858253479  and  0.975163459777832\n",
      "total error:  2.804343581199646\n",
      "training error:  0.6667821407318115  and  1.0806355476379395  and  0.9234212040901184\n",
      "total error:  2.6708388924598694\n",
      "training error:  0.6637647151947021  and  1.116309642791748  and  0.9581689834594727\n",
      "total error:  2.738243341445923\n",
      "training error:  0.6993030309677124  and  1.1851253509521484  and  1.0397474765777588\n",
      "total error:  2.9241758584976196\n",
      "training error:  0.7203977108001709  and  1.1700856685638428  and  0.9928491115570068\n",
      "total error:  2.8833324909210205\n",
      "training error:  0.6487287878990173  and  1.1072003841400146  and  0.934282124042511\n",
      "total error:  2.690211296081543\n",
      "training error:  0.6792380809783936  and  1.1454286575317383  and  0.9478176832199097\n",
      "total error:  2.7724844217300415\n",
      "training error:  0.6971685886383057  and  1.2020658254623413  and  1.0266190767288208\n",
      "total error:  2.9258534908294678\n",
      "training error:  0.6621294021606445  and  1.1730319261550903  and  0.9562968015670776\n",
      "total error:  2.7914581298828125\n",
      "training error:  0.6421338319778442  and  1.1300318241119385  and  0.8995599746704102\n",
      "total error:  2.671725630760193\n",
      "training error:  0.6793861389160156  and  1.1190935373306274  and  0.9008355140686035\n",
      "total error:  2.6993151903152466\n",
      "training error:  0.6457608938217163  and  1.1626675128936768  and  0.9981648921966553\n",
      "total error:  2.8065932989120483\n",
      "training error:  0.6465274691581726  and  1.103242039680481  and  0.9815115928649902\n",
      "total error:  2.731281101703644\n",
      "training error:  0.6692194938659668  and  1.2098395824432373  and  1.0516782999038696\n",
      "total error:  2.9307373762130737\n",
      "training error:  0.6893439292907715  and  1.1582432985305786  and  1.0047754049301147\n",
      "total error:  2.852362632751465\n",
      "training error:  0.6944026947021484  and  1.1509225368499756  and  0.9364487528800964\n",
      "total error:  2.7817739844322205\n",
      "training error:  0.7370237708091736  and  1.4363551139831543  and  1.036362886428833\n",
      "total error:  3.209741771221161\n",
      "training error:  0.6474190950393677  and  1.1495232582092285  and  1.008331298828125\n",
      "total error:  2.805273652076721\n",
      "training error:  0.6424674987792969  and  1.1958461999893188  and  0.9304072856903076\n",
      "total error:  2.7687209844589233\n",
      "training error:  0.6554626226425171  and  1.188476800918579  and  0.9351800680160522\n",
      "total error:  2.7791194915771484\n",
      "training error:  0.6317757368087769  and  1.1021939516067505  and  0.9141194224357605\n",
      "total error:  2.648089110851288\n",
      "training error:  0.6685584187507629  and  1.1715707778930664  and  0.9364578723907471\n",
      "total error:  2.7765870690345764\n",
      "training error:  0.6545631885528564  and  1.1540238857269287  and  0.9934664368629456\n",
      "total error:  2.8020535111427307\n",
      "training error:  0.6588632464408875  and  1.099459171295166  and  0.9578568935394287\n",
      "total error:  2.716179311275482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.694537878036499  and  1.2089664936065674  and  0.9544491171836853\n",
      "total error:  2.8579534888267517\n",
      "training error:  0.6677933931350708  and  1.1386878490447998  and  0.9580751657485962\n",
      "total error:  2.764556407928467\n",
      "training error:  0.6512227654457092  and  1.1295266151428223  and  0.9552132487297058\n",
      "total error:  2.7359626293182373\n",
      "training error:  0.7122535109519958  and  1.6065592765808105  and  1.083182454109192\n",
      "total error:  3.4019952416419983\n",
      "training error:  0.6575085520744324  and  1.1011443138122559  and  0.923171877861023\n",
      "total error:  2.681824743747711\n",
      "training error:  0.6476731300354004  and  1.145766258239746  and  0.9302589893341064\n",
      "total error:  2.723698377609253\n",
      "training error:  0.6500458717346191  and  1.1861763000488281  and  0.9381353855133057\n",
      "total error:  2.774357557296753\n",
      "training error:  0.6605310440063477  and  1.1571216583251953  and  0.9549130797386169\n",
      "total error:  2.77256578207016\n",
      "training error:  0.6626479625701904  and  1.1339601278305054  and  0.9807313680648804\n",
      "total error:  2.777339458465576\n",
      "training error:  0.6335047483444214  and  1.1469850540161133  and  1.176620602607727\n",
      "total error:  2.9571104049682617\n",
      "training error:  0.66456139087677  and  1.2317006587982178  and  1.241531252861023\n",
      "total error:  3.1377933025360107\n",
      "training error:  0.6692878603935242  and  1.1634026765823364  and  1.0283164978027344\n",
      "total error:  2.861007034778595\n",
      "training error:  0.6594916582107544  and  1.1605509519577026  and  0.9958623647689819\n",
      "total error:  2.815904974937439\n",
      "training error:  0.6630192995071411  and  1.2526588439941406  and  0.9567352533340454\n",
      "total error:  2.872413396835327\n",
      "training error:  0.638349711894989  and  1.1164015531539917  and  0.9051802754402161\n",
      "total error:  2.6599315404891968\n",
      "training error:  0.6623558402061462  and  1.1297149658203125  and  0.9502840042114258\n",
      "total error:  2.7423548102378845\n",
      "training error:  0.6652911901473999  and  1.254764199256897  and  1.009472370147705\n",
      "total error:  2.929527759552002\n",
      "training error:  0.6527001857757568  and  1.129671335220337  and  0.9923563003540039\n",
      "total error:  2.7747278213500977\n",
      "training error:  0.6397314071655273  and  1.1556634902954102  and  0.9110849499702454\n",
      "total error:  2.706479847431183\n",
      "training error:  0.6518503427505493  and  1.1500394344329834  and  0.9942468404769897\n",
      "total error:  2.7961366176605225\n",
      "training error:  0.6564087271690369  and  1.118586778640747  and  1.0074049234390259\n",
      "total error:  2.78240042924881\n",
      "training error:  0.6246894598007202  and  1.1374733448028564  and  0.9781503677368164\n",
      "total error:  2.740313172340393\n",
      "training error:  0.7016671299934387  and  1.294272541999817  and  1.1333043575286865\n",
      "total error:  3.129244029521942\n",
      "training error:  0.650542140007019  and  1.100939154624939  and  1.0208468437194824\n",
      "total error:  2.7723281383514404\n",
      "training error:  0.657043993473053  and  1.1612586975097656  and  1.001861333847046\n",
      "total error:  2.8201640248298645\n",
      "training error:  0.6320714950561523  and  1.1565073728561401  and  0.9310497641563416\n",
      "total error:  2.719628632068634\n",
      "training error:  0.6919835805892944  and  1.3899918794631958  and  0.9164508581161499\n",
      "total error:  2.99842631816864\n",
      "training error:  0.6536273956298828  and  1.152418851852417  and  0.9400084018707275\n",
      "total error:  2.7460546493530273\n",
      "training error:  0.6399049758911133  and  1.1425524950027466  and  0.8884981870651245\n",
      "total error:  2.6709556579589844\n",
      "training error:  0.6476767063140869  and  1.134627103805542  and  0.9221363067626953\n",
      "total error:  2.704440116882324\n",
      "training error:  0.6291146874427795  and  1.196711778640747  and  0.9432170391082764\n",
      "total error:  2.769043505191803\n",
      "training error:  0.6380584836006165  and  1.1847468614578247  and  0.9388225674629211\n",
      "total error:  2.7616279125213623\n",
      "training error:  0.6446407437324524  and  1.1139965057373047  and  1.0075418949127197\n",
      "total error:  2.766179144382477\n",
      "training error:  0.649756669998169  and  1.1261065006256104  and  0.8960145711898804\n",
      "total error:  2.6718777418136597\n",
      "training error:  0.6275373101234436  and  1.1301988363265991  and  0.9206147193908691\n",
      "total error:  2.678350865840912\n",
      "training error:  0.6431940793991089  and  1.097459316253662  and  0.9623382091522217\n",
      "total error:  2.7029916048049927\n",
      "training error:  0.6603269577026367  and  1.116441011428833  and  0.9716987013816833\n",
      "total error:  2.748466670513153\n",
      "training error:  0.6589784026145935  and  1.1748372316360474  and  0.9313340187072754\n",
      "total error:  2.7651496529579163\n",
      "training error:  0.6371617913246155  and  1.1149427890777588  and  0.9207180738449097\n",
      "total error:  2.672822654247284\n",
      "training error:  0.6842318773269653  and  1.1004340648651123  and  0.8665353655815125\n",
      "total error:  2.65120130777359\n",
      "training error:  0.6319184303283691  and  1.1399171352386475  and  0.9550744295120239\n",
      "total error:  2.7269099950790405\n",
      "training error:  0.6628096103668213  and  1.1830666065216064  and  0.9652884006500244\n",
      "total error:  2.811164617538452\n",
      "training error:  0.6151146292686462  and  1.1203829050064087  and  0.9409404993057251\n",
      "total error:  2.67643803358078\n",
      "training error:  0.6334013938903809  and  1.1644303798675537  and  0.9781957864761353\n",
      "total error:  2.77602756023407\n",
      "training error:  0.6436320543289185  and  1.1070444583892822  and  0.9130768775939941\n",
      "total error:  2.663753390312195\n",
      "training error:  0.6550981998443604  and  1.1069669723510742  and  0.9287659525871277\n",
      "total error:  2.6908311247825623\n",
      "training error:  0.6355347633361816  and  1.1364355087280273  and  0.8629183173179626\n",
      "total error:  2.6348885893821716\n",
      "training error:  0.6365711092948914  and  1.0743746757507324  and  1.013843297958374\n",
      "total error:  2.724789083003998\n",
      "training error:  0.6491520404815674  and  1.1583251953125  and  0.915367603302002\n",
      "total error:  2.7228448390960693\n",
      "training error:  0.652574896812439  and  1.2979323863983154  and  0.927710771560669\n",
      "total error:  2.8782180547714233\n",
      "training error:  0.633004367351532  and  1.1057515144348145  and  0.9164233207702637\n",
      "total error:  2.65517920255661\n",
      "training error:  0.6326528787612915  and  1.14853835105896  and  0.9535366892814636\n",
      "total error:  2.734727919101715\n",
      "training error:  0.6302734613418579  and  1.1583752632141113  and  0.9047409296035767\n",
      "total error:  2.693389654159546\n",
      "training error:  0.6280554533004761  and  1.1348745822906494  and  0.935897707939148\n",
      "total error:  2.6988277435302734\n",
      "training error:  0.6367278099060059  and  1.1317212581634521  and  1.0091073513031006\n",
      "total error:  2.7775564193725586\n",
      "training error:  0.6186362504959106  and  1.1096223592758179  and  0.9293761849403381\n",
      "total error:  2.6576347947120667\n",
      "training error:  0.6324887275695801  and  1.0481895208358765  and  0.8506306409835815\n",
      "total error:  2.531308889389038\n",
      "training error:  0.6691078543663025  and  1.3918044567108154  and  1.0893688201904297\n",
      "total error:  3.1502811312675476\n",
      "training error:  0.6584486365318298  and  1.1336315870285034  and  0.9449214935302734\n",
      "total error:  2.7370017170906067\n",
      "training error:  0.6245623826980591  and  1.1152455806732178  and  0.9363024234771729\n",
      "total error:  2.6761103868484497\n",
      "training error:  0.6512884497642517  and  1.1216338872909546  and  0.901715874671936\n",
      "total error:  2.6746382117271423\n",
      "training error:  0.6533029079437256  and  1.2462332248687744  and  0.9811704158782959\n",
      "total error:  2.880706548690796\n",
      "training error:  0.6675387024879456  and  1.1093356609344482  and  0.9589941501617432\n",
      "total error:  2.735868513584137\n",
      "training error:  0.6314531564712524  and  1.0723580121994019  and  0.9392216205596924\n",
      "total error:  2.6430327892303467\n",
      "training error:  0.6457328796386719  and  1.114621877670288  and  0.961306095123291\n",
      "total error:  2.721660852432251\n",
      "training error:  0.637635350227356  and  1.1188254356384277  and  0.9718849062919617\n",
      "total error:  2.7283456921577454\n",
      "training error:  0.6320364475250244  and  1.1137573719024658  and  0.9161899089813232\n",
      "total error:  2.6619837284088135\n",
      "training error:  0.6184772253036499  and  1.1147191524505615  and  0.9649865627288818\n",
      "total error:  2.6981829404830933\n",
      "training error:  0.623580276966095  and  1.214224100112915  and  0.9567452669143677\n",
      "total error:  2.7945496439933777\n",
      "training error:  0.6264588236808777  and  1.1257452964782715  and  1.004483699798584\n",
      "total error:  2.756687819957733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6454970836639404  and  1.1271156072616577  and  0.8973233103752136\n",
      "total error:  2.6699360013008118\n",
      "training error:  0.6250676512718201  and  1.1160988807678223  and  0.9026027917861938\n",
      "total error:  2.643769323825836\n",
      "training error:  0.6595950722694397  and  1.1290640830993652  and  0.9199907779693604\n",
      "total error:  2.7086499333381653\n",
      "training error:  0.638208270072937  and  1.0928668975830078  and  0.8591485023498535\n",
      "total error:  2.5902236700057983\n",
      "training error:  0.6459855437278748  and  1.1854891777038574  and  0.9941840767860413\n",
      "total error:  2.8256587982177734\n",
      "training error:  0.680299699306488  and  1.1986448764801025  and  0.9656565189361572\n",
      "total error:  2.844601094722748\n",
      "training error:  0.6327516436576843  and  1.1318528652191162  and  0.9232099056243896\n",
      "total error:  2.68781441450119\n",
      "training error:  0.6429406404495239  and  1.097536563873291  and  1.0304994583129883\n",
      "total error:  2.7709766626358032\n",
      "training error:  0.6334123611450195  and  1.1978185176849365  and  1.001535415649414\n",
      "total error:  2.83276629447937\n",
      "training error:  0.6306890249252319  and  1.0712995529174805  and  0.9439319968223572\n",
      "total error:  2.6459205746650696\n",
      "training error:  0.6156525611877441  and  1.1867220401763916  and  0.9492948055267334\n",
      "total error:  2.751669406890869\n",
      "training error:  0.6141000986099243  and  1.1107044219970703  and  0.9160922765731812\n",
      "total error:  2.640896797180176\n",
      "training error:  0.6329058408737183  and  1.1286041736602783  and  0.9288570880889893\n",
      "total error:  2.690367102622986\n",
      "training error:  0.6036579012870789  and  1.0775911808013916  and  0.9634366035461426\n",
      "total error:  2.644685685634613\n",
      "training error:  0.6386200189590454  and  1.3321565389633179  and  1.0056486129760742\n",
      "total error:  2.9764251708984375\n",
      "training error:  0.6312147974967957  and  1.0752708911895752  and  0.855553388595581\n",
      "total error:  2.562039077281952\n",
      "training error:  0.6552911996841431  and  1.1018882989883423  and  0.9126538634300232\n",
      "total error:  2.6698333621025085\n",
      "training error:  0.6348398923873901  and  1.0897736549377441  and  0.8655495643615723\n",
      "total error:  2.5901631116867065\n",
      "training error:  0.6154972910881042  and  1.0730280876159668  and  0.9071488976478577\n",
      "total error:  2.5956742763519287\n",
      "training error:  0.6246619820594788  and  1.1069762706756592  and  0.9064841866493225\n",
      "total error:  2.6381224393844604\n",
      "training error:  0.6223771572113037  and  1.2739986181259155  and  0.9642904996871948\n",
      "total error:  2.860666275024414\n",
      "training error:  0.6611987352371216  and  1.285663366317749  and  1.2171845436096191\n",
      "total error:  3.1640466451644897\n",
      "training error:  0.6216151118278503  and  1.1236754655838013  and  1.0039445161819458\n",
      "total error:  2.7492350935935974\n",
      "training error:  0.622788667678833  and  1.099963903427124  and  0.9320942163467407\n",
      "total error:  2.6548467874526978\n",
      "training error:  0.6342064738273621  and  1.099166750907898  and  0.9475985765457153\n",
      "total error:  2.6809718012809753\n",
      "training error:  0.6482528448104858  and  1.1147176027297974  and  0.8727285861968994\n",
      "total error:  2.6356990337371826\n",
      "training error:  0.653645932674408  and  1.103661298751831  and  0.9235280752182007\n",
      "total error:  2.6808353066444397\n",
      "training error:  0.620123028755188  and  1.243293046951294  and  0.9384458065032959\n",
      "total error:  2.801861882209778\n",
      "training error:  0.6291875839233398  and  1.1106600761413574  and  0.8990954160690308\n",
      "total error:  2.638943076133728\n",
      "training error:  0.6491740942001343  and  1.2227973937988281  and  0.9711127281188965\n",
      "total error:  2.843084216117859\n",
      "training error:  0.6457579731941223  and  1.1119389533996582  and  0.9273843765258789\n",
      "total error:  2.6850813031196594\n",
      "training error:  0.6427513360977173  and  1.1133702993392944  and  0.9461439251899719\n",
      "total error:  2.7022655606269836\n",
      "training error:  0.6391822695732117  and  1.062431812286377  and  0.9028767347335815\n",
      "total error:  2.60449081659317\n",
      "training error:  0.6850929260253906  and  1.124267816543579  and  1.0374091863632202\n",
      "total error:  2.84676992893219\n",
      "training error:  0.6226935982704163  and  1.0768630504608154  and  0.9042965769767761\n",
      "total error:  2.603853225708008\n",
      "training error:  0.6379337310791016  and  1.1244471073150635  and  0.9562098979949951\n",
      "total error:  2.71859073638916\n",
      "training error:  0.6745070815086365  and  1.333253264427185  and  0.9962289333343506\n",
      "total error:  3.003989279270172\n",
      "training error:  0.6398732662200928  and  1.194997787475586  and  1.0766578912734985\n",
      "total error:  2.9115289449691772\n",
      "training error:  0.6389006972312927  and  1.1402277946472168  and  0.9415899515151978\n",
      "total error:  2.7207184433937073\n",
      "training error:  0.6462044715881348  and  1.1360368728637695  and  0.9275767207145691\n",
      "total error:  2.7098180651664734\n",
      "training error:  0.621761679649353  and  1.168769121170044  and  0.9514713883399963\n",
      "total error:  2.7420021891593933\n",
      "training error:  0.6167017817497253  and  1.1668198108673096  and  1.0494946241378784\n",
      "total error:  2.8330162167549133\n",
      "training error:  0.6394744515419006  and  1.1028361320495605  and  0.9389569759368896\n",
      "total error:  2.681267559528351\n",
      "training error:  0.6134145855903625  and  1.0720553398132324  and  0.9353548288345337\n",
      "total error:  2.6208247542381287\n",
      "training error:  0.6287695169448853  and  1.0998508930206299  and  0.9230478405952454\n",
      "total error:  2.6516682505607605\n",
      "training error:  0.6447105407714844  and  1.1479077339172363  and  0.9878121018409729\n",
      "total error:  2.7804303765296936\n",
      "training error:  0.6471087336540222  and  1.122183084487915  and  0.9597783088684082\n",
      "total error:  2.7290701270103455\n",
      "training error:  0.6361989974975586  and  1.043478012084961  and  0.9157141447067261\n",
      "total error:  2.5953911542892456\n",
      "training error:  0.6352275609970093  and  1.1178935766220093  and  0.9510208368301392\n",
      "total error:  2.7041419744491577\n",
      "training error:  0.6525431871414185  and  1.0934768915176392  and  0.893648087978363\n",
      "total error:  2.6396681666374207\n",
      "training error:  0.6384748220443726  and  1.1534085273742676  and  0.9163001775741577\n",
      "total error:  2.708183526992798\n",
      "training error:  0.6482949256896973  and  1.15339195728302  and  0.9572736620903015\n",
      "total error:  2.758960545063019\n",
      "training error:  0.5998140573501587  and  1.1032094955444336  and  0.8858568668365479\n",
      "total error:  2.58888041973114\n",
      "training error:  0.6115437746047974  and  1.0680320262908936  and  1.0218064785003662\n",
      "total error:  2.701382279396057\n",
      "training error:  0.6119722127914429  and  1.073620080947876  and  0.9419525265693665\n",
      "total error:  2.6275448203086853\n",
      "training error:  0.6460339426994324  and  1.1064138412475586  and  0.9002590179443359\n",
      "total error:  2.652706801891327\n",
      "training error:  0.6359298229217529  and  1.1020641326904297  and  0.960400402545929\n",
      "total error:  2.6983943581581116\n",
      "training error:  0.6593624353408813  and  1.2738213539123535  and  1.0127111673355103\n",
      "total error:  2.945894956588745\n",
      "training error:  0.6578963398933411  and  1.1227855682373047  and  0.9098641872406006\n",
      "total error:  2.6905460953712463\n",
      "training error:  0.6256492733955383  and  1.143666386604309  and  0.987876296043396\n",
      "total error:  2.7571919560432434\n",
      "training error:  0.6501928567886353  and  1.104305386543274  and  0.9112503528594971\n",
      "total error:  2.6657485961914062\n",
      "training error:  0.6522731781005859  and  1.106644630432129  and  0.9263029098510742\n",
      "total error:  2.685220718383789\n",
      "training error:  0.6240247488021851  and  1.1319849491119385  and  0.9531074166297913\n",
      "total error:  2.709117114543915\n",
      "training error:  0.6319584250450134  and  1.123667597770691  and  0.9696546792984009\n",
      "total error:  2.7252807021141052\n",
      "training error:  0.6506859660148621  and  1.0866801738739014  and  0.8927117586135864\n",
      "total error:  2.63007789850235\n",
      "training error:  0.6434978246688843  and  1.1537373065948486  and  0.886328935623169\n",
      "total error:  2.683564066886902\n",
      "training error:  0.6393922567367554  and  1.0963786840438843  and  0.9646196365356445\n",
      "total error:  2.700390577316284\n",
      "training error:  0.6313052177429199  and  1.0942506790161133  and  0.8971482515335083\n",
      "total error:  2.6227041482925415\n",
      "training error:  0.6307907700538635  and  1.1080420017242432  and  0.8756726980209351\n",
      "total error:  2.6145054697990417\n",
      "training error:  0.6282681226730347  and  1.1228256225585938  and  0.9325956106185913\n",
      "total error:  2.6836893558502197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6673921346664429  and  1.1232575178146362  and  0.914074718952179\n",
      "total error:  2.704724371433258\n",
      "training error:  0.6227135062217712  and  1.1478989124298096  and  0.9519436359405518\n",
      "total error:  2.7225560545921326\n",
      "training error:  0.6590138673782349  and  1.1166348457336426  and  0.9409980177879333\n",
      "total error:  2.716646730899811\n",
      "training error:  0.6389837265014648  and  1.3119120597839355  and  1.2159051895141602\n",
      "total error:  3.1668009757995605\n",
      "training error:  0.6195387840270996  and  1.124267339706421  and  0.8793807029724121\n",
      "total error:  2.6231868267059326\n",
      "training error:  0.6435539722442627  and  1.1050307750701904  and  0.9356966614723206\n",
      "total error:  2.6842814087867737\n",
      "training error:  0.6181704998016357  and  1.1561086177825928  and  0.9581528306007385\n",
      "total error:  2.732431948184967\n",
      "training error:  0.6179817914962769  and  1.149308443069458  and  0.9177509546279907\n",
      "total error:  2.6850411891937256\n",
      "training error:  0.6433119773864746  and  1.112067699432373  and  0.9489191770553589\n",
      "total error:  2.7042988538742065\n",
      "training error:  0.6102606058120728  and  1.108933925628662  and  0.9122115969657898\n",
      "total error:  2.6314061284065247\n",
      "training error:  0.6347951889038086  and  1.1243404150009155  and  0.9443343877792358\n",
      "total error:  2.70346999168396\n",
      "training error:  0.6275205612182617  and  1.0989484786987305  and  0.8877143859863281\n",
      "total error:  2.6141834259033203\n",
      "training error:  0.6056510806083679  and  1.054616093635559  and  0.9490092992782593\n",
      "total error:  2.6092764735221863\n",
      "training error:  0.6474963426589966  and  1.0846067667007446  and  0.9124334454536438\n",
      "total error:  2.644536554813385\n",
      "training error:  0.6240532398223877  and  1.0691977739334106  and  0.9056203365325928\n",
      "total error:  2.598871350288391\n",
      "training error:  0.6399745941162109  and  1.1169238090515137  and  0.8982661962509155\n",
      "total error:  2.65516459941864\n",
      "training error:  0.633715033531189  and  1.1453851461410522  and  0.9393786787986755\n",
      "total error:  2.7184788584709167\n",
      "training error:  0.6511101126670837  and  1.1274412870407104  and  0.9103957414627075\n",
      "total error:  2.6889471411705017\n",
      "training error:  0.6073641777038574  and  1.1259928941726685  and  0.9485097527503967\n",
      "total error:  2.6818668246269226\n",
      "training error:  0.6096254587173462  and  1.110246181488037  and  0.9065274000167847\n",
      "total error:  2.626399040222168\n",
      "training error:  0.613682746887207  and  1.1103559732437134  and  0.8970857262611389\n",
      "total error:  2.6211244463920593\n",
      "training error:  0.6136006116867065  and  1.0793614387512207  and  0.8791925311088562\n",
      "total error:  2.5721545815467834\n",
      "training error:  0.6069915890693665  and  1.071276307106018  and  0.932225227355957\n",
      "total error:  2.6104931235313416\n",
      "training error:  0.6003276705741882  and  1.0849707126617432  and  0.9057132005691528\n",
      "total error:  2.5910115838050842\n",
      "training error:  0.6117051243782043  and  1.062544584274292  and  0.8972004652023315\n",
      "total error:  2.571450173854828\n",
      "training error:  0.617918848991394  and  1.1483381986618042  and  1.0318427085876465\n",
      "total error:  2.7980997562408447\n",
      "training error:  0.6139765977859497  and  1.0882371664047241  and  0.9727612137794495\n",
      "total error:  2.6749749779701233\n",
      "training error:  0.61883544921875  and  1.107503890991211  and  0.9222938418388367\n",
      "total error:  2.6486331820487976\n",
      "training error:  0.6556493043899536  and  1.1139228343963623  and  1.0450376272201538\n",
      "total error:  2.8146097660064697\n",
      "training error:  0.6451396942138672  and  1.1915199756622314  and  0.9470106363296509\n",
      "total error:  2.7836703062057495\n",
      "training error:  0.6793047785758972  and  1.2131823301315308  and  1.099790334701538\n",
      "total error:  2.992277443408966\n",
      "training error:  0.6342377066612244  and  1.149568796157837  and  0.9090902805328369\n",
      "total error:  2.692896783351898\n",
      "training error:  0.6093946099281311  and  1.078900933265686  and  0.895385205745697\n",
      "total error:  2.583680748939514\n",
      "training error:  0.6304084062576294  and  1.101135492324829  and  0.8900706768035889\n",
      "total error:  2.6216145753860474\n",
      "training error:  0.6207053661346436  and  1.2140766382217407  and  1.0310122966766357\n",
      "total error:  2.86579430103302\n",
      "training error:  0.6566346883773804  and  1.2210729122161865  and  0.985284686088562\n",
      "total error:  2.862992286682129\n",
      "training error:  0.6172581911087036  and  1.1222813129425049  and  0.9139171838760376\n",
      "total error:  2.653456687927246\n",
      "training error:  0.6193392276763916  and  1.1086902618408203  and  0.9321556091308594\n",
      "total error:  2.6601850986480713\n",
      "training error:  0.6443864107131958  and  1.2506343126296997  and  0.9390965700149536\n",
      "total error:  2.834117293357849\n",
      "training error:  0.620567262172699  and  1.1871991157531738  and  0.9685777425765991\n",
      "total error:  2.776344120502472\n",
      "training error:  0.6456507444381714  and  1.115133285522461  and  0.9246121644973755\n",
      "total error:  2.685396194458008\n",
      "training error:  0.7085502743721008  and  1.2501803636550903  and  0.9363216161727905\n",
      "total error:  2.8950522541999817\n",
      "training error:  0.6093469858169556  and  1.0923786163330078  and  0.8907923102378845\n",
      "total error:  2.592517912387848\n",
      "training error:  0.6445310711860657  and  1.0911579132080078  and  0.9267422556877136\n",
      "total error:  2.662431240081787\n",
      "training error:  0.6364611387252808  and  1.0966765880584717  and  0.9333289861679077\n",
      "total error:  2.66646671295166\n",
      "training error:  0.6159216165542603  and  1.1213247776031494  and  0.9106342792510986\n",
      "total error:  2.6478806734085083\n",
      "training error:  0.6551123261451721  and  1.094929575920105  and  0.8972135782241821\n",
      "total error:  2.6472554802894592\n",
      "training error:  0.6293854713439941  and  1.0718767642974854  and  0.9270117282867432\n",
      "total error:  2.6282739639282227\n",
      "training error:  0.6440414786338806  and  1.1654361486434937  and  0.9431954622268677\n",
      "total error:  2.752673089504242\n",
      "training error:  0.6196484565734863  and  1.0911128520965576  and  0.9557700753211975\n",
      "total error:  2.6665313839912415\n",
      "training error:  0.5909351110458374  and  1.0713982582092285  and  0.9104154109954834\n",
      "total error:  2.5727487802505493\n",
      "training error:  0.6232134103775024  and  1.0892869234085083  and  0.8962610960006714\n",
      "total error:  2.608761429786682\n",
      "training error:  0.6082687377929688  and  1.0831599235534668  and  0.8951468467712402\n",
      "total error:  2.586575508117676\n",
      "training error:  0.6345334649085999  and  1.1773004531860352  and  1.0141754150390625\n",
      "total error:  2.8260093331336975\n",
      "training error:  0.6019956469535828  and  1.0961036682128906  and  0.9318735599517822\n",
      "total error:  2.6299728751182556\n",
      "training error:  0.6078066825866699  and  1.0734989643096924  and  0.9599200487136841\n",
      "total error:  2.6412256956100464\n",
      "training error:  0.6741671562194824  and  1.5947771072387695  and  1.0255308151245117\n",
      "total error:  3.2944750785827637\n",
      "training error:  0.6065361499786377  and  1.1027395725250244  and  0.9535328149795532\n",
      "total error:  2.6628085374832153\n",
      "training error:  0.6247019171714783  and  1.0545039176940918  and  0.8510090112686157\n",
      "total error:  2.530214846134186\n",
      "training error:  0.6682071089744568  and  1.1294546127319336  and  0.9951624274253845\n",
      "total error:  2.792824149131775\n",
      "training error:  0.6096599102020264  and  1.0861272811889648  and  0.9037595987319946\n",
      "total error:  2.599546790122986\n",
      "training error:  0.645201563835144  and  1.1968975067138672  and  1.0910425186157227\n",
      "total error:  2.933141589164734\n",
      "training error:  0.6206139922142029  and  1.0877137184143066  and  0.8817570209503174\n",
      "total error:  2.590084731578827\n",
      "training error:  0.6310377717018127  and  1.0590018033981323  and  0.9456005692481995\n",
      "total error:  2.6356401443481445\n",
      "training error:  0.672355055809021  and  1.238814353942871  and  0.8697991371154785\n",
      "total error:  2.7809685468673706\n",
      "training error:  0.6292092800140381  and  1.1286346912384033  and  0.9538799524307251\n",
      "total error:  2.7117239236831665\n",
      "training error:  0.6129109859466553  and  1.111398458480835  and  1.0675184726715088\n",
      "total error:  2.791827917098999\n",
      "training error:  0.7071621417999268  and  1.232048749923706  and  1.0097949504852295\n",
      "total error:  2.9490058422088623\n",
      "training error:  0.6488508582115173  and  1.0981245040893555  and  0.929658055305481\n",
      "total error:  2.6766334176063538\n",
      "training error:  0.6237909197807312  and  1.1344058513641357  and  0.9222334027290344\n",
      "total error:  2.6804301738739014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6174153685569763  and  1.1660767793655396  and  0.9799121618270874\n",
      "total error:  2.7634043097496033\n",
      "training error:  0.6102837324142456  and  1.1702117919921875  and  0.9301859140396118\n",
      "total error:  2.710681438446045\n",
      "training error:  0.6079088449478149  and  1.1288959980010986  and  0.905596911907196\n",
      "total error:  2.6424017548561096\n",
      "training error:  0.6355558037757874  and  1.15547513961792  and  1.1132344007492065\n",
      "total error:  2.904265344142914\n",
      "training error:  0.6277397871017456  and  1.1290069818496704  and  1.0089654922485352\n",
      "total error:  2.765712261199951\n",
      "training error:  0.6334760189056396  and  1.1263059377670288  and  0.9289933443069458\n",
      "total error:  2.6887753009796143\n",
      "training error:  0.6268795728683472  and  1.1003894805908203  and  1.1216068267822266\n",
      "total error:  2.848875880241394\n",
      "training error:  0.6709077954292297  and  1.1544378995895386  and  0.8812581300735474\n",
      "total error:  2.7066038250923157\n",
      "training error:  0.6322187781333923  and  1.0659093856811523  and  0.9068720936775208\n",
      "total error:  2.6050002574920654\n",
      "training error:  0.631987452507019  and  1.0864108800888062  and  0.9075709581375122\n",
      "total error:  2.6259692907333374\n",
      "training error:  0.618537962436676  and  1.0581943988800049  and  0.8768501281738281\n",
      "total error:  2.553582489490509\n",
      "training error:  0.6177765727043152  and  1.081099033355713  and  0.8702840805053711\n",
      "total error:  2.569159686565399\n",
      "training error:  0.6270962953567505  and  1.1177456378936768  and  0.9340827465057373\n",
      "total error:  2.6789246797561646\n",
      "training error:  0.6295650601387024  and  1.1824970245361328  and  0.8940843343734741\n",
      "total error:  2.7061464190483093\n",
      "training error:  0.624174952507019  and  1.1174256801605225  and  0.9288501739501953\n",
      "total error:  2.670450806617737\n",
      "training error:  0.6099211573600769  and  1.1336853504180908  and  0.9437316060066223\n",
      "total error:  2.68733811378479\n",
      "training error:  0.5931347608566284  and  1.1159605979919434  and  0.8965708613395691\n",
      "total error:  2.605666220188141\n",
      "training error:  0.6061998605728149  and  1.0667283535003662  and  0.8780752420425415\n",
      "total error:  2.5510034561157227\n",
      "training error:  0.604500412940979  and  1.0706915855407715  and  0.9478069543838501\n",
      "total error:  2.6229989528656006\n",
      "training error:  0.6121906638145447  and  1.091714859008789  and  0.8639543056488037\n",
      "total error:  2.5678598284721375\n",
      "training error:  0.5790320634841919  and  1.0751161575317383  and  0.9174330830574036\n",
      "total error:  2.5715813040733337\n",
      "training error:  0.6348044872283936  and  1.2092772722244263  and  0.9610927104949951\n",
      "total error:  2.805174469947815\n",
      "training error:  0.6596568822860718  and  1.1320737600326538  and  0.8691092729568481\n",
      "total error:  2.6608399152755737\n",
      "training error:  0.5986871123313904  and  1.0565159320831299  and  0.8562714457511902\n",
      "total error:  2.5114744901657104\n",
      "training error:  0.6007348299026489  and  1.073266863822937  and  0.9375206232070923\n",
      "total error:  2.6115223169326782\n",
      "training error:  0.6469539999961853  and  1.1973158121109009  and  0.9612393975257874\n",
      "total error:  2.8055092096328735\n",
      "training error:  0.6335891485214233  and  1.1050479412078857  and  0.9276147484779358\n",
      "total error:  2.666251838207245\n",
      "training error:  0.6159377694129944  and  1.0572233200073242  and  0.9177736043930054\n",
      "total error:  2.590934693813324\n",
      "training error:  0.610052764415741  and  1.1615605354309082  and  1.0255954265594482\n",
      "total error:  2.7972087264060974\n",
      "training error:  0.5883297324180603  and  1.0495343208312988  and  0.8795949220657349\n",
      "total error:  2.517458975315094\n",
      "training error:  0.6238895654678345  and  1.1094987392425537  and  0.9483675956726074\n",
      "total error:  2.6817559003829956\n",
      "training error:  0.6186326742172241  and  1.0659847259521484  and  0.8818055391311646\n",
      "total error:  2.566422939300537\n",
      "training error:  0.611190915107727  and  1.086970329284668  and  1.0098093748092651\n",
      "total error:  2.70797061920166\n",
      "training error:  0.623736560344696  and  1.076064944267273  and  0.9066250324249268\n",
      "total error:  2.6064265370368958\n",
      "training error:  0.6190884113311768  and  1.1529674530029297  and  0.9639095664024353\n",
      "total error:  2.7359654307365417\n",
      "training error:  0.6223883628845215  and  1.115618109703064  and  0.8611071109771729\n",
      "total error:  2.5991135835647583\n",
      "training error:  0.6785926818847656  and  1.2612311840057373  and  1.0383327007293701\n",
      "total error:  2.978156566619873\n",
      "training error:  0.6289287805557251  and  1.1515052318572998  and  0.9318892955780029\n",
      "total error:  2.712323307991028\n",
      "training error:  0.5899621844291687  and  1.047055959701538  and  0.8648550510406494\n",
      "total error:  2.501873195171356\n",
      "training error:  0.6103586554527283  and  1.0745726823806763  and  0.9088728427886963\n",
      "total error:  2.593804180622101\n",
      "training error:  0.6446715593338013  and  1.1343693733215332  and  0.9224979877471924\n",
      "total error:  2.701538920402527\n",
      "training error:  0.590084433555603  and  1.047629714012146  and  0.9806883335113525\n",
      "total error:  2.6184024810791016\n",
      "training error:  0.6146769523620605  and  1.124380111694336  and  1.0084114074707031\n",
      "total error:  2.7474684715270996\n",
      "training error:  0.609622061252594  and  1.0875225067138672  and  0.9077436923980713\n",
      "total error:  2.6048882603645325\n",
      "training error:  0.6085367202758789  and  1.078294277191162  and  0.944509744644165\n",
      "total error:  2.631340742111206\n",
      "training error:  0.630940318107605  and  1.1074981689453125  and  0.9456835985183716\n",
      "total error:  2.684122085571289\n",
      "training error:  0.5951405167579651  and  1.075364351272583  and  0.8638280630111694\n",
      "total error:  2.5343329310417175\n",
      "training error:  0.649201512336731  and  1.1172585487365723  and  0.9605608582496643\n",
      "total error:  2.7270209193229675\n",
      "training error:  0.6138269901275635  and  1.0952527523040771  and  0.9150868654251099\n",
      "total error:  2.6241666078567505\n",
      "training error:  0.6260920763015747  and  1.1794484853744507  and  1.015805721282959\n",
      "total error:  2.8213462829589844\n",
      "training error:  0.651275634765625  and  1.1603015661239624  and  0.962521493434906\n",
      "total error:  2.7740986943244934\n",
      "training error:  0.6056269407272339  and  1.0414891242980957  and  0.866734504699707\n",
      "total error:  2.5138505697250366\n",
      "training error:  0.6138462424278259  and  1.3118499517440796  and  1.028981328010559\n",
      "total error:  2.9546775221824646\n",
      "training error:  0.6640289425849915  and  1.17138671875  and  1.11198091506958\n",
      "total error:  2.9473965764045715\n",
      "training error:  0.6182836890220642  and  1.1026722192764282  and  0.886627197265625\n",
      "total error:  2.6075831055641174\n",
      "training error:  0.6355674862861633  and  1.1493761539459229  and  0.8727940917015076\n",
      "total error:  2.6577377319335938\n",
      "training error:  0.6549698710441589  and  1.107313871383667  and  0.935735821723938\n",
      "total error:  2.698019564151764\n",
      "training error:  0.7148017883300781  and  1.220168113708496  and  1.0135841369628906\n",
      "total error:  2.948554039001465\n",
      "training error:  0.6700060367584229  and  1.338606834411621  and  1.1036162376403809\n",
      "total error:  3.112229108810425\n",
      "training error:  0.6753793358802795  and  1.2799142599105835  and  0.9223170876502991\n",
      "total error:  2.877610683441162\n",
      "training error:  0.6254645586013794  and  1.191487193107605  and  0.995642900466919\n",
      "total error:  2.8125946521759033\n",
      "training error:  0.6238781213760376  and  1.1282036304473877  and  0.9849695563316345\n",
      "total error:  2.73705130815506\n",
      "training error:  0.6369003057479858  and  1.1402809619903564  and  0.8790990114212036\n",
      "total error:  2.656280279159546\n",
      "training error:  0.6330908536911011  and  1.148350715637207  and  1.008899211883545\n",
      "total error:  2.790340781211853\n",
      "training error:  0.5986196994781494  and  1.026943564414978  and  0.8916057348251343\n",
      "total error:  2.5171689987182617\n",
      "training error:  0.6323686838150024  and  1.0658955574035645  and  0.9548920392990112\n",
      "total error:  2.653156280517578\n",
      "training error:  0.6563192009925842  and  1.1697311401367188  and  0.9264397621154785\n",
      "total error:  2.7524901032447815\n",
      "training error:  0.6221609115600586  and  1.1151578426361084  and  0.8748522996902466\n",
      "total error:  2.6121710538864136\n",
      "training error:  0.6075310707092285  and  1.0555510520935059  and  0.8886685967445374\n",
      "total error:  2.5517507195472717\n",
      "training error:  0.619949221611023  and  1.1511962413787842  and  0.9349364042282104\n",
      "total error:  2.7060818672180176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6060059666633606  and  1.0758438110351562  and  0.8649581074714661\n",
      "total error:  2.546807885169983\n",
      "training error:  0.6510473489761353  and  1.2356553077697754  and  0.8993418216705322\n",
      "total error:  2.786044478416443\n",
      "training error:  0.6787575483322144  and  1.3075759410858154  and  1.467695713043213\n",
      "total error:  3.4540292024612427\n",
      "training error:  0.6203290224075317  and  1.2382779121398926  and  0.9964420795440674\n",
      "total error:  2.8550490140914917\n",
      "training error:  0.6161158680915833  and  1.2696583271026611  and  0.8964725732803345\n",
      "total error:  2.782246768474579\n",
      "training error:  0.5876244902610779  and  1.054898738861084  and  0.8773679733276367\n",
      "total error:  2.5198912024497986\n",
      "training error:  0.5993084907531738  and  1.0615323781967163  and  0.8956220149993896\n",
      "total error:  2.55646288394928\n",
      "training error:  0.6003743410110474  and  1.0684823989868164  and  0.8658995032310486\n",
      "total error:  2.5347562432289124\n",
      "training error:  0.61202073097229  and  1.0654165744781494  and  0.8429909944534302\n",
      "total error:  2.5204282999038696\n",
      "training error:  0.6190147995948792  and  1.1838502883911133  and  0.944029688835144\n",
      "total error:  2.7468947768211365\n",
      "training error:  0.667378842830658  and  1.2222427129745483  and  0.9844315648078918\n",
      "total error:  2.874053120613098\n",
      "training error:  0.6049367189407349  and  1.1061967611312866  and  0.9207507371902466\n",
      "total error:  2.631884217262268\n",
      "training error:  0.610547661781311  and  1.0831128358840942  and  0.9406192302703857\n",
      "total error:  2.634279727935791\n",
      "training error:  0.584520697593689  and  1.0271360874176025  and  0.889563798904419\n",
      "total error:  2.5012205839157104\n",
      "training error:  0.6279592514038086  and  1.1345468759536743  and  1.0224528312683105\n",
      "total error:  2.7849589586257935\n",
      "training error:  0.6168094873428345  and  1.0542285442352295  and  0.920378565788269\n",
      "total error:  2.591416597366333\n",
      "training error:  0.5987584590911865  and  1.0246632099151611  and  0.8766117095947266\n",
      "total error:  2.500033378601074\n",
      "training error:  0.5906426906585693  and  1.0783125162124634  and  0.9106780886650085\n",
      "total error:  2.5796332955360413\n",
      "training error:  0.6377661228179932  and  1.1951160430908203  and  0.95319664478302\n",
      "total error:  2.7860788106918335\n",
      "training error:  0.6066287159919739  and  1.1429084539413452  and  0.9000385999679565\n",
      "total error:  2.6495757699012756\n",
      "training error:  0.5947167873382568  and  1.058605432510376  and  0.8758711218833923\n",
      "total error:  2.529193341732025\n",
      "training error:  0.6441540718078613  and  1.17476487159729  and  0.9654300212860107\n",
      "total error:  2.784348964691162\n",
      "training error:  0.6528903245925903  and  1.2148241996765137  and  0.9655675292015076\n",
      "total error:  2.8332820534706116\n",
      "training error:  0.6121162176132202  and  1.1301578283309937  and  0.9357046484947205\n",
      "total error:  2.6779786944389343\n",
      "training error:  0.6143912076950073  and  1.2395429611206055  and  1.0187616348266602\n",
      "total error:  2.872695803642273\n",
      "training error:  0.627445638179779  and  1.0971559286117554  and  0.8915522694587708\n",
      "total error:  2.616153836250305\n",
      "training error:  0.6548714637756348  and  1.2108079195022583  and  0.9319716691970825\n",
      "total error:  2.7976510524749756\n",
      "training error:  0.6399128437042236  and  1.182629108428955  and  1.0026817321777344\n",
      "total error:  2.825223684310913\n",
      "training error:  0.6119545698165894  and  1.075335144996643  and  0.8703300952911377\n",
      "total error:  2.55761981010437\n",
      "training error:  0.6876922845840454  and  1.4152851104736328  and  1.1387439966201782\n",
      "total error:  3.2417213916778564\n",
      "training error:  0.6702336668968201  and  1.19636070728302  and  0.9347116351127625\n",
      "total error:  2.8013060092926025\n",
      "training error:  0.6219610571861267  and  1.1488325595855713  and  0.921302318572998\n",
      "total error:  2.692095935344696\n",
      "training error:  0.6293139457702637  and  1.1065322160720825  and  0.8814579248428345\n",
      "total error:  2.6173040866851807\n",
      "training error:  0.6051741242408752  and  1.0845069885253906  and  0.8902338743209839\n",
      "total error:  2.5799149870872498\n",
      "training error:  0.6354557275772095  and  1.0894927978515625  and  0.8614687919616699\n",
      "total error:  2.586417317390442\n",
      "training error:  0.6484119892120361  and  1.136309266090393  and  0.9193758964538574\n",
      "total error:  2.7040971517562866\n",
      "training error:  0.6155717372894287  and  1.1185826063156128  and  0.9016523361206055\n",
      "total error:  2.635806679725647\n",
      "training error:  0.6391899585723877  and  1.0838236808776855  and  0.8896312713623047\n",
      "total error:  2.612644910812378\n",
      "training error:  0.6150327920913696  and  1.1527986526489258  and  0.9344950318336487\n",
      "total error:  2.702326476573944\n",
      "training error:  0.654062032699585  and  1.099561095237732  and  0.9625566601753235\n",
      "total error:  2.7161797881126404\n",
      "training error:  0.5869451761245728  and  1.1068918704986572  and  1.0133826732635498\n",
      "total error:  2.70721971988678\n",
      "training error:  0.6056582927703857  and  1.1142637729644775  and  0.9902778267860413\n",
      "total error:  2.7101998925209045\n",
      "training error:  0.591833233833313  and  1.0966752767562866  and  0.8883123397827148\n",
      "total error:  2.5768208503723145\n",
      "training error:  0.6412618160247803  and  1.1330938339233398  and  0.9468824863433838\n",
      "total error:  2.721238136291504\n",
      "training error:  0.5824354887008667  and  1.0602965354919434  and  0.8741049766540527\n",
      "total error:  2.516837000846863\n",
      "training error:  0.5801475048065186  and  1.006571888923645  and  0.8672959208488464\n",
      "total error:  2.45401531457901\n",
      "training error:  0.5856730937957764  and  1.0499448776245117  and  0.8637087345123291\n",
      "total error:  2.499326705932617\n",
      "training error:  0.6078964471817017  and  1.0998913049697876  and  0.8567493557929993\n",
      "total error:  2.5645371079444885\n",
      "training error:  0.5902157425880432  and  1.0497238636016846  and  0.9043341875076294\n",
      "total error:  2.544273793697357\n",
      "training error:  0.6224707961082458  and  1.0835776329040527  and  0.9350056052207947\n",
      "total error:  2.6410540342330933\n",
      "training error:  0.6101511716842651  and  1.0704379081726074  and  0.953170657157898\n",
      "total error:  2.6337597370147705\n",
      "training error:  0.6307017803192139  and  1.1349265575408936  and  0.8456836342811584\n",
      "total error:  2.611311972141266\n",
      "training error:  0.5843337178230286  and  1.087719202041626  and  0.9287926554679871\n",
      "total error:  2.6008455753326416\n",
      "training error:  0.6434913277626038  and  1.1449284553527832  and  0.9697579145431519\n",
      "total error:  2.758177697658539\n",
      "training error:  0.5957989692687988  and  1.1676886081695557  and  0.879149854183197\n",
      "total error:  2.6426374316215515\n",
      "training error:  0.5987817049026489  and  1.0738412141799927  and  0.892756998538971\n",
      "total error:  2.5653799176216125\n",
      "training error:  0.5931892395019531  and  1.0664465427398682  and  0.8811047077178955\n",
      "total error:  2.540740489959717\n",
      "training error:  0.5974442958831787  and  1.0466437339782715  and  0.9058201313018799\n",
      "total error:  2.54990816116333\n",
      "training error:  0.5903457403182983  and  1.102158546447754  and  0.951013445854187\n",
      "total error:  2.6435177326202393\n",
      "training error:  0.6232336759567261  and  1.0928770303726196  and  0.9573186635971069\n",
      "total error:  2.6734293699264526\n",
      "training error:  0.6222678422927856  and  1.1335564851760864  and  0.9224181175231934\n",
      "total error:  2.6782424449920654\n",
      "training error:  0.6758668422698975  and  1.2445530891418457  and  1.0657075643539429\n",
      "total error:  2.986127495765686\n",
      "training error:  0.6137406826019287  and  1.1305725574493408  and  0.9629957675933838\n",
      "total error:  2.7073090076446533\n",
      "training error:  0.5993129014968872  and  1.0852625370025635  and  0.8425813317298889\n",
      "total error:  2.5271567702293396\n",
      "training error:  0.5957808494567871  and  1.081338882446289  and  0.8837683796882629\n",
      "total error:  2.560888111591339\n",
      "training error:  0.604680061340332  and  1.0837481021881104  and  0.8912092447280884\n",
      "total error:  2.5796374082565308\n",
      "training error:  0.6071970462799072  and  1.209585189819336  and  0.9860897064208984\n",
      "total error:  2.8028719425201416\n",
      "training error:  0.5981899499893188  and  1.1211061477661133  and  1.063570261001587\n",
      "total error:  2.782866358757019\n",
      "training error:  0.5983926057815552  and  1.1115429401397705  and  0.970175564289093\n",
      "total error:  2.6801111102104187\n",
      "training error:  0.6262240409851074  and  1.1511036157608032  and  0.9512895345687866\n",
      "total error:  2.7286171913146973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6298619508743286  and  1.3310742378234863  and  0.9133249521255493\n",
      "total error:  2.8742611408233643\n",
      "training error:  0.6826364398002625  and  1.1444909572601318  and  0.9376972913742065\n",
      "total error:  2.764824688434601\n",
      "training error:  0.6214983463287354  and  1.1100032329559326  and  0.9705526828765869\n",
      "total error:  2.702054262161255\n",
      "training error:  0.640586793422699  and  1.1488685607910156  and  1.0672321319580078\n",
      "total error:  2.8566874861717224\n",
      "training error:  0.610547661781311  and  1.2012039422988892  and  1.088070034980774\n",
      "total error:  2.899821639060974\n",
      "training error:  0.6679860353469849  and  1.1231969594955444  and  0.9144800901412964\n",
      "total error:  2.7056630849838257\n",
      "training error:  0.6284352540969849  and  1.136749267578125  and  0.953014612197876\n",
      "total error:  2.718199133872986\n",
      "training error:  0.7028155326843262  and  1.2173821926116943  and  0.9688256978988647\n",
      "total error:  2.8890234231948853\n",
      "training error:  0.630058765411377  and  1.1244394779205322  and  0.9875660538673401\n",
      "total error:  2.7420642971992493\n",
      "training error:  0.6323434710502625  and  1.122894048690796  and  1.1695539951324463\n",
      "total error:  2.9247915148735046\n",
      "training error:  0.5981131792068481  and  1.1000691652297974  and  0.8722469210624695\n",
      "total error:  2.570429265499115\n",
      "training error:  0.6470445394515991  and  1.1238250732421875  and  0.9227134585380554\n",
      "total error:  2.693583071231842\n",
      "training error:  0.6150379180908203  and  1.1198790073394775  and  1.109719157218933\n",
      "total error:  2.844636082649231\n",
      "training error:  0.6125123500823975  and  1.0991400480270386  and  0.8680263757705688\n",
      "total error:  2.579678773880005\n",
      "training error:  0.6044623851776123  and  1.0730314254760742  and  0.9104443788528442\n",
      "total error:  2.5879381895065308\n",
      "training error:  0.5840184688568115  and  1.0392992496490479  and  0.8874698877334595\n",
      "total error:  2.510787606239319\n",
      "training error:  0.6321262121200562  and  1.1154524087905884  and  0.852144718170166\n",
      "total error:  2.5997233390808105\n",
      "training error:  0.615633487701416  and  1.0485167503356934  and  0.9181853532791138\n",
      "total error:  2.582335591316223\n",
      "training error:  0.5935864448547363  and  1.0568366050720215  and  0.8943768739700317\n",
      "total error:  2.5447999238967896\n",
      "training error:  0.6635110378265381  and  1.1237413883209229  and  0.9362826347351074\n",
      "total error:  2.7235350608825684\n",
      "training error:  0.5943291187286377  and  1.108576774597168  and  1.120008945465088\n",
      "total error:  2.8229148387908936\n",
      "training error:  0.6034468412399292  and  1.134192705154419  and  0.9100247621536255\n",
      "total error:  2.6476643085479736\n",
      "training error:  0.5726337432861328  and  1.0494818687438965  and  0.843319296836853\n",
      "total error:  2.4654349088668823\n",
      "training error:  0.5882294178009033  and  1.0632882118225098  and  0.841222882270813\n",
      "total error:  2.492740511894226\n",
      "training error:  0.6031429767608643  and  1.0891543626785278  and  0.9164965152740479\n",
      "total error:  2.60879385471344\n",
      "training error:  0.5893127918243408  and  1.0983530282974243  and  0.9135610461235046\n",
      "total error:  2.6012268662452698\n",
      "training error:  0.5926766395568848  and  1.0848721265792847  and  0.8709996938705444\n",
      "total error:  2.548548460006714\n",
      "training error:  0.5904967784881592  and  1.137648582458496  and  0.888335645198822\n",
      "total error:  2.6164810061454773\n",
      "training error:  0.650997519493103  and  1.1493254899978638  and  0.9117996096611023\n",
      "total error:  2.712122619152069\n",
      "training error:  0.6152446269989014  and  1.0869790315628052  and  0.8836783170700073\n",
      "total error:  2.585901975631714\n",
      "training error:  0.618535041809082  and  1.154979944229126  and  0.9918882846832275\n",
      "total error:  2.7654032707214355\n",
      "training error:  0.6146844029426575  and  1.061409592628479  and  0.8979054689407349\n",
      "total error:  2.5739994645118713\n",
      "training error:  0.6049723625183105  and  1.0556535720825195  and  0.8849557042121887\n",
      "total error:  2.545581638813019\n",
      "training error:  0.5998295545578003  and  1.0784330368041992  and  0.8856390118598938\n",
      "total error:  2.5639016032218933\n",
      "training error:  0.5692801475524902  and  1.0593409538269043  and  0.8548203110694885\n",
      "total error:  2.483441412448883\n",
      "training error:  0.655584454536438  and  1.1366461515426636  and  1.0952296257019043\n",
      "total error:  2.887460231781006\n",
      "training error:  0.5902253985404968  and  1.0625027418136597  and  0.8707756996154785\n",
      "total error:  2.523503839969635\n",
      "training error:  0.5658830404281616  and  1.071601390838623  and  0.8340334892272949\n",
      "total error:  2.4715179204940796\n",
      "training error:  0.5972644686698914  and  1.1078517436981201  and  0.9906821250915527\n",
      "total error:  2.695798337459564\n",
      "training error:  0.6003578901290894  and  1.0879230499267578  and  0.9262633323669434\n",
      "total error:  2.6145442724227905\n",
      "training error:  0.5832569599151611  and  1.0816125869750977  and  0.8695328235626221\n",
      "total error:  2.534402370452881\n",
      "training error:  0.6021344065666199  and  1.0561096668243408  and  0.9302331209182739\n",
      "total error:  2.5884771943092346\n",
      "training error:  0.5772087574005127  and  1.0468006134033203  and  0.8793244361877441\n",
      "total error:  2.503333806991577\n",
      "training error:  0.6401023864746094  and  1.6601144075393677  and  1.0444252490997314\n",
      "total error:  3.3446420431137085\n",
      "training error:  0.6183832287788391  and  1.0589630603790283  and  0.8890823125839233\n",
      "total error:  2.5664286017417908\n",
      "training error:  0.6236270070075989  and  1.066555142402649  and  0.9321851134300232\n",
      "total error:  2.622367262840271\n",
      "training error:  0.6276994347572327  and  1.1451225280761719  and  0.8308966159820557\n",
      "total error:  2.60371857881546\n",
      "training error:  0.577755331993103  and  1.0725033283233643  and  0.9024859666824341\n",
      "total error:  2.5527446269989014\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys,os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy import linalg\n",
    "from numpy import dot\n",
    "import geomloss as gs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import grad\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.modules import Linear\n",
    "from torch.autograd.functional import jacobian,hessian,vjp,vhp,hvp\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "FilePath = '../../'\n",
    "\n",
    "file_list = ['GSM1599494_ES_d0_main.csv', 'GSM1599497_ES_d2_LIFminus.csv', 'GSM1599498_ES_d4_LIFminus.csv', 'GSM1599499_ES_d7_LIFminus.csv']\n",
    "\n",
    "table_list = []\n",
    "for filein in file_list:\n",
    "    table_list.append(pd.read_csv(FilePath+filein, header=None))\n",
    "\n",
    "matrix_list = []\n",
    "gene_names = table_list[0].values[:,0]\n",
    "for table in table_list:\n",
    "    matrix_list.append(table.values[:,1:].astype('float32'))\n",
    "\n",
    "cell_counts = [matrix.shape[1] for matrix in matrix_list]\n",
    "\n",
    "def normalize_run(mat):\n",
    "    rpm = np.sum(mat,0)/1e6\n",
    "    detect_pr = np.sum(mat==0,0)/float(mat.shape[0])\n",
    "    return np.log(mat*(np.median(detect_pr)/detect_pr)*1.0/rpm + 1.0)\n",
    "\n",
    "norm_mat = [normalize_run(matrix) for matrix in matrix_list]\n",
    "\n",
    "qt_mat = [np.percentile(norm_in,q=np.linspace(0,100,50),axis=1) for norm_in in norm_mat] \n",
    "wdiv=np.sum((qt_mat[0]-qt_mat[3])**2,0)\n",
    "w_order = np.argsort(-wdiv)\n",
    "\n",
    "wsub = w_order[0:100]\n",
    "\n",
    "\n",
    "def nmf(X, latent_features, max_iter=100, error_limit=1e-6, fit_error_limit=1e-6, print_iter=200):\n",
    "    \"\"\"\n",
    "    Decompose X to A*Y\n",
    "    \"\"\"\n",
    "    eps = 1e-5\n",
    "    print('Starting NMF decomposition with {} latent features and {} iterations.'.format(latent_features, max_iter))\n",
    "    #X = X.toarray()   I am passing in a scipy sparse matrix\n",
    "\n",
    "    # mask\n",
    "    mask = np.sign(X)\n",
    "\n",
    "    # initial matrices. A is random [0,1] and Y is A\\X.\n",
    "    rows, columns = X.shape\n",
    "    A = np.random.rand(rows, latent_features)\n",
    "    A = np.maximum(A, eps)\n",
    "\n",
    "    Y = linalg.lstsq(A, X)[0]\n",
    "    Y = np.maximum(Y, eps)\n",
    "\n",
    "    masked_X = mask * X\n",
    "    X_est_prev = dot(A, Y)\n",
    "    for i in range(1, max_iter + 1):\n",
    "        # ===== updates =====\n",
    "        # Matlab: A=A.*(((W.*X)*Y')./((W.*(A*Y))*Y'));\n",
    "        top = dot(masked_X, Y.T)\n",
    "        bottom = (dot((mask * dot(A, Y)), Y.T)) + eps\n",
    "        A *= top / bottom\n",
    "\n",
    "        A = np.maximum(A, eps)\n",
    "        # print 'A',  np.round(A, 2)\n",
    "\n",
    "        # Matlab: Y=Y.*((A'*(W.*X))./(A'*(W.*(A*Y))));\n",
    "        top = dot(A.T, masked_X)\n",
    "        bottom = dot(A.T, mask * dot(A, Y)) + eps\n",
    "        Y *= top / bottom\n",
    "        Y = np.maximum(Y, eps)\n",
    "        # print 'Y', np.round(Y, 2)\n",
    "\n",
    "\n",
    "        # ==== evaluation ====\n",
    "        if i % print_iter == 0 or i == 1 or i == max_iter:\n",
    "            print('Iteration {}:'.format(i),)\n",
    "            X_est = dot(A, Y)\n",
    "            err = mask * (X_est_prev - X_est)\n",
    "            fit_residual = np.sqrt(np.sum(err ** 2))\n",
    "            X_est_prev = X_est\n",
    "\n",
    "            curRes = linalg.norm(mask * (X - X_est), ord='fro')\n",
    "            print('fit residual', np.round(fit_residual, 4),)\n",
    "            print('total residual', np.round(curRes, 4))\n",
    "            if curRes < error_limit or fit_residual < fit_error_limit:\n",
    "                break\n",
    "    return A, Y, dot(A,Y)\n",
    "\n",
    "np.random.seed(0)\n",
    "norm_imputed = [nmf(normin[wsub,:], latent_features = len(wsub)*4, max_iter=500)[2] for normin in norm_mat]\n",
    "\n",
    "norm_adj = np.mean(norm_imputed[3],1)[:,np.newaxis]\n",
    "subvec = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "gnvec = gene_names[w_order[subvec]]\n",
    "\n",
    "cov_mat = np.cov(norm_imputed[3][subvec,:])\n",
    "whiten = np.diag(np.diag(cov_mat)**(-0.5))\n",
    "unwhiten = np.diag(np.diag(cov_mat)**(0.5))\n",
    "\n",
    "norm_imputed2 = [np.dot(whiten,(normin - norm_adj)[subvec,:]) for normin in norm_imputed]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, dim_hidden=64, num_hidden=0, activation=nn.LeakyReLU()):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        if num_hidden == 0:\n",
    "            self.linears = nn.ModuleList([nn.Linear(dim_in, dim_out)])\n",
    "        elif num_hidden >= 1:\n",
    "            self.linears = nn.ModuleList() \n",
    "            self.linears.append(nn.Linear(dim_in, dim_hidden))\n",
    "            self.linears.extend([nn.Linear(dim_hidden, dim_hidden) for _ in range(num_hidden-1)])\n",
    "            self.linears.append(nn.Linear(dim_hidden, dim_out))\n",
    "        else:\n",
    "            raise Exception('number of hidden layers must be positive')\n",
    "\n",
    "        for m in self.linears:\n",
    "            #nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.uniform_(m.bias,a=-0.1,b=0.1)\n",
    "            #nn.init.constant_(m.bias,0)\n",
    " \n",
    "        self.activation = activation # \n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.linears[:-1]:\n",
    "            x = self.activation(m(x))\n",
    "            #x = F.dropout(x,p=0.5)\n",
    "\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "\n",
    "def compute_gradient_penalty(D, real_sample, fake_sample,k,p):\n",
    "    real_samples = real_sample.requires_grad_(True)\n",
    "    fake_samples = fake_sample.requires_grad_(True)\n",
    "\n",
    "    real_validity = D(real_samples)\n",
    "    fake_validity = D(fake_samples)\n",
    "\n",
    "    real_grad_out = torch.ones((real_samples.shape[0],1),dtype=torch.float32,requires_grad=False,device=\"cuda\")\n",
    "    real_grad = grad(\n",
    "        real_validity, real_samples, real_grad_out, create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    real_grad_norm = real_grad.view(real_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "\n",
    "    fake_grad_out = torch.ones((fake_samples.shape[0],1),dtype=torch.float32,requires_grad=False,device=\"cuda\")\n",
    "    fake_grad = grad(\n",
    "        fake_validity, fake_samples, fake_grad_out, create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    fake_grad_norm = fake_grad.view(fake_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "\n",
    "    return (torch.sum(real_grad_norm) + torch.sum(fake_grad_norm)) * k / (real_sample.shape[0]+fake_sample.shape[0])\n",
    "\n",
    "class JumpEulerForwardCuda(nn.Module):\n",
    "    def __init__(self,in_features,num_hidden,dim_hidden,step_size):\n",
    "        super(JumpEulerForwardCuda,self).__init__()\n",
    "\n",
    "        self.drift = MLP(in_features,in_features,dim_hidden,num_hidden)\n",
    "        self.intensity = torch.tensor(intensity,device=\"cuda\")\n",
    "        self.mean = nn.Parameter(0.01*torch.ones(in_features))\n",
    "        self.covHalf = nn.Parameter(0.08*torch.eye(in_features))\n",
    "        self.diffusion = nn.Parameter(torch.ones(bd,10))\n",
    "        self.in_features = in_features\n",
    "        self.jump = MLP(in_features,in_features,dim_hidden,num_hidden)\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def forward(self,z0,Nsim,steps):\n",
    "\n",
    "        PopulationPath = torch.empty(size = (Nsim,steps+1,self.in_features),device=\"cuda\")\n",
    "        PopulationPath[:,0,:] = z0\n",
    "        state = z0\n",
    "\n",
    "        for i in range(1,steps+1):\n",
    "            DP = D.poisson.Poisson(self.intensity*self.step_size) \n",
    "            pois = DP.sample((Nsim,1)).cuda()\n",
    "            state = state + self.drift(state)*self.step_size + math.sqrt(self.step_size)*torch.normal(0,1,size=(Nsim,bd),device=\"cuda\")@self.diffusion+\\\n",
    "                (pois*self.mean + pois**(0.5)*torch.normal(0,1,size=(Nsim,self.in_features),device=\"cuda\")@self.covHalf)*self.jump(state)\n",
    "            PopulationPath[:,i,:] = state\n",
    "        return PopulationPath\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "sed = 200\n",
    "setup_seed(sed)\n",
    "\n",
    "a=gs.SamplesLoss(loss='sinkhorn',p=2,blur=0.01)\n",
    "\n",
    "\n",
    "train_data = norm_imputed2\n",
    "\n",
    "train0 = torch.tensor(train_data[0],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "train2 = torch.tensor(train_data[1],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "train4 = torch.tensor(train_data[2],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "train7 = torch.tensor(train_data[3],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "\n",
    "train0 = train0+0.1*torch.normal(0,1,size=train0.shape,device=\"cuda\")\n",
    "train2 = train2+0.1*torch.normal(0,1,size=train2.shape,device=\"cuda\")\n",
    "train4 = train4+0.1*torch.normal(0,1,size=train4.shape,device=\"cuda\")\n",
    "train7 = train7+0.1*torch.normal(0,1,size=train7.shape,device=\"cuda\")\n",
    "\n",
    "\n",
    "intensity = 10\n",
    "lr = 0.0003\n",
    "step_size = 0.03\n",
    "kuan = 256\n",
    "ceng = 4\n",
    "bd = 2\n",
    "n_critic = 3\n",
    "k = 2\n",
    "p = 6\n",
    "\n",
    "n_sims = train0.shape[0]\n",
    "in_features = train0.shape[1]\n",
    "n_steps = [10,20,35]\n",
    "\n",
    "\n",
    "netG = JumpEulerForwardCuda(10,ceng,kuan,step_size).cuda()\n",
    "netD1 = MLP(10,1,dim_hidden=kuan,num_hidden=ceng).cuda()\n",
    "netD2 = MLP(10,1,dim_hidden=kuan,num_hidden=ceng).cuda()\n",
    "netD3 = MLP(10,1,dim_hidden=kuan,num_hidden=ceng).cuda()\n",
    "\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerSD1 = optim.Adam(netD1.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerSD2 = optim.Adam(netD2.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerSD3 = optim.Adam(netD3.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "n_epochs =  20000\n",
    "\n",
    "wd = []\n",
    "for epoch in range(n_epochs):\n",
    "  \n",
    "\n",
    "\n",
    "    for _ in range(n_critic):\n",
    "        fake_data = netG(train0,n_sims,n_steps[2])\n",
    "        fake1 = fake_data[:,n_steps[0],:]\n",
    "        fake2 = fake_data[:,n_steps[1],:]\n",
    "        fake3 = fake_data[:,n_steps[2],:]\n",
    "\n",
    "        optimizerSD1.zero_grad()\n",
    "\n",
    "        div_gp1 = compute_gradient_penalty(netD1,train2,fake1,k,p)\n",
    "        d1_loss = -torch.mean(netD1(train2))+torch.mean(netD1(fake1))+div_gp1\n",
    "        d1_loss.backward(retain_graph=True) # retain_graph=True\n",
    "\n",
    "        optimizerSD1.step()\n",
    "\n",
    "        optimizerSD2.zero_grad()\n",
    "        \n",
    "        div_gp2 = compute_gradient_penalty(netD2,train4,fake2,k,p)\n",
    "        d2_loss = -torch.mean(netD2(train4))+torch.mean(netD2(fake2))+div_gp2\n",
    "        d2_loss.backward(retain_graph=True)\n",
    "\n",
    "        optimizerSD2.step()\n",
    "        \n",
    "        \n",
    "        optimizerSD3.zero_grad()\n",
    "        \n",
    "        div_gp3 = compute_gradient_penalty(netD3,train7,fake3,k,p)\n",
    "        d3_loss = -torch.mean(netD3(train7))+torch.mean(netD3(fake3))+div_gp3\n",
    "        d3_loss.backward(retain_graph=True)\n",
    "\n",
    "        optimizerSD3.step()\n",
    "        \n",
    "\n",
    "    \n",
    "    for _ in range(1):\n",
    "        optimizerG.zero_grad()\n",
    "        fake_data = netG(train0,n_sims,n_steps[2])\n",
    "        fake1 = fake_data[:,n_steps[0],:]\n",
    "        fake2 = fake_data[:,n_steps[1],:]\n",
    "        fake3 = fake_data[:,n_steps[2],:]\n",
    "        g_loss = -torch.mean(netD1(fake1))-torch.mean(netD2(fake2))-torch.mean(netD3(fake3))\n",
    "        g_loss.backward() \n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "    if epoch %10==0:\n",
    "        x1 = a(fake_data[:,n_steps[0],:],train2).item()\n",
    "        x2 = a(fake_data[:,n_steps[1],:],train4).item()\n",
    "        x3 = a(fake_data[:,n_steps[2],:],train7).item()\n",
    "        \n",
    "        wd.append(x1+x2+x3)\n",
    "        \n",
    "        print(\"training error: \",x1,\" and \",x2,\" and \",x3)\n",
    "        print(\"total error: \",wd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(),\"./epsilon0.1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
