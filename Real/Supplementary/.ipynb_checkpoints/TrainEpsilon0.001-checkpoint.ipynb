{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 4000.1233\n",
      "total residual 138.7959\n",
      "Iteration 200:\n",
      "fit residual 123.6643\n",
      "total residual 31.8464\n",
      "Iteration 400:\n",
      "fit residual 21.7894\n",
      "total residual 12.6115\n",
      "Iteration 500:\n",
      "fit residual 4.3442\n",
      "total residual 8.6686\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 2257.8358\n",
      "total residual 58.2265\n",
      "Iteration 200:\n",
      "fit residual 56.8198\n",
      "total residual 4.0447\n",
      "Iteration 400:\n",
      "fit residual 3.4777\n",
      "total residual 0.6841\n",
      "Iteration 500:\n",
      "fit residual 0.3744\n",
      "total residual 0.3185\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 2925.3664\n",
      "total residual 80.2657\n",
      "Iteration 200:\n",
      "fit residual 78.1148\n",
      "total residual 6.1559\n",
      "Iteration 400:\n",
      "fit residual 5.1661\n",
      "total residual 1.1917\n",
      "Iteration 500:\n",
      "fit residual 0.6183\n",
      "total residual 0.5917\n",
      "Starting NMF decomposition with 400 latent features and 500 iterations.\n",
      "Iteration 1:\n",
      "fit residual 3719.0401\n",
      "total residual 121.8387\n",
      "Iteration 200:\n",
      "fit residual 113.1429\n",
      "total residual 20.967\n",
      "Iteration 400:\n",
      "fit residual 15.2785\n",
      "total residual 7.12\n",
      "Iteration 500:\n",
      "fit residual 2.7305\n",
      "total residual 4.5793\n",
      "training error:  8.587867736816406  and  27.764400482177734  and  84.19031524658203\n",
      "total error:  120.54258346557617\n",
      "training error:  5.671388149261475  and  11.90617561340332  and  27.104814529418945\n",
      "total error:  44.68237829208374\n",
      "training error:  5.933389663696289  and  7.385428428649902  and  10.209099769592285\n",
      "total error:  23.527917861938477\n",
      "training error:  5.0943474769592285  and  5.205466270446777  and  5.926298141479492\n",
      "total error:  16.226111888885498\n",
      "training error:  4.1289496421813965  and  3.8591794967651367  and  4.3755035400390625\n",
      "total error:  12.363632678985596\n",
      "training error:  3.6928205490112305  and  3.5171186923980713  and  3.3377909660339355\n",
      "total error:  10.547730207443237\n",
      "training error:  3.3951101303100586  and  3.5070056915283203  and  2.807501792907715\n",
      "total error:  9.709617614746094\n",
      "training error:  3.3875393867492676  and  3.2995285987854004  and  2.574429988861084\n",
      "total error:  9.261497974395752\n",
      "training error:  3.2582170963287354  and  3.1318845748901367  and  2.2675604820251465\n",
      "total error:  8.657662153244019\n",
      "training error:  2.949213743209839  and  2.892343044281006  and  2.2140402793884277\n",
      "total error:  8.055597066879272\n",
      "training error:  2.959176540374756  and  3.3552255630493164  and  3.0689034461975098\n",
      "total error:  9.383305549621582\n",
      "training error:  3.0832607746124268  and  3.898848533630371  and  2.279130220413208\n",
      "total error:  9.261239528656006\n",
      "training error:  2.756410598754883  and  3.1300432682037354  and  3.5795624256134033\n",
      "total error:  9.466016292572021\n",
      "training error:  2.6277003288269043  and  3.1368398666381836  and  3.549424648284912\n",
      "total error:  9.31396484375\n",
      "training error:  2.5966010093688965  and  3.151027202606201  and  2.7767837047576904\n",
      "total error:  8.524411916732788\n",
      "training error:  2.489104747772217  and  2.7771663665771484  and  2.4394378662109375\n",
      "total error:  7.705708980560303\n",
      "training error:  2.3986544609069824  and  2.744217872619629  and  2.156992197036743\n",
      "total error:  7.2998645305633545\n",
      "training error:  2.4691944122314453  and  2.778958320617676  and  2.4875502586364746\n",
      "total error:  7.735702991485596\n",
      "training error:  2.44311785697937  and  2.703134536743164  and  2.2801477909088135\n",
      "total error:  7.426400184631348\n",
      "training error:  2.4562878608703613  and  2.6711888313293457  and  2.0744447708129883\n",
      "total error:  7.201921463012695\n",
      "training error:  2.291347026824951  and  2.7524561882019043  and  1.9072418212890625\n",
      "total error:  6.951045036315918\n",
      "training error:  2.2243292331695557  and  2.860976457595825  and  1.8427937030792236\n",
      "total error:  6.9280993938446045\n",
      "training error:  2.222106456756592  and  2.3589682579040527  and  1.9629627466201782\n",
      "total error:  6.544037461280823\n",
      "training error:  2.251532554626465  and  2.479536533355713  and  1.8200383186340332\n",
      "total error:  6.551107406616211\n",
      "training error:  2.065032482147217  and  2.336204767227173  and  1.9154071807861328\n",
      "total error:  6.3166444301605225\n",
      "training error:  2.1826117038726807  and  2.9251832962036133  and  2.250575542449951\n",
      "total error:  7.358370542526245\n",
      "training error:  2.0389277935028076  and  3.0520849227905273  and  2.3331356048583984\n",
      "total error:  7.424148321151733\n",
      "training error:  2.064229965209961  and  2.8428914546966553  and  2.554009437561035\n",
      "total error:  7.461130857467651\n",
      "training error:  2.0211782455444336  and  3.4224557876586914  and  2.8596205711364746\n",
      "total error:  8.3032546043396\n",
      "training error:  2.0288467407226562  and  2.4295499324798584  and  1.757983684539795\n",
      "total error:  6.21638035774231\n",
      "training error:  2.1196885108947754  and  3.7217695713043213  and  2.017536163330078\n",
      "total error:  7.858994245529175\n",
      "training error:  1.9603769779205322  and  2.67289400100708  and  2.044919967651367\n",
      "total error:  6.6781909465789795\n",
      "training error:  1.9927794933319092  and  2.6668739318847656  and  2.1309590339660645\n",
      "total error:  6.790612459182739\n",
      "training error:  1.9222116470336914  and  2.2263565063476562  and  1.8028242588043213\n",
      "total error:  5.951392412185669\n",
      "training error:  2.144134521484375  and  3.3602755069732666  and  1.7714407444000244\n",
      "total error:  7.275850772857666\n",
      "training error:  2.0385665893554688  and  2.6072916984558105  and  1.6576552391052246\n",
      "total error:  6.303513526916504\n",
      "training error:  1.9800927639007568  and  3.7203097343444824  and  3.4902069568634033\n",
      "total error:  9.190609455108643\n",
      "training error:  2.0490355491638184  and  2.9077882766723633  and  1.7729957103729248\n",
      "total error:  6.7298195362091064\n",
      "training error:  1.978102445602417  and  2.7051568031311035  and  1.577183723449707\n",
      "total error:  6.2604429721832275\n",
      "training error:  1.848433017730713  and  3.0403099060058594  and  2.7575883865356445\n",
      "total error:  7.646331310272217\n",
      "training error:  2.0875492095947266  and  4.297420024871826  and  1.8076984882354736\n",
      "total error:  8.192667722702026\n",
      "training error:  1.8379368782043457  and  2.800753593444824  and  3.1185145378112793\n",
      "total error:  7.757205009460449\n",
      "training error:  1.9778330326080322  and  3.073648452758789  and  1.8153737783432007\n",
      "total error:  6.866855263710022\n",
      "training error:  1.7940361499786377  and  2.285914421081543  and  1.844571590423584\n",
      "total error:  5.924522161483765\n",
      "training error:  1.8379268646240234  and  2.2024741172790527  and  1.4571236371994019\n",
      "total error:  5.497524619102478\n",
      "training error:  1.8423045873641968  and  2.591320037841797  and  2.069204092025757\n",
      "total error:  6.5028287172317505\n",
      "training error:  1.9172656536102295  and  2.398913860321045  and  1.983931064605713\n",
      "total error:  6.300110578536987\n",
      "training error:  1.8225796222686768  and  2.5339515209198  and  1.9698927402496338\n",
      "total error:  6.32642388343811\n",
      "training error:  1.8232746124267578  and  2.4099526405334473  and  2.0118179321289062\n",
      "total error:  6.245045185089111\n",
      "training error:  1.8380670547485352  and  3.3916878700256348  and  2.7599387168884277\n",
      "total error:  7.989693641662598\n",
      "training error:  1.863560438156128  and  2.464395046234131  and  1.5064460039138794\n",
      "total error:  5.834401488304138\n",
      "training error:  1.8000471591949463  and  2.418748378753662  and  1.552431583404541\n",
      "total error:  5.771227121353149\n",
      "training error:  1.7122130393981934  and  2.5219554901123047  and  2.004821300506592\n",
      "total error:  6.23898983001709\n",
      "training error:  1.8429430723190308  and  2.515963554382324  and  1.5879888534545898\n",
      "total error:  5.946895480155945\n",
      "training error:  1.7669456005096436  and  2.149409770965576  and  1.7605475187301636\n",
      "total error:  5.676902890205383\n",
      "training error:  1.726939082145691  and  2.799865961074829  and  2.0470728874206543\n",
      "total error:  6.573877930641174\n",
      "training error:  1.8901963233947754  and  3.513073444366455  and  1.3873730897903442\n",
      "total error:  6.790642857551575\n",
      "training error:  1.7308082580566406  and  2.553694248199463  and  2.1486613750457764\n",
      "total error:  6.43316388130188\n",
      "training error:  1.7618792057037354  and  2.1551403999328613  and  1.585015058517456\n",
      "total error:  5.502034664154053\n",
      "training error:  1.6262524127960205  and  2.1248297691345215  and  1.8471934795379639\n",
      "total error:  5.598275661468506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.6631428003311157  and  2.257638454437256  and  1.9392973184585571\n",
      "total error:  5.860078573226929\n",
      "training error:  1.6949307918548584  and  2.3249282836914062  and  1.478002905845642\n",
      "total error:  5.497861981391907\n",
      "training error:  1.6639182567596436  and  2.250065565109253  and  1.6895270347595215\n",
      "total error:  5.603510856628418\n",
      "training error:  1.6989284753799438  and  2.583865165710449  and  1.9762165546417236\n",
      "total error:  6.259010195732117\n",
      "training error:  1.633334755897522  and  2.3654940128326416  and  1.926204800605774\n",
      "total error:  5.9250335693359375\n",
      "training error:  1.6879818439483643  and  2.5570151805877686  and  1.682377815246582\n",
      "total error:  5.927374839782715\n",
      "training error:  1.6364991664886475  and  2.665964365005493  and  2.0688893795013428\n",
      "total error:  6.371352910995483\n",
      "training error:  1.8367919921875  and  3.4953718185424805  and  1.7718031406402588\n",
      "total error:  7.103966951370239\n",
      "training error:  1.6168009042739868  and  3.5174005031585693  and  2.534539222717285\n",
      "total error:  7.668740630149841\n",
      "training error:  1.7831509113311768  and  3.165165901184082  and  1.623618245124817\n",
      "total error:  6.571935057640076\n",
      "training error:  1.658484697341919  and  2.426734209060669  and  1.8757034540176392\n",
      "total error:  5.960922360420227\n",
      "training error:  1.5772559642791748  and  2.1196460723876953  and  1.49501633644104\n",
      "total error:  5.19191837310791\n",
      "training error:  1.832639455795288  and  3.8582968711853027  and  1.6122537851333618\n",
      "total error:  7.303190112113953\n",
      "training error:  1.5892927646636963  and  3.7377631664276123  and  2.9196693897247314\n",
      "total error:  8.24672532081604\n",
      "training error:  1.7243400812149048  and  4.416390895843506  and  1.6404192447662354\n",
      "total error:  7.781150221824646\n",
      "training error:  1.5868730545043945  and  2.3710813522338867  and  2.001936912536621\n",
      "total error:  5.959891319274902\n",
      "training error:  1.6291890144348145  and  3.056088924407959  and  1.9877572059631348\n",
      "total error:  6.673035144805908\n",
      "training error:  1.6550414562225342  and  3.0884151458740234  and  1.6919012069702148\n",
      "total error:  6.4353578090667725\n",
      "training error:  1.6139755249023438  and  2.2317004203796387  and  1.6689143180847168\n",
      "total error:  5.514590263366699\n",
      "training error:  1.532158374786377  and  3.4427764415740967  and  2.3968334197998047\n",
      "total error:  7.371768236160278\n",
      "training error:  1.6409648656845093  and  2.5265681743621826  and  1.7544667720794678\n",
      "total error:  5.92199981212616\n",
      "training error:  1.6251318454742432  and  2.7366440296173096  and  1.5927824974060059\n",
      "total error:  5.954558372497559\n",
      "training error:  1.5439510345458984  and  2.9303226470947266  and  1.9300878047943115\n",
      "total error:  6.4043614864349365\n",
      "training error:  1.6172733306884766  and  2.927461624145508  and  1.5216052532196045\n",
      "total error:  6.066340208053589\n",
      "training error:  1.6840336322784424  and  3.4200429916381836  and  1.7140790224075317\n",
      "total error:  6.818155646324158\n",
      "training error:  1.5075064897537231  and  3.714588165283203  and  2.3150134086608887\n",
      "total error:  7.537108063697815\n",
      "training error:  1.4472907781600952  and  2.121765613555908  and  1.5790189504623413\n",
      "total error:  5.148075342178345\n",
      "training error:  1.668892502784729  and  4.98082160949707  and  1.8640614748001099\n",
      "total error:  8.51377558708191\n",
      "training error:  1.5659974813461304  and  2.3975393772125244  and  2.029176712036133\n",
      "total error:  5.992713570594788\n",
      "training error:  1.4692498445510864  and  3.066788673400879  and  1.8447315692901611\n",
      "total error:  6.3807700872421265\n",
      "training error:  1.5459935665130615  and  2.181027889251709  and  1.8014155626296997\n",
      "total error:  5.52843701839447\n",
      "training error:  1.600533366203308  and  3.981113910675049  and  2.254746437072754\n",
      "total error:  7.836393713951111\n",
      "training error:  1.521396279335022  and  2.258796215057373  and  1.8429713249206543\n",
      "total error:  5.623163819313049\n",
      "training error:  1.4672542810440063  and  2.2296013832092285  and  1.5393086671829224\n",
      "total error:  5.236164331436157\n",
      "training error:  1.6602048873901367  and  4.762845039367676  and  1.656095266342163\n",
      "total error:  8.079145193099976\n",
      "training error:  1.4212160110473633  and  2.150528907775879  and  1.6032147407531738\n",
      "total error:  5.174959659576416\n",
      "training error:  1.5056018829345703  and  3.204254388809204  and  2.496727228164673\n",
      "total error:  7.206583499908447\n",
      "training error:  1.4627058506011963  and  1.9887363910675049  and  1.5254734754562378\n",
      "total error:  4.976915717124939\n",
      "training error:  1.4849191904067993  and  2.755459785461426  and  1.5684435367584229\n",
      "total error:  5.808822512626648\n",
      "training error:  1.4504116773605347  and  2.0233681201934814  and  1.433302402496338\n",
      "total error:  4.907082200050354\n",
      "training error:  1.403477668762207  and  3.086465835571289  and  1.7978168725967407\n",
      "total error:  6.287760376930237\n",
      "training error:  1.4063222408294678  and  2.220972776412964  and  1.5642045736312866\n",
      "total error:  5.191499590873718\n",
      "training error:  1.4577982425689697  and  3.015289068222046  and  1.5732522010803223\n",
      "total error:  6.046339511871338\n",
      "training error:  1.4713244438171387  and  2.6499712467193604  and  1.6758896112442017\n",
      "total error:  5.797185301780701\n",
      "training error:  1.4295716285705566  and  2.246626615524292  and  1.7206506729125977\n",
      "total error:  5.396848917007446\n",
      "training error:  1.4051223993301392  and  2.2541351318359375  and  1.5398807525634766\n",
      "total error:  5.199138283729553\n",
      "training error:  1.4838911294937134  and  3.3342971801757812  and  1.8388001918792725\n",
      "total error:  6.656988501548767\n",
      "training error:  1.4080811738967896  and  2.37394118309021  and  1.7719271183013916\n",
      "total error:  5.553949475288391\n",
      "training error:  1.4167193174362183  and  2.5073294639587402  and  2.276775598526001\n",
      "total error:  6.2008243799209595\n",
      "training error:  1.593254804611206  and  2.5095462799072266  and  1.5190551280975342\n",
      "total error:  5.621856212615967\n",
      "training error:  1.4191806316375732  and  2.070807456970215  and  1.607421875\n",
      "total error:  5.097409963607788\n",
      "training error:  1.3825287818908691  and  2.028146266937256  and  1.5381717681884766\n",
      "total error:  4.948846817016602\n",
      "training error:  1.5165600776672363  and  2.5514116287231445  and  1.6005423069000244\n",
      "total error:  5.668514013290405\n",
      "training error:  1.3966736793518066  and  2.0634634494781494  and  1.6467410326004028\n",
      "total error:  5.106878161430359\n",
      "training error:  1.3417237997055054  and  1.9802135229110718  and  1.5220293998718262\n",
      "total error:  4.843966722488403\n",
      "training error:  1.4336861371994019  and  2.734143018722534  and  1.6876046657562256\n",
      "total error:  5.855433821678162\n",
      "training error:  1.4005711078643799  and  2.5059797763824463  and  1.9008069038391113\n",
      "total error:  5.8073577880859375\n",
      "training error:  1.6081995964050293  and  5.37617301940918  and  1.738373875617981\n",
      "total error:  8.72274649143219\n",
      "training error:  1.3564269542694092  and  2.536990165710449  and  2.1222925186157227\n",
      "total error:  6.015709638595581\n",
      "training error:  1.3551205396652222  and  2.3952643871307373  and  2.0727877616882324\n",
      "total error:  5.823172688484192\n",
      "training error:  1.6084727048873901  and  4.543265342712402  and  1.5730006694793701\n",
      "total error:  7.724738717079163\n",
      "training error:  1.3621330261230469  and  3.6652450561523438  and  2.381688117980957\n",
      "total error:  7.409066200256348\n",
      "training error:  1.4285327196121216  and  2.1705639362335205  and  1.5046753883361816\n",
      "total error:  5.103772044181824\n",
      "training error:  1.4581173658370972  and  4.323886394500732  and  1.7088799476623535\n",
      "total error:  7.490883708000183\n",
      "training error:  1.3678330183029175  and  2.015742778778076  and  1.8747378587722778\n",
      "total error:  5.2583136558532715\n",
      "training error:  1.314620018005371  and  1.91740083694458  and  1.6003926992416382\n",
      "total error:  4.832413554191589\n",
      "training error:  1.3205556869506836  and  2.013467788696289  and  1.568152666091919\n",
      "total error:  4.902176141738892\n",
      "training error:  1.3339757919311523  and  2.2643632888793945  and  1.5149991512298584\n",
      "total error:  5.113338232040405\n",
      "training error:  1.2766563892364502  and  1.9928683042526245  and  1.4947664737701416\n",
      "total error:  4.764291167259216\n",
      "training error:  1.3432774543762207  and  2.0653200149536133  and  1.5460169315338135\n",
      "total error:  4.9546144008636475\n",
      "training error:  1.3033714294433594  and  1.9769270420074463  and  1.6170923709869385\n",
      "total error:  4.897390842437744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.3214328289031982  and  2.006110429763794  and  1.4769978523254395\n",
      "total error:  4.804541110992432\n",
      "training error:  1.2692029476165771  and  2.000307083129883  and  1.5152500867843628\n",
      "total error:  4.784760117530823\n",
      "training error:  1.3622065782546997  and  2.644624948501587  and  1.4285255670547485\n",
      "total error:  5.435357093811035\n",
      "training error:  1.2582054138183594  and  1.9149502515792847  and  1.434206247329712\n",
      "total error:  4.607361912727356\n",
      "training error:  1.3171777725219727  and  2.3729639053344727  and  1.6659202575683594\n",
      "total error:  5.356061935424805\n",
      "training error:  1.508386254310608  and  4.744759559631348  and  1.836484670639038\n",
      "total error:  8.089630484580994\n",
      "training error:  1.3420811891555786  and  1.9973245859146118  and  1.783073902130127\n",
      "total error:  5.122479677200317\n",
      "training error:  1.2929071187973022  and  3.9035634994506836  and  2.3464736938476562\n",
      "total error:  7.542944312095642\n",
      "training error:  1.291176438331604  and  1.9643149375915527  and  1.4076800346374512\n",
      "total error:  4.663171410560608\n",
      "training error:  1.3834528923034668  and  3.6859312057495117  and  1.6092414855957031\n",
      "total error:  6.678625583648682\n",
      "training error:  1.2906748056411743  and  2.176025390625  and  1.6044319868087769\n",
      "total error:  5.071132183074951\n",
      "training error:  1.3796534538269043  and  3.5569958686828613  and  2.576594352722168\n",
      "total error:  7.513243675231934\n",
      "training error:  1.3166440725326538  and  2.6792984008789062  and  1.5779658555984497\n",
      "total error:  5.57390832901001\n",
      "training error:  1.3640928268432617  and  2.5856754779815674  and  1.4897897243499756\n",
      "total error:  5.439558029174805\n",
      "training error:  1.3462915420532227  and  2.534419059753418  and  1.5299583673477173\n",
      "total error:  5.410668969154358\n",
      "training error:  1.246628999710083  and  2.1557092666625977  and  1.5175384283065796\n",
      "total error:  4.91987669467926\n",
      "training error:  1.246886968612671  and  1.8605148792266846  and  1.4935402870178223\n",
      "total error:  4.600942134857178\n",
      "training error:  1.2771868705749512  and  2.4300899505615234  and  1.4225819110870361\n",
      "total error:  5.129858732223511\n",
      "training error:  1.257096529006958  and  2.38226318359375  and  1.627458095550537\n",
      "total error:  5.266817808151245\n",
      "training error:  1.2549585103988647  and  2.191465377807617  and  1.5464346408843994\n",
      "total error:  4.992858529090881\n",
      "training error:  1.2794103622436523  and  2.5180702209472656  and  1.5242987871170044\n",
      "total error:  5.321779370307922\n",
      "training error:  1.3013365268707275  and  2.0569515228271484  and  1.5648252964019775\n",
      "total error:  4.9231133460998535\n",
      "training error:  1.2723850011825562  and  2.129689931869507  and  1.4478166103363037\n",
      "total error:  4.849891543388367\n",
      "training error:  1.3211736679077148  and  2.8462867736816406  and  1.4493718147277832\n",
      "total error:  5.616832256317139\n",
      "training error:  1.3060133457183838  and  3.868652582168579  and  2.2289862632751465\n",
      "total error:  7.403652191162109\n",
      "training error:  1.4124115705490112  and  4.470541954040527  and  1.4590238332748413\n",
      "total error:  7.34197735786438\n",
      "training error:  1.2782871723175049  and  2.081092596054077  and  1.371614933013916\n",
      "total error:  4.730994701385498\n",
      "training error:  1.2440974712371826  and  4.031392574310303  and  2.0246541500091553\n",
      "total error:  7.300144195556641\n",
      "training error:  1.2344400882720947  and  2.782935619354248  and  1.537347674369812\n",
      "total error:  5.554723381996155\n",
      "training error:  1.2370266914367676  and  2.024850606918335  and  1.5311689376831055\n",
      "total error:  4.793046236038208\n",
      "training error:  1.336477518081665  and  2.5757176876068115  and  1.5715605020523071\n",
      "total error:  5.483755707740784\n",
      "training error:  1.4038864374160767  and  3.2519545555114746  and  1.5708931684494019\n",
      "total error:  6.226734161376953\n",
      "training error:  1.3382810354232788  and  2.894540309906006  and  1.5434600114822388\n",
      "total error:  5.776281356811523\n",
      "training error:  1.3176385164260864  and  2.6288349628448486  and  1.8660496473312378\n",
      "total error:  5.812523126602173\n",
      "training error:  1.2316397428512573  and  1.9061024188995361  and  1.4371724128723145\n",
      "total error:  4.574914574623108\n",
      "training error:  1.2539305686950684  and  2.8725249767303467  and  1.4704502820968628\n",
      "total error:  5.596905827522278\n",
      "training error:  1.190657615661621  and  1.8498797416687012  and  1.2749214172363281\n",
      "total error:  4.31545877456665\n",
      "training error:  1.2337865829467773  and  3.4673986434936523  and  1.865824818611145\n",
      "total error:  6.567010045051575\n",
      "training error:  1.2164522409439087  and  2.123986005783081  and  1.4282797574996948\n",
      "total error:  4.768718004226685\n",
      "training error:  1.3002831935882568  and  2.8238911628723145  and  1.3972505331039429\n",
      "total error:  5.521424889564514\n",
      "training error:  1.3370898962020874  and  3.7199835777282715  and  1.5584063529968262\n",
      "total error:  6.615479826927185\n",
      "training error:  1.2673993110656738  and  2.0205488204956055  and  1.5476754903793335\n",
      "total error:  4.835623621940613\n",
      "training error:  1.2812485694885254  and  2.265474557876587  and  1.8248459100723267\n",
      "total error:  5.371569037437439\n",
      "training error:  1.2533000707626343  and  1.9456815719604492  and  1.4778900146484375\n",
      "total error:  4.676871657371521\n",
      "training error:  1.2595160007476807  and  2.922497034072876  and  1.489222764968872\n",
      "total error:  5.671235799789429\n",
      "training error:  1.238560438156128  and  1.9338350296020508  and  1.5534554719924927\n",
      "total error:  4.725850939750671\n",
      "training error:  1.2136337757110596  and  1.9937429428100586  and  1.6878033876419067\n",
      "total error:  4.895180106163025\n",
      "training error:  1.2536611557006836  and  2.0426411628723145  and  1.5178322792053223\n",
      "total error:  4.81413459777832\n",
      "training error:  1.207140326499939  and  2.3808586597442627  and  1.5571393966674805\n",
      "total error:  5.145138382911682\n",
      "training error:  1.213295578956604  and  1.9113502502441406  and  1.6475447416305542\n",
      "total error:  4.772190570831299\n",
      "training error:  1.2139157056808472  and  1.8445830345153809  and  1.410376787185669\n",
      "total error:  4.468875527381897\n",
      "training error:  1.2456039190292358  and  1.8879785537719727  and  1.597869634628296\n",
      "total error:  4.731452107429504\n",
      "training error:  1.186189889907837  and  2.027585983276367  and  1.522674560546875\n",
      "total error:  4.736450433731079\n",
      "training error:  1.1937888860702515  and  1.9053401947021484  and  1.500396966934204\n",
      "total error:  4.599526047706604\n",
      "training error:  1.1781435012817383  and  1.7462682723999023  and  1.4022595882415771\n",
      "total error:  4.326671361923218\n",
      "training error:  1.2278164625167847  and  1.906337857246399  and  1.3541271686553955\n",
      "total error:  4.488281488418579\n",
      "training error:  1.184563159942627  and  1.7174571752548218  and  1.337207555770874\n",
      "total error:  4.239227890968323\n",
      "training error:  1.170409083366394  and  1.7467159032821655  and  1.3866357803344727\n",
      "total error:  4.303760766983032\n",
      "training error:  1.1989505290985107  and  1.736274003982544  and  1.3659601211547852\n",
      "total error:  4.30118465423584\n",
      "training error:  1.1997954845428467  and  1.8272267580032349  and  1.308763027191162\n",
      "total error:  4.335785269737244\n",
      "training error:  1.1957881450653076  and  1.8625328540802002  and  1.358252763748169\n",
      "total error:  4.416573762893677\n",
      "training error:  1.1452326774597168  and  1.9163458347320557  and  1.313119888305664\n",
      "total error:  4.3746984004974365\n",
      "training error:  1.198343276977539  and  1.8883000612258911  and  1.3431808948516846\n",
      "total error:  4.429824233055115\n",
      "training error:  1.157740831375122  and  1.8185229301452637  and  1.3340754508972168\n",
      "total error:  4.3103392124176025\n",
      "training error:  1.1808710098266602  and  1.7444939613342285  and  1.397006630897522\n",
      "total error:  4.322371602058411\n",
      "training error:  1.1513054370880127  and  1.7504850625991821  and  1.3499207496643066\n",
      "total error:  4.2517112493515015\n",
      "training error:  1.1837058067321777  and  1.7505803108215332  and  1.3558611869812012\n",
      "total error:  4.290147304534912\n",
      "training error:  1.158575415611267  and  1.7344458103179932  and  1.3609048128128052\n",
      "total error:  4.253926038742065\n",
      "training error:  1.1931676864624023  and  1.744869589805603  and  1.3789573907852173\n",
      "total error:  4.316994667053223\n",
      "training error:  1.1653554439544678  and  2.0908074378967285  and  1.34575355052948\n",
      "total error:  4.601916432380676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.1857914924621582  and  1.9521067142486572  and  1.3319575786590576\n",
      "total error:  4.469855785369873\n",
      "training error:  1.1738567352294922  and  1.784622311592102  and  1.3472869396209717\n",
      "total error:  4.305765986442566\n",
      "training error:  1.1662921905517578  and  1.9954631328582764  and  1.3303676843643188\n",
      "total error:  4.492123007774353\n",
      "training error:  1.1809489727020264  and  1.805793046951294  and  1.321408987045288\n",
      "total error:  4.308151006698608\n",
      "training error:  1.186049222946167  and  1.7189922332763672  and  1.327791452407837\n",
      "total error:  4.232832908630371\n",
      "training error:  1.1680128574371338  and  1.6822724342346191  and  1.288053274154663\n",
      "total error:  4.138338565826416\n",
      "training error:  1.1668903827667236  and  1.847100853919983  and  1.422107219696045\n",
      "total error:  4.4360984563827515\n",
      "training error:  1.1537805795669556  and  1.8025716543197632  and  1.3597456216812134\n",
      "total error:  4.316097855567932\n",
      "training error:  1.1695475578308105  and  1.767630934715271  and  1.3229212760925293\n",
      "total error:  4.260099768638611\n",
      "training error:  1.1584973335266113  and  1.7349539995193481  and  1.2583866119384766\n",
      "total error:  4.151837944984436\n",
      "training error:  1.1783795356750488  and  1.7868480682373047  and  1.3065049648284912\n",
      "total error:  4.271732568740845\n",
      "training error:  1.1541748046875  and  1.9638423919677734  and  1.3020515441894531\n",
      "total error:  4.420068740844727\n",
      "training error:  1.1994242668151855  and  2.060421943664551  and  1.3694432973861694\n",
      "total error:  4.629289507865906\n",
      "training error:  1.1205052137374878  and  1.783503532409668  and  1.3071191310882568\n",
      "total error:  4.211127877235413\n",
      "training error:  1.1343401670455933  and  1.691340684890747  and  1.3471276760101318\n",
      "total error:  4.172808527946472\n",
      "training error:  1.1286463737487793  and  1.8244818449020386  and  1.2679662704467773\n",
      "total error:  4.221094489097595\n",
      "training error:  1.1482130289077759  and  1.6643669605255127  and  1.3391637802124023\n",
      "total error:  4.151743769645691\n",
      "training error:  1.1488661766052246  and  1.6764930486679077  and  1.3023401498794556\n",
      "total error:  4.127699375152588\n",
      "training error:  1.1305427551269531  and  1.6891032457351685  and  1.3095744848251343\n",
      "total error:  4.129220485687256\n",
      "training error:  1.1221718788146973  and  1.6945626735687256  and  1.2556207180023193\n",
      "total error:  4.072355270385742\n",
      "training error:  1.1459898948669434  and  1.7411596775054932  and  1.3400156497955322\n",
      "total error:  4.227165222167969\n",
      "training error:  1.1629688739776611  and  1.67099928855896  and  1.3092361688613892\n",
      "total error:  4.14320433139801\n",
      "training error:  1.1301894187927246  and  1.7826042175292969  and  1.24566650390625\n",
      "total error:  4.1584601402282715\n",
      "training error:  1.1174750328063965  and  1.8173571825027466  and  1.2908289432525635\n",
      "total error:  4.2256611585617065\n",
      "training error:  1.14493727684021  and  1.7178186178207397  and  1.3361750841140747\n",
      "total error:  4.198930978775024\n",
      "training error:  1.1381319761276245  and  1.6534888744354248  and  1.2932565212249756\n",
      "total error:  4.084877371788025\n",
      "training error:  1.133261799812317  and  1.718676209449768  and  1.369217038154602\n",
      "total error:  4.221155047416687\n",
      "training error:  1.1345770359039307  and  1.8024708032608032  and  1.2957637310028076\n",
      "total error:  4.2328115701675415\n",
      "training error:  1.144283413887024  and  1.9071736335754395  and  1.2921757698059082\n",
      "total error:  4.343632817268372\n",
      "training error:  1.1266447305679321  and  1.6619770526885986  and  1.2960703372955322\n",
      "total error:  4.084692120552063\n",
      "training error:  1.1175049543380737  and  1.7022325992584229  and  1.2822823524475098\n",
      "total error:  4.102019906044006\n",
      "training error:  1.118101954460144  and  1.6562012434005737  and  1.285285234451294\n",
      "total error:  4.059588432312012\n",
      "training error:  1.1160261631011963  and  1.6208088397979736  and  1.308199405670166\n",
      "total error:  4.045034408569336\n",
      "training error:  1.1131031513214111  and  1.633758306503296  and  1.2483512163162231\n",
      "total error:  3.99521267414093\n",
      "training error:  1.09293532371521  and  1.6652029752731323  and  1.330432415008545\n",
      "total error:  4.088570713996887\n",
      "training error:  1.1235146522521973  and  1.6280875205993652  and  1.2614277601242065\n",
      "total error:  4.013029932975769\n",
      "training error:  1.1438241004943848  and  1.627316951751709  and  1.3148473501205444\n",
      "total error:  4.085988402366638\n",
      "training error:  1.0936256647109985  and  1.6674835681915283  and  1.2799267768859863\n",
      "total error:  4.041036009788513\n",
      "training error:  1.132226586341858  and  1.651090383529663  and  1.262089729309082\n",
      "total error:  4.045406699180603\n",
      "training error:  1.103692889213562  and  1.7041754722595215  and  1.2569423913955688\n",
      "total error:  4.064810752868652\n",
      "training error:  1.0917272567749023  and  1.6587562561035156  and  1.2316231727600098\n",
      "total error:  3.9821066856384277\n",
      "training error:  1.1007646322250366  and  1.6629743576049805  and  1.311069369316101\n",
      "total error:  4.074808359146118\n",
      "training error:  1.1185322999954224  and  1.6645214557647705  and  1.3413918018341064\n",
      "total error:  4.124445557594299\n",
      "training error:  1.109072208404541  and  1.6670459508895874  and  1.2668421268463135\n",
      "total error:  4.042960286140442\n",
      "training error:  1.129636526107788  and  1.610817551612854  and  1.2579143047332764\n",
      "total error:  3.9983683824539185\n",
      "training error:  1.1437616348266602  and  1.7419211864471436  and  1.2822864055633545\n",
      "total error:  4.167969226837158\n",
      "training error:  1.1267774105072021  and  1.6335396766662598  and  1.2902363538742065\n",
      "total error:  4.0505534410476685\n",
      "training error:  1.1001920700073242  and  1.626186490058899  and  1.3226228952407837\n",
      "total error:  4.049001455307007\n",
      "training error:  1.094893217086792  and  1.631134271621704  and  1.3185878992080688\n",
      "total error:  4.044615387916565\n",
      "training error:  1.0938769578933716  and  1.5662484169006348  and  1.2529220581054688\n",
      "total error:  3.913047432899475\n",
      "training error:  1.1456547975540161  and  1.9158076047897339  and  1.3393748998641968\n",
      "total error:  4.400837302207947\n",
      "training error:  1.1126371622085571  and  1.637966513633728  and  1.2473784685134888\n",
      "total error:  3.997982144355774\n",
      "training error:  1.118727207183838  and  1.587039589881897  and  1.2648687362670898\n",
      "total error:  3.9706355333328247\n",
      "training error:  1.0895593166351318  and  1.6442168951034546  and  1.2519850730895996\n",
      "total error:  3.985761284828186\n",
      "training error:  1.1365787982940674  and  1.8201656341552734  and  1.3029495477676392\n",
      "total error:  4.25969398021698\n",
      "training error:  1.1005862951278687  and  1.5699517726898193  and  1.2454869747161865\n",
      "total error:  3.9160250425338745\n",
      "training error:  1.1123759746551514  and  1.6083232164382935  and  1.2493455410003662\n",
      "total error:  3.970044732093811\n",
      "training error:  1.1203570365905762  and  1.5903739929199219  and  1.2809332609176636\n",
      "total error:  3.9916642904281616\n",
      "training error:  1.123551845550537  and  1.821550965309143  and  1.293210744857788\n",
      "total error:  4.238313555717468\n",
      "training error:  1.1331591606140137  and  1.6180951595306396  and  1.284224271774292\n",
      "total error:  4.035478591918945\n",
      "training error:  1.0928891897201538  and  1.5940015316009521  and  1.2689447402954102\n",
      "total error:  3.955835461616516\n",
      "training error:  1.0990009307861328  and  1.6050053834915161  and  1.2521493434906006\n",
      "total error:  3.9561556577682495\n",
      "training error:  1.1080013513565063  and  1.613713264465332  and  1.2214338779449463\n",
      "total error:  3.9431484937667847\n",
      "training error:  1.1085892915725708  and  1.7754685878753662  and  1.298976182937622\n",
      "total error:  4.183034062385559\n",
      "training error:  1.0974681377410889  and  1.6552183628082275  and  1.2543048858642578\n",
      "total error:  4.006991386413574\n",
      "training error:  1.1403381824493408  and  1.5985314846038818  and  1.2440578937530518\n",
      "total error:  3.9829275608062744\n",
      "training error:  1.1323978900909424  and  1.6611850261688232  and  1.355715036392212\n",
      "total error:  4.1492979526519775\n",
      "training error:  1.0951409339904785  and  1.6613776683807373  and  1.3065165281295776\n",
      "total error:  4.0630351305007935\n",
      "training error:  1.063819169998169  and  1.6243517398834229  and  1.3339452743530273\n",
      "total error:  4.022116184234619\n",
      "training error:  1.1025071144104004  and  1.6187208890914917  and  1.2663164138793945\n",
      "total error:  3.9875444173812866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.082651972770691  and  1.5694411993026733  and  1.2398974895477295\n",
      "total error:  3.8919906616210938\n",
      "training error:  1.1065235137939453  and  1.6438207626342773  and  1.2793476581573486\n",
      "total error:  4.029691934585571\n",
      "training error:  1.0807836055755615  and  1.5614514350891113  and  1.256742000579834\n",
      "total error:  3.898977041244507\n",
      "training error:  1.100866675376892  and  1.6280901432037354  and  1.2389788627624512\n",
      "total error:  3.9679356813430786\n",
      "training error:  1.118648886680603  and  1.7439281940460205  and  1.266235113143921\n",
      "total error:  4.128812193870544\n",
      "training error:  1.1081326007843018  and  1.6104834079742432  and  1.2418253421783447\n",
      "total error:  3.9604413509368896\n",
      "training error:  1.0709325075149536  and  1.591498851776123  and  1.266540288925171\n",
      "total error:  3.9289716482162476\n",
      "training error:  1.1084856986999512  and  1.6166077852249146  and  1.2851698398590088\n",
      "total error:  4.0102633237838745\n",
      "training error:  1.0877759456634521  and  1.586242437362671  and  1.2432353496551514\n",
      "total error:  3.9172537326812744\n",
      "training error:  1.0986666679382324  and  1.5654456615447998  and  1.300690770149231\n",
      "total error:  3.964803099632263\n",
      "training error:  1.1244360208511353  and  1.6108508110046387  and  1.2322142124176025\n",
      "total error:  3.9675010442733765\n",
      "training error:  1.0699670314788818  and  1.5853201150894165  and  1.250629186630249\n",
      "total error:  3.9059163331985474\n",
      "training error:  1.0812392234802246  and  1.583701729774475  and  1.2241334915161133\n",
      "total error:  3.889074444770813\n",
      "training error:  1.0852601528167725  and  1.6038938760757446  and  1.2623190879821777\n",
      "total error:  3.951473116874695\n",
      "training error:  1.1248173713684082  and  1.6164087057113647  and  1.2537397146224976\n",
      "total error:  3.9949657917022705\n",
      "training error:  1.0773414373397827  and  1.6596277952194214  and  1.272505760192871\n",
      "total error:  4.009474992752075\n",
      "training error:  1.0983331203460693  and  1.636617660522461  and  1.2634674310684204\n",
      "total error:  3.9984182119369507\n",
      "training error:  1.1172165870666504  and  1.544576644897461  and  1.2439444065093994\n",
      "total error:  3.9057376384735107\n",
      "training error:  1.0748955011367798  and  1.6751093864440918  and  1.2298282384872437\n",
      "total error:  3.9798331260681152\n",
      "training error:  1.079002022743225  and  1.617157220840454  and  1.2374420166015625\n",
      "total error:  3.9336012601852417\n",
      "training error:  1.080796480178833  and  1.695396900177002  and  1.318109393119812\n",
      "total error:  4.094302773475647\n",
      "training error:  1.0723528861999512  and  1.6718157529830933  and  1.2704463005065918\n",
      "total error:  4.014614939689636\n",
      "training error:  1.0938915014266968  and  1.6493138074874878  and  1.2355363368988037\n",
      "total error:  3.9787416458129883\n",
      "training error:  1.0847551822662354  and  1.6126072406768799  and  1.2471866607666016\n",
      "total error:  3.944549083709717\n",
      "training error:  1.0842543840408325  and  1.6754311323165894  and  1.2416290044784546\n",
      "total error:  4.0013145208358765\n",
      "training error:  1.104017734527588  and  1.70831298828125  and  1.210833191871643\n",
      "total error:  4.023163914680481\n",
      "training error:  1.0520527362823486  and  1.587618350982666  and  1.1987279653549194\n",
      "total error:  3.838399052619934\n",
      "training error:  1.1125717163085938  and  1.5752227306365967  and  1.246315360069275\n",
      "total error:  3.9341098070144653\n",
      "training error:  1.0578020811080933  and  1.573542594909668  and  1.240339756011963\n",
      "total error:  3.871684432029724\n",
      "training error:  1.074841856956482  and  1.5725438594818115  and  1.252790927886963\n",
      "total error:  3.9001766443252563\n",
      "training error:  1.0726641416549683  and  1.542494773864746  and  1.2273616790771484\n",
      "total error:  3.842520594596863\n",
      "training error:  1.0559613704681396  and  1.5772716999053955  and  1.239157795906067\n",
      "total error:  3.872390866279602\n",
      "training error:  1.0659047365188599  and  1.6505229473114014  and  1.2123690843582153\n",
      "total error:  3.9287967681884766\n",
      "training error:  1.062217354774475  and  1.568149447441101  and  1.2085447311401367\n",
      "total error:  3.838911533355713\n",
      "training error:  1.0735100507736206  and  1.5420126914978027  and  1.2115966081619263\n",
      "total error:  3.8271193504333496\n",
      "training error:  1.096257209777832  and  1.542409896850586  and  1.3055981397628784\n",
      "total error:  3.9442652463912964\n",
      "training error:  1.068683385848999  and  1.9793541431427002  and  1.3713769912719727\n",
      "total error:  4.419414520263672\n",
      "training error:  1.046271562576294  and  1.5825552940368652  and  1.2287499904632568\n",
      "total error:  3.857576847076416\n",
      "training error:  1.0855745077133179  and  1.5257463455200195  and  1.1964284181594849\n",
      "total error:  3.8077492713928223\n",
      "training error:  1.0314887762069702  and  1.5368537902832031  and  1.2147369384765625\n",
      "total error:  3.783079504966736\n",
      "training error:  1.0623037815093994  and  1.6013634204864502  and  1.2569167613983154\n",
      "total error:  3.920583963394165\n",
      "training error:  1.0448377132415771  and  1.5614380836486816  and  1.2351497411727905\n",
      "total error:  3.8414255380630493\n",
      "training error:  1.077803373336792  and  1.5442306995391846  and  1.2189819812774658\n",
      "total error:  3.8410160541534424\n",
      "training error:  1.0981533527374268  and  1.5438294410705566  and  1.2698988914489746\n",
      "total error:  3.911881685256958\n",
      "training error:  1.078798532485962  and  1.5117027759552002  and  1.2165051698684692\n",
      "total error:  3.8070064783096313\n",
      "training error:  1.0592454671859741  and  1.627641201019287  and  1.1953130960464478\n",
      "total error:  3.882199764251709\n",
      "training error:  1.0652439594268799  and  1.5186526775360107  and  1.204298734664917\n",
      "total error:  3.7881953716278076\n",
      "training error:  1.0723594427108765  and  1.5787158012390137  and  1.2248716354370117\n",
      "total error:  3.875946879386902\n",
      "training error:  1.0575289726257324  and  1.6122667789459229  and  1.2196252346038818\n",
      "total error:  3.889420986175537\n",
      "training error:  1.0606321096420288  and  1.5634896755218506  and  1.186123013496399\n",
      "total error:  3.8102447986602783\n",
      "training error:  1.0659812688827515  and  1.5986900329589844  and  1.2166249752044678\n",
      "total error:  3.8812962770462036\n",
      "training error:  1.0391464233398438  and  1.5281131267547607  and  1.254162311553955\n",
      "total error:  3.8214218616485596\n",
      "training error:  1.0675162076950073  and  1.5578794479370117  and  1.1705257892608643\n",
      "total error:  3.7959214448928833\n",
      "training error:  1.080957293510437  and  1.6652061939239502  and  1.2783889770507812\n",
      "total error:  4.0245524644851685\n",
      "training error:  1.0333038568496704  and  1.6398651599884033  and  1.2011014223098755\n",
      "total error:  3.874270439147949\n",
      "training error:  1.0681114196777344  and  1.522084355354309  and  1.1903122663497925\n",
      "total error:  3.780508041381836\n",
      "training error:  1.0440534353256226  and  1.6235651969909668  and  1.2539820671081543\n",
      "total error:  3.9216006994247437\n",
      "training error:  1.0643553733825684  and  1.4880762100219727  and  1.2439676523208618\n",
      "total error:  3.796399235725403\n",
      "training error:  1.0739859342575073  and  1.8138837814331055  and  1.2254059314727783\n",
      "total error:  4.113275647163391\n",
      "training error:  1.0392210483551025  and  1.5490641593933105  and  1.222447395324707\n",
      "total error:  3.81073260307312\n",
      "training error:  1.0838223695755005  and  1.533951997756958  and  1.1800870895385742\n",
      "total error:  3.7978614568710327\n",
      "training error:  1.0703896284103394  and  1.5289195775985718  and  1.23992919921875\n",
      "total error:  3.839238405227661\n",
      "training error:  1.0764036178588867  and  1.5092525482177734  and  1.2597084045410156\n",
      "total error:  3.845364570617676\n",
      "training error:  1.0483143329620361  and  1.5304572582244873  and  1.2048156261444092\n",
      "total error:  3.7835872173309326\n",
      "training error:  1.0628665685653687  and  1.5593321323394775  and  1.219743251800537\n",
      "total error:  3.8419419527053833\n",
      "training error:  1.077816128730774  and  1.5282416343688965  and  1.257596731185913\n",
      "total error:  3.8636544942855835\n",
      "training error:  1.03049635887146  and  1.473973274230957  and  1.2022086381912231\n",
      "total error:  3.70667827129364\n",
      "training error:  1.039921760559082  and  1.5345187187194824  and  1.2170953750610352\n",
      "total error:  3.7915358543395996\n",
      "training error:  1.061988353729248  and  1.7374119758605957  and  1.2328153848648071\n",
      "total error:  4.032215714454651\n",
      "training error:  1.0383684635162354  and  1.5288536548614502  and  1.2124087810516357\n",
      "total error:  3.7796308994293213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.029987096786499  and  1.51487398147583  and  1.2212293148040771\n",
      "total error:  3.7660903930664062\n",
      "training error:  1.067935585975647  and  1.5122263431549072  and  1.2857176065444946\n",
      "total error:  3.865879535675049\n",
      "training error:  1.068639874458313  and  1.5118070840835571  and  1.229353427886963\n",
      "total error:  3.809800386428833\n",
      "training error:  1.065548062324524  and  1.5322118997573853  and  1.2237532138824463\n",
      "total error:  3.8215131759643555\n",
      "training error:  1.0294502973556519  and  1.5439766645431519  and  1.2547876834869385\n",
      "total error:  3.828214645385742\n",
      "training error:  1.042989730834961  and  1.5254201889038086  and  1.2457191944122314\n",
      "total error:  3.814129114151001\n",
      "training error:  1.0366590023040771  and  1.5158822536468506  and  1.2146503925323486\n",
      "total error:  3.7671916484832764\n",
      "training error:  1.0300216674804688  and  1.5625526905059814  and  1.178154468536377\n",
      "total error:  3.770728826522827\n",
      "training error:  1.029659628868103  and  1.5105122327804565  and  1.2398873567581177\n",
      "total error:  3.7800592184066772\n",
      "training error:  1.0461987257003784  and  1.5171606540679932  and  1.1975122690200806\n",
      "total error:  3.760871648788452\n",
      "training error:  1.0589154958724976  and  1.5248557329177856  and  1.1775048971176147\n",
      "total error:  3.761276125907898\n",
      "training error:  1.034684658050537  and  1.5250673294067383  and  1.1851060390472412\n",
      "total error:  3.7448580265045166\n",
      "training error:  1.0225956439971924  and  1.5437942743301392  and  1.289442539215088\n",
      "total error:  3.8558324575424194\n",
      "training error:  1.0671035051345825  and  1.6107420921325684  and  1.2260257005691528\n",
      "total error:  3.9038712978363037\n",
      "training error:  1.0393884181976318  and  1.5313199758529663  and  1.1937627792358398\n",
      "total error:  3.764471173286438\n",
      "training error:  1.078629732131958  and  1.4696905612945557  and  1.171665072441101\n",
      "total error:  3.7199853658676147\n",
      "training error:  1.0321033000946045  and  1.5951507091522217  and  1.1926106214523315\n",
      "total error:  3.8198646306991577\n",
      "training error:  1.0303819179534912  and  1.5451931953430176  and  1.1998748779296875\n",
      "total error:  3.7754499912261963\n",
      "training error:  1.030068278312683  and  1.5062086582183838  and  1.159017562866211\n",
      "total error:  3.695294499397278\n",
      "training error:  1.0421733856201172  and  1.5101659297943115  and  1.1965563297271729\n",
      "total error:  3.7488956451416016\n",
      "training error:  1.0584386587142944  and  1.6323425769805908  and  1.223801851272583\n",
      "total error:  3.9145830869674683\n",
      "training error:  1.054037094116211  and  1.5466375350952148  and  1.163069248199463\n",
      "total error:  3.7637438774108887\n",
      "training error:  1.057855486869812  and  1.5321564674377441  and  1.2058427333831787\n",
      "total error:  3.795854687690735\n",
      "training error:  1.0056110620498657  and  1.5053508281707764  and  1.197205901145935\n",
      "total error:  3.708167791366577\n",
      "training error:  1.0491435527801514  and  1.5699976682662964  and  1.196016788482666\n",
      "total error:  3.8151580095291138\n",
      "training error:  1.0481659173965454  and  1.5195313692092896  and  1.1593811511993408\n",
      "total error:  3.727078437805176\n",
      "training error:  1.0299265384674072  and  1.4977576732635498  and  1.1812312602996826\n",
      "total error:  3.7089154720306396\n",
      "training error:  1.062819242477417  and  1.4805235862731934  and  1.2153377532958984\n",
      "total error:  3.758680582046509\n",
      "training error:  1.0323063135147095  and  1.5224111080169678  and  1.2239224910736084\n",
      "total error:  3.7786399126052856\n",
      "training error:  1.033247709274292  and  1.487772822380066  and  1.1648595333099365\n",
      "total error:  3.6858800649642944\n",
      "training error:  1.0361897945404053  and  1.6449499130249023  and  1.223583459854126\n",
      "total error:  3.9047231674194336\n",
      "training error:  1.0260698795318604  and  1.4625412225723267  and  1.1844172477722168\n",
      "total error:  3.673028349876404\n",
      "training error:  1.029867172241211  and  1.6362922191619873  and  1.2031084299087524\n",
      "total error:  3.8692678213119507\n",
      "training error:  1.0143706798553467  and  1.551007866859436  and  1.2215049266815186\n",
      "total error:  3.7868834733963013\n",
      "training error:  1.0133168697357178  and  1.548686146736145  and  1.1838852167129517\n",
      "total error:  3.7458882331848145\n",
      "training error:  1.0301059484481812  and  1.611273169517517  and  1.2167937755584717\n",
      "total error:  3.85817289352417\n",
      "training error:  1.0324897766113281  and  1.4690876007080078  and  1.1844600439071655\n",
      "total error:  3.6860374212265015\n",
      "training error:  1.04823899269104  and  1.6148264408111572  and  1.204385757446289\n",
      "total error:  3.8674511909484863\n",
      "training error:  1.0240468978881836  and  1.5060248374938965  and  1.2322943210601807\n",
      "total error:  3.7623660564422607\n",
      "training error:  1.0207459926605225  and  1.710101842880249  and  1.1827867031097412\n",
      "total error:  3.9136345386505127\n",
      "training error:  1.0317343473434448  and  1.5097503662109375  and  1.1999554634094238\n",
      "total error:  3.741440176963806\n",
      "training error:  1.0603275299072266  and  1.5063120126724243  and  1.1564245223999023\n",
      "total error:  3.7230640649795532\n",
      "training error:  1.0405408143997192  and  1.4579436779022217  and  1.2317582368850708\n",
      "total error:  3.7302427291870117\n",
      "training error:  1.0257031917572021  and  1.5028047561645508  and  1.1606059074401855\n",
      "total error:  3.6891138553619385\n",
      "training error:  1.0449912548065186  and  1.4521859884262085  and  1.1645445823669434\n",
      "total error:  3.6617218255996704\n",
      "training error:  1.022428274154663  and  1.5568304061889648  and  1.192880630493164\n",
      "total error:  3.772139310836792\n",
      "training error:  1.0268713235855103  and  1.4624714851379395  and  1.1623542308807373\n",
      "total error:  3.651697039604187\n",
      "training error:  1.0507707595825195  and  1.7363383769989014  and  1.1926966905593872\n",
      "total error:  3.979805827140808\n",
      "training error:  1.0159188508987427  and  1.490699052810669  and  1.2095292806625366\n",
      "total error:  3.7161471843719482\n",
      "training error:  1.0262565612792969  and  1.5354474782943726  and  1.269287347793579\n",
      "total error:  3.8309913873672485\n",
      "training error:  1.005702018737793  and  1.4705827236175537  and  1.2169663906097412\n",
      "total error:  3.693251132965088\n",
      "training error:  1.0150635242462158  and  1.4710489511489868  and  1.2035460472106934\n",
      "total error:  3.689658522605896\n",
      "training error:  1.0257558822631836  and  1.4701346158981323  and  1.1964536905288696\n",
      "total error:  3.6923441886901855\n",
      "training error:  1.0199376344680786  and  1.6781407594680786  and  1.1652313470840454\n",
      "total error:  3.8633097410202026\n",
      "training error:  1.024430751800537  and  1.5279788970947266  and  1.2242908477783203\n",
      "total error:  3.776700496673584\n",
      "training error:  1.0216788053512573  and  1.5445739030838013  and  1.2291220426559448\n",
      "total error:  3.7953747510910034\n",
      "training error:  1.017088532447815  and  1.5135747194290161  and  1.2229019403457642\n",
      "total error:  3.753565192222595\n",
      "training error:  1.0077028274536133  and  1.505213737487793  and  1.1711719036102295\n",
      "total error:  3.6840884685516357\n",
      "training error:  1.0275181531906128  and  1.4804534912109375  and  1.2696980237960815\n",
      "total error:  3.777669668197632\n",
      "training error:  1.0080413818359375  and  1.4432567358016968  and  1.2277812957763672\n",
      "total error:  3.6790794134140015\n",
      "training error:  0.9999887943267822  and  1.4597268104553223  and  1.1580653190612793\n",
      "total error:  3.617780923843384\n",
      "training error:  1.0096979141235352  and  1.6865988969802856  and  1.3232628107070923\n",
      "total error:  4.019559621810913\n",
      "training error:  1.0043023824691772  and  1.6075128316879272  and  1.2492321729660034\n",
      "total error:  3.861047387123108\n",
      "training error:  1.0081239938735962  and  1.4618377685546875  and  1.2084437608718872\n",
      "total error:  3.678405523300171\n",
      "training error:  0.9983049631118774  and  1.5114330053329468  and  1.1475317478179932\n",
      "total error:  3.6572697162628174\n",
      "training error:  1.0355238914489746  and  1.5234832763671875  and  1.2332345247268677\n",
      "total error:  3.79224169254303\n",
      "training error:  1.0244903564453125  and  1.5431723594665527  and  1.2248568534851074\n",
      "total error:  3.7925195693969727\n",
      "training error:  1.0195033550262451  and  1.4713428020477295  and  1.178559422492981\n",
      "total error:  3.6694055795669556\n",
      "training error:  1.0020071268081665  and  1.4638290405273438  and  1.1630998849868774\n",
      "total error:  3.6289360523223877\n",
      "training error:  0.9851049184799194  and  1.4628506898880005  and  1.1969141960144043\n",
      "total error:  3.644869804382324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  1.0164449214935303  and  1.5324208736419678  and  1.1856513023376465\n",
      "total error:  3.7345170974731445\n",
      "training error:  0.9769495129585266  and  1.553206205368042  and  1.2043198347091675\n",
      "total error:  3.734475553035736\n",
      "training error:  1.009903073310852  and  1.7243211269378662  and  1.2075533866882324\n",
      "total error:  3.9417775869369507\n",
      "training error:  0.9907833337783813  and  1.4781527519226074  and  1.1599719524383545\n",
      "total error:  3.6289080381393433\n",
      "training error:  1.0300356149673462  and  1.5102877616882324  and  1.1479480266571045\n",
      "total error:  3.688271403312683\n",
      "training error:  1.0111464262008667  and  1.4378809928894043  and  1.1392751932144165\n",
      "total error:  3.5883026123046875\n",
      "training error:  1.0282728672027588  and  1.4978820085525513  and  1.175278902053833\n",
      "total error:  3.701433777809143\n",
      "training error:  1.0427746772766113  and  1.4995089769363403  and  1.2064015865325928\n",
      "total error:  3.7486852407455444\n",
      "training error:  1.0033353567123413  and  1.4717488288879395  and  1.1946532726287842\n",
      "total error:  3.669737458229065\n",
      "training error:  1.012441635131836  and  1.4791181087493896  and  1.3051146268844604\n",
      "total error:  3.796674370765686\n",
      "training error:  0.9916924238204956  and  1.4421963691711426  and  1.1761753559112549\n",
      "total error:  3.610064148902893\n",
      "training error:  0.997399091720581  and  1.4749717712402344  and  1.2257115840911865\n",
      "total error:  3.698082447052002\n",
      "training error:  0.9936738014221191  and  1.493309736251831  and  1.2162694931030273\n",
      "total error:  3.7032530307769775\n",
      "training error:  1.0160558223724365  and  1.452932596206665  and  1.1658927202224731\n",
      "total error:  3.6348811388015747\n",
      "training error:  1.0015525817871094  and  1.5532456636428833  and  1.237591028213501\n",
      "total error:  3.7923892736434937\n",
      "training error:  0.9843428730964661  and  1.486104965209961  and  1.2123154401779175\n",
      "total error:  3.6827632784843445\n",
      "training error:  1.0024032592773438  and  1.5328068733215332  and  1.2218773365020752\n",
      "total error:  3.757087469100952\n",
      "training error:  1.012192726135254  and  1.5036656856536865  and  1.1379634141921997\n",
      "total error:  3.65382182598114\n",
      "training error:  0.9842840433120728  and  1.4481902122497559  and  1.1822330951690674\n",
      "total error:  3.614707350730896\n",
      "training error:  0.9873908758163452  and  1.4473462104797363  and  1.1471049785614014\n",
      "total error:  3.581842064857483\n",
      "training error:  1.0046091079711914  and  1.4293744564056396  and  1.1339335441589355\n",
      "total error:  3.5679171085357666\n",
      "training error:  1.0158584117889404  and  1.6428660154342651  and  1.2009223699569702\n",
      "total error:  3.859646797180176\n",
      "training error:  0.9846881628036499  and  1.6343252658843994  and  1.2312557697296143\n",
      "total error:  3.8502691984176636\n",
      "training error:  0.9894236326217651  and  1.5060558319091797  and  1.1408708095550537\n",
      "total error:  3.6363502740859985\n",
      "training error:  1.0038487911224365  and  1.485397219657898  and  1.1900020837783813\n",
      "total error:  3.679248094558716\n",
      "training error:  0.9945077896118164  and  1.4530338048934937  and  1.2181737422943115\n",
      "total error:  3.6657153367996216\n",
      "training error:  1.0055148601531982  and  1.4266250133514404  and  1.1588778495788574\n",
      "total error:  3.591017723083496\n",
      "training error:  0.9991443157196045  and  1.4772381782531738  and  1.133413314819336\n",
      "total error:  3.6097958087921143\n",
      "training error:  0.9999754428863525  and  1.4582500457763672  and  1.1524817943572998\n",
      "total error:  3.6107072830200195\n",
      "training error:  1.001716136932373  and  1.510759949684143  and  1.1864157915115356\n",
      "total error:  3.6988918781280518\n",
      "training error:  1.0067648887634277  and  1.4045355319976807  and  1.1568915843963623\n",
      "total error:  3.5681920051574707\n",
      "training error:  0.9840114116668701  and  1.4339103698730469  and  1.220558524131775\n",
      "total error:  3.638480305671692\n",
      "training error:  0.9913424253463745  and  1.5775268077850342  and  1.1856803894042969\n",
      "total error:  3.7545496225357056\n",
      "training error:  0.9924851655960083  and  1.4490070343017578  and  1.1788721084594727\n",
      "total error:  3.6203643083572388\n",
      "training error:  0.971244752407074  and  1.44505774974823  and  1.1302344799041748\n",
      "total error:  3.5465369820594788\n",
      "training error:  0.9786745309829712  and  1.4617793560028076  and  1.185378074645996\n",
      "total error:  3.625831961631775\n",
      "training error:  0.9898366928100586  and  1.4587228298187256  and  1.181595802307129\n",
      "total error:  3.630155324935913\n",
      "training error:  0.958996057510376  and  1.5455710887908936  and  1.173031210899353\n",
      "total error:  3.6775983572006226\n",
      "training error:  0.9914953708648682  and  1.4545561075210571  and  1.1464512348175049\n",
      "total error:  3.59250271320343\n",
      "training error:  0.9971606135368347  and  1.4783048629760742  and  1.1658138036727905\n",
      "total error:  3.6412792801856995\n",
      "training error:  1.0143089294433594  and  1.4265445470809937  and  1.1257703304290771\n",
      "total error:  3.56662380695343\n",
      "training error:  0.9965859055519104  and  1.4217990636825562  and  1.1391210556030273\n",
      "total error:  3.557506024837494\n",
      "training error:  0.968552827835083  and  1.473920226097107  and  1.2624702453613281\n",
      "total error:  3.704943299293518\n",
      "training error:  0.9915691614151001  and  1.4224344491958618  and  1.125387191772461\n",
      "total error:  3.539390802383423\n",
      "training error:  1.0044784545898438  and  1.5077834129333496  and  1.1597206592559814\n",
      "total error:  3.671982526779175\n",
      "training error:  0.9733738899230957  and  1.5210939645767212  and  1.1437257528305054\n",
      "total error:  3.6381936073303223\n",
      "training error:  0.989139199256897  and  1.4673725366592407  and  1.1377187967300415\n",
      "total error:  3.594230532646179\n",
      "training error:  0.9781379699707031  and  1.4878201484680176  and  1.1904659271240234\n",
      "total error:  3.656424045562744\n",
      "training error:  0.9919207096099854  and  1.4367880821228027  and  1.1125028133392334\n",
      "total error:  3.5412116050720215\n",
      "training error:  0.9743490219116211  and  1.4427506923675537  and  1.1670291423797607\n",
      "total error:  3.5841288566589355\n",
      "training error:  0.9850219488143921  and  1.5305547714233398  and  1.1370904445648193\n",
      "total error:  3.6526671648025513\n",
      "training error:  0.9849753975868225  and  1.4553637504577637  and  1.1501071453094482\n",
      "total error:  3.5904462933540344\n",
      "training error:  0.9779796600341797  and  1.4336776733398438  and  1.153733730316162\n",
      "total error:  3.5653910636901855\n",
      "training error:  0.9735103845596313  and  1.4991645812988281  and  1.1572682857513428\n",
      "total error:  3.6299432516098022\n",
      "training error:  0.9941129088401794  and  1.433318853378296  and  1.1761471033096313\n",
      "total error:  3.6035788655281067\n",
      "training error:  0.9700876474380493  and  1.4823267459869385  and  1.144352674484253\n",
      "total error:  3.5967670679092407\n",
      "training error:  1.0112650394439697  and  1.5053791999816895  and  1.198081612586975\n",
      "total error:  3.7147258520126343\n",
      "training error:  1.0025969743728638  and  1.4335253238677979  and  1.1601057052612305\n",
      "total error:  3.596228003501892\n",
      "training error:  0.984929084777832  and  1.4599132537841797  and  1.173980474472046\n",
      "total error:  3.6188228130340576\n",
      "training error:  0.9797899127006531  and  1.5011314153671265  and  1.1956534385681152\n",
      "total error:  3.6765747666358948\n",
      "training error:  0.9915598034858704  and  1.4274307489395142  and  1.1542940139770508\n",
      "total error:  3.5732845664024353\n",
      "training error:  0.9763878583908081  and  1.411271095275879  and  1.2199079990386963\n",
      "total error:  3.6075669527053833\n",
      "training error:  0.9621928334236145  and  1.3794903755187988  and  1.1206742525100708\n",
      "total error:  3.462357461452484\n",
      "training error:  0.9599812030792236  and  1.3999741077423096  and  1.151652455329895\n",
      "total error:  3.5116077661514282\n",
      "training error:  0.9720372557640076  and  1.4571294784545898  and  1.2195708751678467\n",
      "total error:  3.648737609386444\n",
      "training error:  0.9608480930328369  and  1.4617211818695068  and  1.25087571144104\n",
      "total error:  3.673444986343384\n",
      "training error:  1.011299967765808  and  1.4474843740463257  and  1.1592686176300049\n",
      "total error:  3.6180529594421387\n",
      "training error:  0.9626729488372803  and  1.6444543600082397  and  1.2013739347457886\n",
      "total error:  3.8085012435913086\n",
      "training error:  0.9590344429016113  and  1.4758247137069702  and  1.202082872390747\n",
      "total error:  3.6369420289993286\n",
      "training error:  0.9454584717750549  and  1.4411578178405762  and  1.1765081882476807\n",
      "total error:  3.5631244778633118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.9823002219200134  and  1.4484996795654297  and  1.1520602703094482\n",
      "total error:  3.5828601717948914\n",
      "training error:  0.9843050241470337  and  1.5161516666412354  and  1.1573905944824219\n",
      "total error:  3.657847285270691\n",
      "training error:  0.9649534821510315  and  1.4747616052627563  and  1.100407600402832\n",
      "total error:  3.54012268781662\n",
      "training error:  0.9935953617095947  and  1.4499573707580566  and  1.2484896183013916\n",
      "total error:  3.692042350769043\n",
      "training error:  0.978767454624176  and  1.4307787418365479  and  1.1567296981811523\n",
      "total error:  3.566275894641876\n",
      "training error:  0.959815263748169  and  1.5248788595199585  and  1.1058937311172485\n",
      "total error:  3.590587854385376\n",
      "training error:  0.9860512614250183  and  1.412726879119873  and  1.1626418828964233\n",
      "total error:  3.5614200234413147\n",
      "training error:  0.9812066555023193  and  1.5396990776062012  and  1.174234390258789\n",
      "total error:  3.6951401233673096\n",
      "training error:  0.9610597491264343  and  1.4189026355743408  and  1.1284396648406982\n",
      "total error:  3.5084020495414734\n",
      "training error:  0.9926238059997559  and  1.4766206741333008  and  1.1738721132278442\n",
      "total error:  3.643116593360901\n",
      "training error:  0.9747720956802368  and  1.5652186870574951  and  1.3422183990478516\n",
      "total error:  3.8822091817855835\n",
      "training error:  0.9680067896842957  and  1.535710096359253  and  1.1715595722198486\n",
      "total error:  3.675276458263397\n",
      "training error:  0.9521937370300293  and  1.4173511266708374  and  1.155325174331665\n",
      "total error:  3.5248700380325317\n",
      "training error:  0.9591206312179565  and  1.3822472095489502  and  1.1231293678283691\n",
      "total error:  3.464497208595276\n",
      "training error:  0.9680885076522827  and  1.454207420349121  and  1.1873235702514648\n",
      "total error:  3.6096194982528687\n",
      "training error:  0.987064003944397  and  1.3852044343948364  and  1.1273081302642822\n",
      "total error:  3.4995765686035156\n",
      "training error:  0.9623907208442688  and  1.4169631004333496  and  1.1151890754699707\n",
      "total error:  3.494542896747589\n",
      "training error:  0.9576064348220825  and  1.4522943496704102  and  1.1234345436096191\n",
      "total error:  3.533335328102112\n",
      "training error:  0.983648955821991  and  1.4424221515655518  and  1.112002968788147\n",
      "total error:  3.5380740761756897\n",
      "training error:  0.9812087416648865  and  1.4575374126434326  and  1.1568293571472168\n",
      "total error:  3.595575511455536\n",
      "training error:  0.9628626704216003  and  1.7207447290420532  and  1.2761056423187256\n",
      "total error:  3.959713041782379\n",
      "training error:  0.9480425119400024  and  1.442962646484375  and  1.2116966247558594\n",
      "total error:  3.602701783180237\n",
      "training error:  0.9290321469306946  and  1.4133121967315674  and  1.1218897104263306\n",
      "total error:  3.4642340540885925\n",
      "training error:  0.9569169878959656  and  1.4227094650268555  and  1.1908717155456543\n",
      "total error:  3.5704981684684753\n",
      "training error:  0.9577769637107849  and  1.4096391201019287  and  1.1695725917816162\n",
      "total error:  3.53698867559433\n",
      "training error:  0.9541815519332886  and  1.5140161514282227  and  1.1131706237792969\n",
      "total error:  3.581368327140808\n",
      "training error:  0.9438137412071228  and  1.4051613807678223  and  1.1124536991119385\n",
      "total error:  3.4614288210868835\n",
      "training error:  0.9361835718154907  and  1.4262542724609375  and  1.1070666313171387\n",
      "total error:  3.469504475593567\n",
      "training error:  0.9376731514930725  and  1.4584885835647583  and  1.1635751724243164\n",
      "total error:  3.559736907482147\n",
      "training error:  0.9478268623352051  and  1.6960487365722656  and  1.24858558177948\n",
      "total error:  3.8924611806869507\n",
      "training error:  0.9393983483314514  and  1.767927646636963  and  1.1917871236801147\n",
      "total error:  3.899113118648529\n",
      "training error:  0.9438288807868958  and  1.443374752998352  and  1.1535842418670654\n",
      "total error:  3.5407878756523132\n",
      "training error:  0.9545983076095581  and  1.4450397491455078  and  1.1678671836853027\n",
      "total error:  3.5675052404403687\n",
      "training error:  0.9439069628715515  and  1.4292923212051392  and  1.1993640661239624\n",
      "total error:  3.572563350200653\n",
      "training error:  0.9540488123893738  and  1.399445652961731  and  1.1396222114562988\n",
      "total error:  3.4931166768074036\n",
      "training error:  0.9474366307258606  and  1.5261911153793335  and  1.1251311302185059\n",
      "total error:  3.5987588763237\n",
      "training error:  0.956734299659729  and  1.534493088722229  and  1.197106122970581\n",
      "total error:  3.688333511352539\n",
      "training error:  0.9633456468582153  and  1.5208427906036377  and  1.1162917613983154\n",
      "total error:  3.6004801988601685\n",
      "training error:  0.9624136686325073  and  1.4093469381332397  and  1.1418436765670776\n",
      "total error:  3.5136042833328247\n",
      "training error:  0.9660017490386963  and  1.7106900215148926  and  1.1166207790374756\n",
      "total error:  3.7933125495910645\n",
      "training error:  0.9665944576263428  and  1.441579818725586  and  1.1747726202011108\n",
      "total error:  3.5829468965530396\n",
      "training error:  0.9448426365852356  and  1.383236289024353  and  1.1107115745544434\n",
      "total error:  3.438790500164032\n",
      "training error:  0.9625575542449951  and  1.5117483139038086  and  1.1678578853607178\n",
      "total error:  3.6421637535095215\n",
      "training error:  0.9325104355812073  and  1.3991782665252686  and  1.1675312519073486\n",
      "total error:  3.4992199540138245\n",
      "training error:  0.982762336730957  and  1.3993734121322632  and  1.1494903564453125\n",
      "total error:  3.5316261053085327\n",
      "training error:  0.9670400023460388  and  1.9340641498565674  and  1.219785213470459\n",
      "total error:  4.120889365673065\n",
      "training error:  0.9744076728820801  and  1.6394001245498657  and  1.3256328105926514\n",
      "total error:  3.939440608024597\n",
      "training error:  0.9575861692428589  and  1.5197908878326416  and  1.186972737312317\n",
      "total error:  3.6643497943878174\n",
      "training error:  0.9628434777259827  and  1.511286973953247  and  1.191105604171753\n",
      "total error:  3.6652360558509827\n",
      "training error:  0.9533184170722961  and  1.377333402633667  and  1.1089906692504883\n",
      "total error:  3.4396424889564514\n",
      "training error:  0.9498010277748108  and  1.4392402172088623  and  1.16688871383667\n",
      "total error:  3.555929958820343\n",
      "training error:  0.9709435105323792  and  1.4726883172988892  and  1.1749991178512573\n",
      "total error:  3.6186309456825256\n",
      "training error:  0.9563825726509094  and  1.469728708267212  and  1.1276456117630005\n",
      "total error:  3.553756892681122\n",
      "training error:  0.9492619037628174  and  1.4061131477355957  and  1.103647232055664\n",
      "total error:  3.459022283554077\n",
      "training error:  0.9574565291404724  and  1.3950330018997192  and  1.1550567150115967\n",
      "total error:  3.5075462460517883\n",
      "training error:  0.9392127394676208  and  1.384605884552002  and  1.1160671710968018\n",
      "total error:  3.4398857951164246\n",
      "training error:  0.9730920195579529  and  1.3934581279754639  and  1.1470102071762085\n",
      "total error:  3.5135603547096252\n",
      "training error:  0.940833330154419  and  1.3931708335876465  and  1.1436002254486084\n",
      "total error:  3.477604389190674\n",
      "training error:  0.9571929574012756  and  1.4387892484664917  and  1.130545973777771\n",
      "total error:  3.5265281796455383\n",
      "training error:  0.9732155799865723  and  1.394258737564087  and  1.1152558326721191\n",
      "total error:  3.4827301502227783\n",
      "training error:  0.9419622421264648  and  1.4408553838729858  and  1.1754330396652222\n",
      "total error:  3.558250665664673\n",
      "training error:  0.9292420148849487  and  1.377471923828125  and  1.0849339962005615\n",
      "total error:  3.3916479349136353\n",
      "training error:  0.9570358395576477  and  1.4448667764663696  and  1.1692774295806885\n",
      "total error:  3.571180045604706\n",
      "training error:  0.9512143135070801  and  1.4502582550048828  and  1.1151764392852783\n",
      "total error:  3.516649007797241\n",
      "training error:  0.9960744976997375  and  1.4472436904907227  and  1.1503510475158691\n",
      "total error:  3.5936692357063293\n",
      "training error:  0.9514014720916748  and  1.4724599123001099  and  1.1729906797409058\n",
      "total error:  3.5968520641326904\n",
      "training error:  0.9399523735046387  and  1.383817434310913  and  1.1465981006622314\n",
      "total error:  3.470367908477783\n",
      "training error:  0.9098625779151917  and  1.4303619861602783  and  1.1142457723617554\n",
      "total error:  3.4544703364372253\n",
      "training error:  0.9598549008369446  and  1.5451655387878418  and  1.2572731971740723\n",
      "total error:  3.7622936367988586\n",
      "training error:  0.942467212677002  and  1.4328651428222656  and  1.1536400318145752\n",
      "total error:  3.5289723873138428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.9297829866409302  and  1.5389668941497803  and  1.1560089588165283\n",
      "total error:  3.6247588396072388\n",
      "training error:  0.9142119884490967  and  1.4597601890563965  and  1.1387615203857422\n",
      "total error:  3.5127336978912354\n",
      "training error:  0.9421353340148926  and  1.4087321758270264  and  1.1902048587799072\n",
      "total error:  3.541072368621826\n",
      "training error:  0.9462657570838928  and  1.4452800750732422  and  1.111417531967163\n",
      "total error:  3.502963364124298\n",
      "training error:  0.9698188900947571  and  1.4350814819335938  and  1.168384075164795\n",
      "total error:  3.5732844471931458\n",
      "training error:  0.9796668291091919  and  1.3813393115997314  and  1.1421434879302979\n",
      "total error:  3.503149628639221\n",
      "training error:  0.936327338218689  and  1.425174355506897  and  1.1411128044128418\n",
      "total error:  3.5026144981384277\n",
      "training error:  0.9503235816955566  and  1.3688173294067383  and  1.1193532943725586\n",
      "total error:  3.4384942054748535\n",
      "training error:  0.9593188762664795  and  1.451507329940796  and  1.176248550415039\n",
      "total error:  3.5870747566223145\n",
      "training error:  0.9412019848823547  and  1.3726978302001953  and  1.2647380828857422\n",
      "total error:  3.5786378979682922\n",
      "training error:  0.9394168853759766  and  1.4232162237167358  and  1.0949511528015137\n",
      "total error:  3.457584261894226\n",
      "training error:  0.935019314289093  and  1.409927487373352  and  1.1319245100021362\n",
      "total error:  3.4768713116645813\n",
      "training error:  0.9029641151428223  and  1.3828527927398682  and  1.1161584854125977\n",
      "total error:  3.401975393295288\n",
      "training error:  0.9510409832000732  and  1.3897764682769775  and  1.1458098888397217\n",
      "total error:  3.4866273403167725\n",
      "training error:  0.938653826713562  and  1.377123475074768  and  1.1008596420288086\n",
      "total error:  3.4166369438171387\n",
      "training error:  0.9417495727539062  and  1.3949558734893799  and  1.1056276559829712\n",
      "total error:  3.4423331022262573\n",
      "training error:  0.9596129655838013  and  1.400933027267456  and  1.0986948013305664\n",
      "total error:  3.4592407941818237\n",
      "training error:  0.9383954405784607  and  1.4710195064544678  and  1.1196943521499634\n",
      "total error:  3.529109299182892\n",
      "training error:  0.9130717515945435  and  1.4499752521514893  and  1.114324927330017\n",
      "total error:  3.47737193107605\n",
      "training error:  0.9475952982902527  and  1.3930392265319824  and  1.1094437837600708\n",
      "total error:  3.450078308582306\n",
      "training error:  0.8965110182762146  and  1.3722176551818848  and  1.0903053283691406\n",
      "total error:  3.35903400182724\n",
      "training error:  0.9361357092857361  and  1.4235702753067017  and  1.11781644821167\n",
      "total error:  3.4775224328041077\n",
      "training error:  0.9558098912239075  and  1.3693681955337524  and  1.0774412155151367\n",
      "total error:  3.4026193022727966\n",
      "training error:  0.9325700998306274  and  1.4248082637786865  and  1.1947252750396729\n",
      "total error:  3.552103638648987\n",
      "training error:  0.9346606731414795  and  1.3540709018707275  and  1.0905951261520386\n",
      "total error:  3.3793267011642456\n",
      "training error:  0.9404771327972412  and  1.4271596670150757  and  1.1015233993530273\n",
      "total error:  3.4691601991653442\n",
      "training error:  0.9336124062538147  and  1.3471382856369019  and  1.1583836078643799\n",
      "total error:  3.4391342997550964\n",
      "training error:  0.9860851764678955  and  1.666641354560852  and  1.3635241985321045\n",
      "total error:  4.016250729560852\n",
      "training error:  0.9179551601409912  and  1.5038807392120361  and  1.163313388824463\n",
      "total error:  3.5851492881774902\n",
      "training error:  0.930461049079895  and  1.3901704549789429  and  1.0669770240783691\n",
      "total error:  3.387608528137207\n",
      "training error:  0.9409024715423584  and  1.579343318939209  and  1.0964373350143433\n",
      "total error:  3.6166831254959106\n",
      "training error:  0.9034160375595093  and  1.3538086414337158  and  1.1422046422958374\n",
      "total error:  3.3994293212890625\n",
      "training error:  0.9390171766281128  and  1.366834282875061  and  1.0907704830169678\n",
      "total error:  3.3966219425201416\n",
      "training error:  0.9415039420127869  and  1.5304616689682007  and  1.129184365272522\n",
      "total error:  3.6011499762535095\n",
      "training error:  0.9371825456619263  and  1.4198896884918213  and  1.0967724323272705\n",
      "total error:  3.453844666481018\n",
      "training error:  0.9346935153007507  and  1.6075270175933838  and  1.1200177669525146\n",
      "total error:  3.662238299846649\n",
      "training error:  0.9074161052703857  and  1.352068543434143  and  1.1175459623336792\n",
      "total error:  3.377030611038208\n",
      "training error:  0.91525799036026  and  1.5212804079055786  and  1.1048305034637451\n",
      "total error:  3.5413689017295837\n",
      "training error:  0.926849365234375  and  1.4999617338180542  and  1.1501085758209229\n",
      "total error:  3.576919674873352\n",
      "training error:  0.918272852897644  and  1.345159888267517  and  1.068800926208496\n",
      "total error:  3.3322336673736572\n",
      "training error:  0.8974153995513916  and  1.4819471836090088  and  1.0872461795806885\n",
      "total error:  3.466608762741089\n",
      "training error:  0.9246402978897095  and  1.361297845840454  and  1.1213719844818115\n",
      "total error:  3.407310128211975\n",
      "training error:  0.9307645559310913  and  1.4168965816497803  and  1.150146245956421\n",
      "total error:  3.4978073835372925\n",
      "training error:  0.9358421564102173  and  1.4408807754516602  and  1.1076308488845825\n",
      "total error:  3.48435378074646\n",
      "training error:  0.90257728099823  and  1.373563528060913  and  1.0895066261291504\n",
      "total error:  3.3656474351882935\n",
      "training error:  0.9036630988121033  and  1.4149012565612793  and  1.0739045143127441\n",
      "total error:  3.3924688696861267\n",
      "training error:  0.9016578197479248  and  1.3838856220245361  and  1.0875270366668701\n",
      "total error:  3.373070478439331\n",
      "training error:  0.9335670471191406  and  1.3626558780670166  and  1.1009283065795898\n",
      "total error:  3.397151231765747\n",
      "training error:  0.9269064664840698  and  1.4420433044433594  and  1.1337242126464844\n",
      "total error:  3.5026739835739136\n",
      "training error:  0.9265181422233582  and  1.5200188159942627  and  1.1203455924987793\n",
      "total error:  3.5668825507164\n",
      "training error:  0.9144728183746338  and  1.359410047531128  and  1.0702793598175049\n",
      "total error:  3.3441622257232666\n",
      "training error:  0.921267569065094  and  1.497934341430664  and  1.1662027835845947\n",
      "total error:  3.585404694080353\n",
      "training error:  0.9317972660064697  and  1.3585155010223389  and  1.1538691520690918\n",
      "total error:  3.4441819190979004\n",
      "training error:  0.9208166599273682  and  1.4539316892623901  and  1.0959240198135376\n",
      "total error:  3.470672369003296\n",
      "training error:  0.9074755907058716  and  1.460193157196045  and  1.0982754230499268\n",
      "total error:  3.4659441709518433\n",
      "training error:  0.911891758441925  and  1.3826913833618164  and  1.1074165105819702\n",
      "total error:  3.4019996523857117\n",
      "training error:  0.8979367017745972  and  1.3607873916625977  and  1.0902330875396729\n",
      "total error:  3.3489571809768677\n",
      "training error:  0.8913862109184265  and  1.3978205919265747  and  1.0949382781982422\n",
      "total error:  3.3841450810432434\n",
      "training error:  0.9021199941635132  and  1.3559672832489014  and  1.1584844589233398\n",
      "total error:  3.4165717363357544\n",
      "training error:  0.9011150002479553  and  1.3601254224777222  and  1.0762097835540771\n",
      "total error:  3.3374502062797546\n",
      "training error:  0.9440054893493652  and  1.482665777206421  and  1.1242902278900146\n",
      "total error:  3.550961494445801\n",
      "training error:  0.9073001146316528  and  1.4245017766952515  and  1.0949236154556274\n",
      "total error:  3.4267255067825317\n",
      "training error:  0.9161378741264343  and  1.4666173458099365  and  1.1189988851547241\n",
      "total error:  3.501754105091095\n",
      "training error:  0.914747416973114  and  1.3620622158050537  and  1.132720708847046\n",
      "total error:  3.4095303416252136\n",
      "training error:  0.9349371194839478  and  1.3689062595367432  and  1.0909578800201416\n",
      "total error:  3.3948012590408325\n",
      "training error:  0.8981138467788696  and  1.3608330488204956  and  1.1442816257476807\n",
      "total error:  3.403228521347046\n",
      "training error:  0.9096391201019287  and  1.5084254741668701  and  1.1340688467025757\n",
      "total error:  3.5521334409713745\n",
      "training error:  0.9110388159751892  and  1.469538688659668  and  1.116743803024292\n",
      "total error:  3.497321307659149\n",
      "training error:  0.9101141691207886  and  1.3893795013427734  and  1.1419868469238281\n",
      "total error:  3.44148051738739\n",
      "training error:  0.9337060451507568  and  1.3464913368225098  and  1.0874156951904297\n",
      "total error:  3.3676130771636963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.9306355118751526  and  1.5375537872314453  and  1.099687099456787\n",
      "total error:  3.567876398563385\n",
      "training error:  0.9145578145980835  and  1.4067373275756836  and  1.065438985824585\n",
      "total error:  3.386734127998352\n",
      "training error:  0.9039539098739624  and  1.3761367797851562  and  1.0959484577178955\n",
      "total error:  3.376039147377014\n",
      "training error:  0.9086968898773193  and  1.3790833950042725  and  1.1145988702774048\n",
      "total error:  3.4023791551589966\n",
      "training error:  0.9331225752830505  and  1.3588283061981201  and  1.1043044328689575\n",
      "total error:  3.396255314350128\n",
      "training error:  0.9008907675743103  and  1.394452452659607  and  1.126993179321289\n",
      "total error:  3.4223363995552063\n",
      "training error:  0.9196387529373169  and  1.358971118927002  and  1.1265347003936768\n",
      "total error:  3.4051445722579956\n",
      "training error:  0.911771297454834  and  1.3966856002807617  and  1.321079969406128\n",
      "total error:  3.6295368671417236\n",
      "training error:  0.9196392297744751  and  1.414931297302246  and  1.079759120941162\n",
      "total error:  3.4143296480178833\n",
      "training error:  0.8820342421531677  and  1.3587292432785034  and  1.070859670639038\n",
      "total error:  3.3116231560707092\n",
      "training error:  0.8906986713409424  and  1.34928297996521  and  1.0742838382720947\n",
      "total error:  3.314265489578247\n",
      "training error:  0.9140868186950684  and  1.5556222200393677  and  1.1209325790405273\n",
      "total error:  3.5906416177749634\n",
      "training error:  0.9277286529541016  and  1.4329948425292969  and  1.154464840888977\n",
      "total error:  3.5151883363723755\n",
      "training error:  0.9168547987937927  and  1.3561680316925049  and  1.0870778560638428\n",
      "total error:  3.3601006865501404\n",
      "training error:  0.9044629335403442  and  1.412531852722168  and  1.0746936798095703\n",
      "total error:  3.3916884660720825\n",
      "training error:  0.8736709356307983  and  1.5912550687789917  and  1.1070674657821655\n",
      "total error:  3.5719934701919556\n",
      "training error:  0.9183467626571655  and  1.4006506204605103  and  1.128420114517212\n",
      "total error:  3.4474174976348877\n",
      "training error:  0.8965853452682495  and  1.361961841583252  and  1.0705349445343018\n",
      "total error:  3.3290821313858032\n",
      "training error:  0.9308786988258362  and  1.5956941843032837  and  1.1030343770980835\n",
      "total error:  3.6296072602272034\n",
      "training error:  0.922302782535553  and  1.5321428775787354  and  1.1459460258483887\n",
      "total error:  3.600391685962677\n",
      "training error:  0.9567229747772217  and  1.6159319877624512  and  1.129006266593933\n",
      "total error:  3.701661229133606\n",
      "training error:  0.8956406712532043  and  1.374534249305725  and  1.1656978130340576\n",
      "total error:  3.435872733592987\n",
      "training error:  0.9114707112312317  and  1.3649805784225464  and  1.1829588413238525\n",
      "total error:  3.4594101309776306\n",
      "training error:  0.9341270923614502  and  1.478996992111206  and  1.1087865829467773\n",
      "total error:  3.5219106674194336\n",
      "training error:  0.924239993095398  and  1.3567986488342285  and  1.1166419982910156\n",
      "total error:  3.397680640220642\n",
      "training error:  0.9106521606445312  and  1.3981900215148926  and  1.088930606842041\n",
      "total error:  3.397772789001465\n",
      "training error:  0.9280250072479248  and  1.4345881938934326  and  1.102423906326294\n",
      "total error:  3.4650371074676514\n",
      "training error:  0.8880631327629089  and  1.3714525699615479  and  1.1182599067687988\n",
      "total error:  3.3777756094932556\n",
      "training error:  0.8736265897750854  and  1.340956687927246  and  1.0724519491195679\n",
      "total error:  3.2870352268218994\n",
      "training error:  0.9148017168045044  and  1.536691665649414  and  1.3425272703170776\n",
      "total error:  3.794020652770996\n",
      "training error:  0.9130471348762512  and  1.3321518898010254  and  1.0780316591262817\n",
      "total error:  3.3232306838035583\n",
      "training error:  0.902122974395752  and  1.433973789215088  and  1.1145215034484863\n",
      "total error:  3.450618267059326\n",
      "training error:  0.9012328386306763  and  1.4230802059173584  and  1.0903713703155518\n",
      "total error:  3.4146844148635864\n",
      "training error:  0.8950604796409607  and  1.3679544925689697  and  1.093231201171875\n",
      "total error:  3.3562461733818054\n",
      "training error:  0.9151358604431152  and  1.362895131111145  and  1.1083364486694336\n",
      "total error:  3.386367440223694\n",
      "training error:  0.9242942333221436  and  1.3835583925247192  and  1.1492230892181396\n",
      "total error:  3.4570757150650024\n",
      "training error:  0.9118709564208984  and  1.6108243465423584  and  1.1247427463531494\n",
      "total error:  3.6474380493164062\n",
      "training error:  0.8957849740982056  and  1.4334189891815186  and  1.0856029987335205\n",
      "total error:  3.4148069620132446\n",
      "training error:  0.8717411756515503  and  1.3795371055603027  and  1.078004240989685\n",
      "total error:  3.329282522201538\n",
      "training error:  0.9125824570655823  and  1.3598004579544067  and  1.0951436758041382\n",
      "total error:  3.367526590824127\n",
      "training error:  0.8779124021530151  and  1.3242170810699463  and  1.0871549844741821\n",
      "total error:  3.2892844676971436\n",
      "training error:  0.8951672315597534  and  1.3293797969818115  and  1.092850923538208\n",
      "total error:  3.317397952079773\n",
      "training error:  0.8716951012611389  and  1.3643569946289062  and  1.1231579780578613\n",
      "total error:  3.3592100739479065\n",
      "training error:  0.8881075978279114  and  1.3434560298919678  and  1.0874245166778564\n",
      "total error:  3.3189881443977356\n",
      "training error:  0.8957309126853943  and  1.4147850275039673  and  1.1026636362075806\n",
      "total error:  3.413179576396942\n",
      "training error:  0.8833432793617249  and  1.4222652912139893  and  1.0817344188690186\n",
      "total error:  3.3873429894447327\n",
      "training error:  0.8876503705978394  and  1.3281419277191162  and  1.0938223600387573\n",
      "total error:  3.309614658355713\n",
      "training error:  0.8981432318687439  and  1.4166440963745117  and  1.2084137201309204\n",
      "total error:  3.523201048374176\n",
      "training error:  0.8986772298812866  and  1.5437957048416138  and  1.1068751811981201\n",
      "total error:  3.5493481159210205\n",
      "training error:  0.8947787284851074  and  1.3732688426971436  and  1.119418978691101\n",
      "total error:  3.387466549873352\n",
      "training error:  0.8938583135604858  and  1.4221348762512207  and  1.0910755395889282\n",
      "total error:  3.4070687294006348\n",
      "training error:  0.8855558633804321  and  1.3343167304992676  and  1.0796098709106445\n",
      "total error:  3.2994824647903442\n",
      "training error:  0.8881404995918274  and  1.512860894203186  and  1.0934200286865234\n",
      "total error:  3.494421422481537\n",
      "training error:  0.8922426700592041  and  1.3273911476135254  and  1.096092700958252\n",
      "total error:  3.3157265186309814\n",
      "training error:  0.8989648222923279  and  1.3829998970031738  and  1.0965518951416016\n",
      "total error:  3.3785166144371033\n",
      "training error:  0.9184688329696655  and  1.6050546169281006  and  1.1306607723236084\n",
      "total error:  3.6541842222213745\n",
      "training error:  0.9073368310928345  and  1.3763295412063599  and  1.1184382438659668\n",
      "total error:  3.402104616165161\n",
      "training error:  0.8610185980796814  and  1.3134815692901611  and  1.0592303276062012\n",
      "total error:  3.2337304949760437\n",
      "training error:  0.8620380163192749  and  1.3418827056884766  and  1.1160966157913208\n",
      "total error:  3.3200173377990723\n",
      "training error:  0.8833305835723877  and  1.336350440979004  and  1.1268620491027832\n",
      "total error:  3.346543073654175\n",
      "training error:  0.8591299653053284  and  1.3560659885406494  and  1.0670268535614014\n",
      "total error:  3.282222807407379\n",
      "training error:  0.8934534788131714  and  1.5484752655029297  and  1.0624516010284424\n",
      "total error:  3.5043803453445435\n",
      "training error:  0.8748028874397278  and  1.3120641708374023  and  1.1307461261749268\n",
      "total error:  3.317613184452057\n",
      "training error:  0.872744083404541  and  1.5449621677398682  and  1.1047379970550537\n",
      "total error:  3.522444248199463\n",
      "training error:  0.8931492567062378  and  1.6401407718658447  and  1.1534265279769897\n",
      "total error:  3.6867165565490723\n",
      "training error:  0.8840656876564026  and  1.3499315977096558  and  1.1206663846969604\n",
      "total error:  3.354663670063019\n",
      "training error:  0.8812927007675171  and  1.3578740358352661  and  1.07534921169281\n",
      "total error:  3.3145159482955933\n",
      "training error:  0.9011662006378174  and  1.3293147087097168  and  1.0387189388275146\n",
      "total error:  3.269199848175049\n",
      "training error:  0.9161216020584106  and  1.5168712139129639  and  1.1775054931640625\n",
      "total error:  3.610498309135437\n",
      "training error:  0.8617317080497742  and  1.4900842905044556  and  1.0961196422576904\n",
      "total error:  3.44793564081192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.8967896103858948  and  1.4265923500061035  and  1.1170666217803955\n",
      "total error:  3.440448582172394\n",
      "training error:  0.9036704301834106  and  1.3771719932556152  and  1.2388168573379517\n",
      "total error:  3.5196592807769775\n",
      "training error:  0.8816125988960266  and  1.452840805053711  and  1.0936747789382935\n",
      "total error:  3.428128182888031\n",
      "training error:  0.886893630027771  and  1.4672276973724365  and  1.1117265224456787\n",
      "total error:  3.4658478498458862\n",
      "training error:  0.8762737512588501  and  1.3164844512939453  and  1.0498331785202026\n",
      "total error:  3.242591381072998\n",
      "training error:  0.8764644861221313  and  1.3305926322937012  and  1.1010825634002686\n",
      "total error:  3.308139681816101\n",
      "training error:  0.8605129718780518  and  1.3441441059112549  and  1.112669825553894\n",
      "total error:  3.3173269033432007\n",
      "training error:  0.8593778610229492  and  1.3319101333618164  and  1.0618994235992432\n",
      "total error:  3.253187417984009\n",
      "training error:  0.8773601055145264  and  1.3566172122955322  and  1.0665338039398193\n",
      "total error:  3.300511121749878\n",
      "training error:  0.8972914218902588  and  1.3433645963668823  and  1.0388270616531372\n",
      "total error:  3.2794830799102783\n",
      "training error:  0.8719912767410278  and  1.3602529764175415  and  1.0806763172149658\n",
      "total error:  3.312920570373535\n",
      "training error:  0.8827279806137085  and  1.3256900310516357  and  1.066344141960144\n",
      "total error:  3.2747621536254883\n",
      "training error:  0.8758735656738281  and  1.3337382078170776  and  1.0798561573028564\n",
      "total error:  3.289467930793762\n",
      "training error:  0.8603565692901611  and  1.3245949745178223  and  1.0486712455749512\n",
      "total error:  3.2336227893829346\n",
      "training error:  0.8845309019088745  and  1.3441665172576904  and  1.1738169193267822\n",
      "total error:  3.402514338493347\n",
      "training error:  0.873475193977356  and  1.3161664009094238  and  1.0659593343734741\n",
      "total error:  3.255600929260254\n",
      "training error:  0.8794134855270386  and  1.3770325183868408  and  1.0697532892227173\n",
      "total error:  3.3261992931365967\n",
      "training error:  0.895013153553009  and  1.3805127143859863  and  1.1177423000335693\n",
      "total error:  3.3932681679725647\n",
      "training error:  0.8812314867973328  and  1.3650720119476318  and  1.0957486629486084\n",
      "total error:  3.342052161693573\n",
      "training error:  0.869312047958374  and  1.4594228267669678  and  1.067976474761963\n",
      "total error:  3.3967113494873047\n",
      "training error:  0.8578742742538452  and  1.3174713850021362  and  1.0568028688430786\n",
      "total error:  3.23214852809906\n",
      "training error:  0.8788098096847534  and  1.326146125793457  and  1.0968356132507324\n",
      "total error:  3.301791548728943\n",
      "training error:  0.8820869326591492  and  1.356308937072754  and  1.1257083415985107\n",
      "total error:  3.364104211330414\n",
      "training error:  0.8579200506210327  and  1.3746337890625  and  1.1095958948135376\n",
      "total error:  3.3421497344970703\n",
      "training error:  0.8430306315422058  and  1.371851921081543  and  1.0824809074401855\n",
      "total error:  3.2973634600639343\n",
      "training error:  0.872191309928894  and  1.509064793586731  and  1.1077666282653809\n",
      "total error:  3.489022731781006\n",
      "training error:  0.8567668199539185  and  1.29127836227417  and  1.0463894605636597\n",
      "total error:  3.194434642791748\n",
      "training error:  0.8948233127593994  and  1.702181100845337  and  1.1583895683288574\n",
      "total error:  3.7553939819335938\n",
      "training error:  0.9072027802467346  and  1.3566102981567383  and  1.1280546188354492\n",
      "total error:  3.391867697238922\n",
      "training error:  0.8653125762939453  and  1.3908706903457642  and  1.1314270496368408\n",
      "total error:  3.3876103162765503\n",
      "training error:  0.8708420991897583  and  1.3272151947021484  and  1.0906777381896973\n",
      "total error:  3.288735032081604\n",
      "training error:  0.8440195918083191  and  1.3634943962097168  and  1.0690550804138184\n",
      "total error:  3.2765690684318542\n",
      "training error:  0.8885157108306885  and  1.343397617340088  and  1.028327465057373\n",
      "total error:  3.2602407932281494\n",
      "training error:  0.8797957897186279  and  1.4262628555297852  and  1.0550645589828491\n",
      "total error:  3.361123204231262\n",
      "training error:  0.8310588598251343  and  1.3019561767578125  and  1.0733128786087036\n",
      "total error:  3.2063279151916504\n",
      "training error:  0.8550552129745483  and  1.3209030628204346  and  1.0747004747390747\n",
      "total error:  3.2506587505340576\n",
      "training error:  0.8818078637123108  and  1.3256689310073853  and  1.0529825687408447\n",
      "total error:  3.2604593634605408\n",
      "training error:  0.841960072517395  and  1.34549081325531  and  1.045615315437317\n",
      "total error:  3.233066201210022\n",
      "training error:  0.8575246334075928  and  1.3383891582489014  and  1.059119462966919\n",
      "total error:  3.255033254623413\n",
      "training error:  0.8595921993255615  and  1.3715696334838867  and  1.1021170616149902\n",
      "total error:  3.3332788944244385\n",
      "training error:  0.8772743940353394  and  1.3420295715332031  and  1.0627397298812866\n",
      "total error:  3.282043695449829\n",
      "training error:  0.864996075630188  and  1.4155004024505615  and  1.0966432094573975\n",
      "total error:  3.377139687538147\n",
      "training error:  0.8537408113479614  and  1.3587346076965332  and  1.096303105354309\n",
      "total error:  3.3087785243988037\n",
      "training error:  0.8633900284767151  and  1.3048255443572998  and  1.041015863418579\n",
      "total error:  3.209231436252594\n",
      "training error:  0.8796651363372803  and  1.3171088695526123  and  1.0677196979522705\n",
      "total error:  3.264493703842163\n",
      "training error:  0.8804242014884949  and  1.4404677152633667  and  1.1441715955734253\n",
      "total error:  3.465063512325287\n",
      "training error:  0.8663202524185181  and  1.4217287302017212  and  1.0764756202697754\n",
      "total error:  3.3645246028900146\n",
      "training error:  0.8570666909217834  and  1.4322679042816162  and  1.0682508945465088\n",
      "total error:  3.3575854897499084\n",
      "training error:  0.8514846563339233  and  1.359864354133606  and  1.1207027435302734\n",
      "total error:  3.3320517539978027\n",
      "training error:  0.8714281320571899  and  1.3521710634231567  and  1.1075706481933594\n",
      "total error:  3.331169843673706\n",
      "training error:  0.8475351929664612  and  1.296069860458374  and  1.0410709381103516\n",
      "total error:  3.1846759915351868\n",
      "training error:  0.8371449708938599  and  1.3401864767074585  and  1.0893487930297852\n",
      "total error:  3.2666802406311035\n",
      "training error:  0.8593567609786987  and  1.6240463256835938  and  1.178786039352417\n",
      "total error:  3.6621891260147095\n",
      "training error:  0.8620949387550354  and  1.3290133476257324  and  1.0372477769851685\n",
      "total error:  3.2283560633659363\n",
      "training error:  0.8425540924072266  and  1.3522183895111084  and  1.0527877807617188\n",
      "total error:  3.2475602626800537\n",
      "training error:  0.8424004316329956  and  1.3165955543518066  and  1.1352577209472656\n",
      "total error:  3.294253706932068\n",
      "training error:  0.8576779365539551  and  1.3648244142532349  and  1.1413230895996094\n",
      "total error:  3.3638254404067993\n",
      "training error:  0.8750574588775635  and  1.4200040102005005  and  1.234835147857666\n",
      "total error:  3.52989661693573\n",
      "training error:  0.8667665719985962  and  1.3108608722686768  and  1.056370735168457\n",
      "total error:  3.23399817943573\n",
      "training error:  0.8502341508865356  and  1.3545336723327637  and  1.0471869707107544\n",
      "total error:  3.2519547939300537\n",
      "training error:  0.8747670650482178  and  1.3719134330749512  and  1.051847219467163\n",
      "total error:  3.298527717590332\n",
      "training error:  0.8329744935035706  and  1.3648278713226318  and  1.0580365657806396\n",
      "total error:  3.255838930606842\n",
      "training error:  0.861876368522644  and  1.4710185527801514  and  1.1073493957519531\n",
      "total error:  3.4402443170547485\n",
      "training error:  0.8891753554344177  and  1.3170192241668701  and  1.0815250873565674\n",
      "total error:  3.2877196669578552\n",
      "training error:  0.8153934478759766  and  1.2925673723220825  and  1.0210552215576172\n",
      "total error:  3.1290160417556763\n",
      "training error:  0.8378893733024597  and  1.2955856323242188  and  1.0483598709106445\n",
      "total error:  3.181834876537323\n",
      "training error:  0.8461523652076721  and  1.3113203048706055  and  1.0515260696411133\n",
      "total error:  3.208998739719391\n",
      "training error:  0.8364561200141907  and  1.3894273042678833  and  1.0673232078552246\n",
      "total error:  3.2932066321372986\n",
      "training error:  0.8457590341567993  and  1.3875129222869873  and  1.0639820098876953\n",
      "total error:  3.297253966331482\n",
      "training error:  0.8784195780754089  and  1.3301103115081787  and  1.0613592863082886\n",
      "total error:  3.269889175891876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.8499705791473389  and  1.3869389295578003  and  1.0714194774627686\n",
      "total error:  3.3083289861679077\n",
      "training error:  0.862732470035553  and  1.3158268928527832  and  1.1245216131210327\n",
      "total error:  3.303080976009369\n",
      "training error:  0.8458577394485474  and  1.3548166751861572  and  1.0766499042510986\n",
      "total error:  3.2773243188858032\n",
      "training error:  0.8642997741699219  and  1.3251093626022339  and  1.0586634874343872\n",
      "total error:  3.248072624206543\n",
      "training error:  0.8629542589187622  and  1.3148136138916016  and  1.0730834007263184\n",
      "total error:  3.250851273536682\n",
      "training error:  0.8557963967323303  and  1.2727001905441284  and  1.0567522048950195\n",
      "total error:  3.1852487921714783\n",
      "training error:  0.8375076055526733  and  1.3437548875808716  and  1.094325065612793\n",
      "total error:  3.275587558746338\n",
      "training error:  0.8670086860656738  and  1.3068227767944336  and  1.0481183528900146\n",
      "total error:  3.221949815750122\n",
      "training error:  0.8544324636459351  and  1.2960569858551025  and  1.0393991470336914\n",
      "total error:  3.189888596534729\n",
      "training error:  0.8506791591644287  and  1.3164546489715576  and  1.1397855281829834\n",
      "total error:  3.3069193363189697\n",
      "training error:  0.8295130729675293  and  1.3587708473205566  and  1.0962319374084473\n",
      "total error:  3.284515857696533\n",
      "training error:  0.8678977489471436  and  1.316136360168457  and  1.0227917432785034\n",
      "total error:  3.206825852394104\n",
      "training error:  0.8577302694320679  and  1.3082804679870605  and  1.098235845565796\n",
      "total error:  3.2642465829849243\n",
      "training error:  0.8534924387931824  and  1.4193198680877686  and  1.0650522708892822\n",
      "total error:  3.337864577770233\n",
      "training error:  0.892830491065979  and  1.539363145828247  and  1.1468617916107178\n",
      "total error:  3.579055428504944\n",
      "training error:  0.8394018411636353  and  1.4503378868103027  and  1.0847772359848022\n",
      "total error:  3.3745169639587402\n",
      "training error:  0.8404272198677063  and  1.299054741859436  and  1.0081055164337158\n",
      "total error:  3.147587478160858\n",
      "training error:  0.8777320384979248  and  1.3057730197906494  and  1.0772782564163208\n",
      "total error:  3.260783314704895\n",
      "training error:  0.8398922681808472  and  1.3355634212493896  and  1.017143964767456\n",
      "total error:  3.192599654197693\n",
      "training error:  0.8208223581314087  and  1.553943157196045  and  1.0915460586547852\n",
      "total error:  3.4663115739822388\n",
      "training error:  0.8426337242126465  and  1.385854959487915  and  1.1128898859024048\n",
      "total error:  3.3413785696029663\n",
      "training error:  0.8507117629051208  and  1.2929019927978516  and  1.0578458309173584\n",
      "total error:  3.201459586620331\n",
      "training error:  0.8513311743736267  and  1.319860816001892  and  0.9970413446426392\n",
      "total error:  3.168233335018158\n",
      "training error:  0.8489819765090942  and  1.3086484670639038  and  1.0722086429595947\n",
      "total error:  3.2298390865325928\n",
      "training error:  0.8609411120414734  and  1.433088779449463  and  1.046156644821167\n",
      "total error:  3.3401865363121033\n",
      "training error:  0.8053793907165527  and  1.3379713296890259  and  1.0634407997131348\n",
      "total error:  3.2067915201187134\n",
      "training error:  0.8528633713722229  and  1.3007850646972656  and  1.0815165042877197\n",
      "total error:  3.2351649403572083\n",
      "training error:  0.8423948884010315  and  1.3129459619522095  and  1.0823763608932495\n",
      "total error:  3.2377172112464905\n",
      "training error:  0.8553522825241089  and  1.306113839149475  and  1.0303208827972412\n",
      "total error:  3.191787004470825\n",
      "training error:  0.8263919353485107  and  1.2614805698394775  and  1.0633952617645264\n",
      "total error:  3.1512677669525146\n",
      "training error:  0.8573330640792847  and  1.3062965869903564  and  1.1182372570037842\n",
      "total error:  3.2818669080734253\n",
      "training error:  0.8332818746566772  and  1.2871803045272827  and  1.1227056980133057\n",
      "total error:  3.2431678771972656\n",
      "training error:  0.8719503283500671  and  1.417182445526123  and  1.1473476886749268\n",
      "total error:  3.436480462551117\n",
      "training error:  0.8425838351249695  and  1.330251932144165  and  1.0749258995056152\n",
      "total error:  3.2477616667747498\n",
      "training error:  0.8240786790847778  and  1.2766907215118408  and  1.1011345386505127\n",
      "total error:  3.2019039392471313\n",
      "training error:  0.8602941632270813  and  1.3053961992263794  and  1.0597572326660156\n",
      "total error:  3.2254475951194763\n",
      "training error:  0.8427007794380188  and  1.417720079421997  and  0.9952608942985535\n",
      "total error:  3.2556817531585693\n",
      "training error:  0.8206524848937988  and  1.378299593925476  and  1.0649409294128418\n",
      "total error:  3.2638930082321167\n",
      "training error:  0.8304253220558167  and  1.8008030652999878  and  1.0333960056304932\n",
      "total error:  3.6646243929862976\n",
      "training error:  0.8727911710739136  and  1.3852981328964233  and  1.0920288562774658\n",
      "total error:  3.3501181602478027\n",
      "training error:  0.8502177000045776  and  1.4208824634552002  and  1.0613576173782349\n",
      "total error:  3.3324577808380127\n",
      "training error:  0.8312731981277466  and  1.3047528266906738  and  1.0777685642242432\n",
      "total error:  3.2137945890426636\n",
      "training error:  0.8639531135559082  and  1.3346043825149536  and  1.1965560913085938\n",
      "total error:  3.3951135873794556\n",
      "training error:  0.8505717515945435  and  1.3829047679901123  and  1.0592820644378662\n",
      "total error:  3.292758584022522\n",
      "training error:  0.8379278182983398  and  1.339774489402771  and  1.0394446849822998\n",
      "total error:  3.2171469926834106\n",
      "training error:  0.8153208494186401  and  1.3891806602478027  and  1.0292913913726807\n",
      "total error:  3.2337929010391235\n",
      "training error:  0.815604567527771  and  1.3836023807525635  and  1.1631247997283936\n",
      "total error:  3.362331748008728\n",
      "training error:  0.8409295678138733  and  1.3890020847320557  and  1.0663530826568604\n",
      "total error:  3.2962847352027893\n",
      "training error:  0.8458033204078674  and  1.4194891452789307  and  1.0451111793518066\n",
      "total error:  3.3104036450386047\n",
      "training error:  0.8452088832855225  and  1.275342345237732  and  1.0624297857284546\n",
      "total error:  3.182981014251709\n",
      "training error:  0.8200359344482422  and  1.2928380966186523  and  1.0414555072784424\n",
      "total error:  3.154329538345337\n",
      "training error:  0.8231101036071777  and  1.31595778465271  and  1.0421977043151855\n",
      "total error:  3.1812655925750732\n",
      "training error:  0.8286042213439941  and  1.3743740320205688  and  1.0488450527191162\n",
      "total error:  3.251823306083679\n",
      "training error:  0.8140258193016052  and  1.2887216806411743  and  1.0197224617004395\n",
      "total error:  3.122469961643219\n",
      "training error:  0.8420866131782532  and  1.315788745880127  and  1.0617659091949463\n",
      "total error:  3.2196412682533264\n",
      "training error:  0.8298699855804443  and  1.4433730840682983  and  1.1219818592071533\n",
      "total error:  3.395224928855896\n",
      "training error:  0.8483077883720398  and  1.3334965705871582  and  1.0323657989501953\n",
      "total error:  3.2141701579093933\n",
      "training error:  0.8215664625167847  and  1.2881810665130615  and  1.0275565385818481\n",
      "total error:  3.1373040676116943\n",
      "training error:  0.8245142698287964  and  1.2926125526428223  and  1.0379118919372559\n",
      "total error:  3.1550387144088745\n",
      "training error:  0.8088910579681396  and  1.279308557510376  and  1.0284167528152466\n",
      "total error:  3.116616368293762\n",
      "training error:  0.8468352556228638  and  1.2930479049682617  and  1.039206862449646\n",
      "total error:  3.1790900230407715\n",
      "training error:  0.8162586688995361  and  1.2815841436386108  and  1.039696455001831\n",
      "total error:  3.137539267539978\n",
      "training error:  0.813226580619812  and  1.322812557220459  and  1.0211784839630127\n",
      "total error:  3.1572176218032837\n",
      "training error:  0.8107395172119141  and  1.2708112001419067  and  0.9970601797103882\n",
      "total error:  3.078610897064209\n",
      "training error:  0.8577114939689636  and  1.3552329540252686  and  1.0658514499664307\n",
      "total error:  3.278795897960663\n",
      "training error:  0.841056227684021  and  1.4945811033248901  and  1.1130170822143555\n",
      "total error:  3.4486544132232666\n",
      "training error:  0.8242459297180176  and  1.2945566177368164  and  1.052044153213501\n",
      "total error:  3.170846700668335\n",
      "training error:  0.8256323337554932  and  1.3071023225784302  and  1.0469450950622559\n",
      "total error:  3.179679751396179\n",
      "training error:  0.825383186340332  and  1.2834954261779785  and  1.0449085235595703\n",
      "total error:  3.153787136077881\n",
      "training error:  0.8130149841308594  and  1.2663078308105469  and  1.0697941780090332\n",
      "total error:  3.1491169929504395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.8271832466125488  and  1.258244276046753  and  1.0351170301437378\n",
      "total error:  3.1205445528030396\n",
      "training error:  0.8408120274543762  and  1.317200779914856  and  1.0814545154571533\n",
      "total error:  3.2394673228263855\n",
      "training error:  0.8096740245819092  and  1.246523380279541  and  1.047299861907959\n",
      "total error:  3.103497266769409\n",
      "training error:  0.8160289525985718  and  1.337223768234253  and  1.0319616794586182\n",
      "total error:  3.185214400291443\n",
      "training error:  0.8170487284660339  and  1.2751742601394653  and  1.0018768310546875\n",
      "total error:  3.0940998196601868\n",
      "training error:  0.8392819166183472  and  1.3202449083328247  and  1.0650169849395752\n",
      "total error:  3.224543809890747\n",
      "training error:  0.8224081993103027  and  1.2995898723602295  and  1.0759243965148926\n",
      "total error:  3.197922468185425\n",
      "training error:  0.8261072039604187  and  1.3642514944076538  and  1.1204538345336914\n",
      "total error:  3.310812532901764\n",
      "training error:  0.8351098299026489  and  1.295542597770691  and  1.024043083190918\n",
      "total error:  3.154695510864258\n",
      "training error:  0.8433934450149536  and  1.3091446161270142  and  1.0981581211090088\n",
      "total error:  3.2506961822509766\n",
      "training error:  0.8190663456916809  and  1.3253614902496338  and  1.0762971639633179\n",
      "total error:  3.2207249999046326\n",
      "training error:  0.8040057420730591  and  1.324708104133606  and  1.1129608154296875\n",
      "total error:  3.2416746616363525\n",
      "training error:  0.8051917552947998  and  1.2973450422286987  and  1.061586856842041\n",
      "total error:  3.1641236543655396\n",
      "training error:  0.80256187915802  and  1.283595323562622  and  1.0185703039169312\n",
      "total error:  3.1047275066375732\n",
      "training error:  0.8217850923538208  and  1.2830681800842285  and  1.0459282398223877\n",
      "total error:  3.150781512260437\n",
      "training error:  0.8106352090835571  and  1.317912220954895  and  1.0257487297058105\n",
      "total error:  3.1542961597442627\n",
      "training error:  0.8111525774002075  and  1.2622959613800049  and  0.9942135214805603\n",
      "total error:  3.0676620602607727\n",
      "training error:  0.8543738126754761  and  1.353379726409912  and  1.0255017280578613\n",
      "total error:  3.2332552671432495\n",
      "training error:  0.8336832523345947  and  1.2750258445739746  and  0.9865920543670654\n",
      "total error:  3.0953011512756348\n",
      "training error:  0.8036974668502808  and  1.3123316764831543  and  1.1826841831207275\n",
      "total error:  3.2987133264541626\n",
      "training error:  0.8265952467918396  and  1.317770004272461  and  1.0728973150253296\n",
      "total error:  3.21726256608963\n",
      "training error:  0.8167870044708252  and  1.2514911890029907  and  1.0402045249938965\n",
      "total error:  3.1084827184677124\n",
      "training error:  0.819495677947998  and  1.3494784832000732  and  1.0130400657653809\n",
      "total error:  3.182014226913452\n",
      "training error:  0.8101240396499634  and  1.31148362159729  and  1.0350604057312012\n",
      "total error:  3.1566680669784546\n",
      "training error:  0.8036960959434509  and  1.333078384399414  and  1.0735400915145874\n",
      "total error:  3.2103145718574524\n",
      "training error:  0.8178977966308594  and  1.2858757972717285  and  1.0468230247497559\n",
      "total error:  3.1505966186523438\n",
      "training error:  0.8184370398521423  and  1.312032699584961  and  1.01714289188385\n",
      "total error:  3.1476126313209534\n",
      "training error:  0.8159886598587036  and  1.2442095279693604  and  1.0226848125457764\n",
      "total error:  3.0828830003738403\n",
      "training error:  0.7846069931983948  and  1.3163542747497559  and  1.0736699104309082\n",
      "total error:  3.174631178379059\n",
      "training error:  0.812235414981842  and  1.2511463165283203  and  0.9813569784164429\n",
      "total error:  3.0447387099266052\n",
      "training error:  0.801964282989502  and  1.3974981307983398  and  1.0454374551773071\n",
      "total error:  3.244899868965149\n",
      "training error:  0.8201411366462708  and  1.2699984312057495  and  1.060341238975525\n",
      "total error:  3.150480806827545\n",
      "training error:  0.8340074419975281  and  1.2902717590332031  and  1.0403493642807007\n",
      "total error:  3.164628565311432\n",
      "training error:  0.8182647228240967  and  1.2680411338806152  and  1.1117198467254639\n",
      "total error:  3.198025703430176\n",
      "training error:  0.8114301562309265  and  1.2898082733154297  and  1.0925432443618774\n",
      "total error:  3.1937816739082336\n",
      "training error:  0.791527509689331  and  1.3000792264938354  and  1.017836570739746\n",
      "total error:  3.1094433069229126\n",
      "training error:  0.8047497272491455  and  1.3964550495147705  and  1.1086755990982056\n",
      "total error:  3.3098803758621216\n",
      "training error:  0.8137896060943604  and  1.3791314363479614  and  1.0325584411621094\n",
      "total error:  3.225479483604431\n",
      "training error:  0.8179399371147156  and  1.2919028997421265  and  1.0879089832305908\n",
      "total error:  3.197751820087433\n",
      "training error:  0.7873866558074951  and  1.236086130142212  and  1.0262970924377441\n",
      "total error:  3.049769878387451\n",
      "training error:  0.8170133233070374  and  1.2680858373641968  and  1.0074981451034546\n",
      "total error:  3.0925973057746887\n",
      "training error:  0.7767737507820129  and  1.2810518741607666  and  1.013038992881775\n",
      "total error:  3.0708646178245544\n",
      "training error:  0.8390370607376099  and  1.3097069263458252  and  1.022371530532837\n",
      "total error:  3.171115517616272\n",
      "training error:  0.8068642020225525  and  1.3061847686767578  and  1.0545179843902588\n",
      "total error:  3.167566955089569\n",
      "training error:  0.8247042298316956  and  1.3039331436157227  and  1.041278600692749\n",
      "total error:  3.1699159741401672\n",
      "training error:  0.8296871781349182  and  1.2706758975982666  and  1.0287870168685913\n",
      "total error:  3.129150092601776\n",
      "training error:  0.7749952077865601  and  1.2465205192565918  and  1.0336198806762695\n",
      "total error:  3.0551356077194214\n",
      "training error:  0.7864739894866943  and  1.2863476276397705  and  1.0116946697235107\n",
      "total error:  3.0845162868499756\n",
      "training error:  0.8280534148216248  and  1.2943817377090454  and  1.0167667865753174\n",
      "total error:  3.1392019391059875\n",
      "training error:  0.7923420667648315  and  1.406507968902588  and  1.083026647567749\n",
      "total error:  3.2818766832351685\n",
      "training error:  0.8506079912185669  and  1.3639682531356812  and  1.2115912437438965\n",
      "total error:  3.4261674880981445\n",
      "training error:  0.8246375918388367  and  1.330504298210144  and  1.0728262662887573\n",
      "total error:  3.227968156337738\n",
      "training error:  0.8285382986068726  and  1.3019864559173584  and  1.0036596059799194\n",
      "total error:  3.1341843605041504\n",
      "training error:  0.7962686419487  and  1.3078018426895142  and  1.0093388557434082\n",
      "total error:  3.1134093403816223\n",
      "training error:  0.7906103134155273  and  1.267552137374878  and  1.034420371055603\n",
      "total error:  3.0925828218460083\n",
      "training error:  0.7756655812263489  and  1.2538117170333862  and  1.007965087890625\n",
      "total error:  3.03744238615036\n",
      "training error:  0.8195429444313049  and  1.254767656326294  and  1.0923632383346558\n",
      "total error:  3.1666738390922546\n",
      "training error:  0.8008860349655151  and  1.432692050933838  and  1.0354864597320557\n",
      "total error:  3.2690645456314087\n",
      "training error:  0.8108314871788025  and  1.258497714996338  and  1.0152368545532227\n",
      "total error:  3.084566056728363\n",
      "training error:  0.8203870058059692  and  1.3211102485656738  and  1.0597072839736938\n",
      "total error:  3.201204538345337\n",
      "training error:  0.7806973457336426  and  1.303633689880371  and  0.9908424615859985\n",
      "total error:  3.075173497200012\n",
      "training error:  0.789270281791687  and  1.2412878274917603  and  1.0147417783737183\n",
      "total error:  3.0452998876571655\n",
      "training error:  0.7874288558959961  and  1.308002233505249  and  1.1450343132019043\n",
      "total error:  3.2404654026031494\n",
      "training error:  0.7849208116531372  and  1.3023455142974854  and  1.0369279384613037\n",
      "total error:  3.1241942644119263\n",
      "training error:  0.7815721035003662  and  1.2766947746276855  and  1.0106785297393799\n",
      "total error:  3.0689454078674316\n",
      "training error:  0.785233199596405  and  1.296897292137146  and  1.0114763975143433\n",
      "total error:  3.0936068892478943\n",
      "training error:  0.786835789680481  and  1.2354260683059692  and  1.0519111156463623\n",
      "total error:  3.0741729736328125\n",
      "training error:  0.8065671920776367  and  1.2600274085998535  and  1.0688073635101318\n",
      "total error:  3.135401964187622\n",
      "training error:  0.8388403654098511  and  1.6536483764648438  and  1.0829118490219116\n",
      "total error:  3.5754005908966064\n",
      "training error:  0.8055309057235718  and  1.2455971240997314  and  0.9908567667007446\n",
      "total error:  3.041984796524048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.8311899900436401  and  1.311295509338379  and  1.0885093212127686\n",
      "total error:  3.2309948205947876\n",
      "training error:  0.847199559211731  and  1.2836873531341553  and  1.0551663637161255\n",
      "total error:  3.1860532760620117\n",
      "training error:  0.8004895448684692  and  1.3188987970352173  and  1.1073946952819824\n",
      "total error:  3.226783037185669\n",
      "training error:  0.794191837310791  and  1.2966194152832031  and  1.0332164764404297\n",
      "total error:  3.124027729034424\n",
      "training error:  0.7830031514167786  and  1.2648364305496216  and  1.035666584968567\n",
      "total error:  3.083506166934967\n",
      "training error:  0.7714688777923584  and  1.263152837753296  and  1.1245713233947754\n",
      "total error:  3.1591930389404297\n",
      "training error:  0.7987768054008484  and  1.2806310653686523  and  1.0668412446975708\n",
      "total error:  3.1462491154670715\n",
      "training error:  0.790244460105896  and  1.406087875366211  and  1.0677905082702637\n",
      "total error:  3.2641228437423706\n",
      "training error:  0.7742235660552979  and  1.2101554870605469  and  1.0178195238113403\n",
      "total error:  3.002198576927185\n",
      "training error:  0.7733825445175171  and  1.3583266735076904  and  1.0170202255249023\n",
      "total error:  3.14872944355011\n",
      "training error:  0.8206019401550293  and  1.3889007568359375  and  1.1493878364562988\n",
      "total error:  3.3588905334472656\n",
      "training error:  0.8080452680587769  and  1.258211374282837  and  1.0514570474624634\n",
      "total error:  3.117713689804077\n",
      "training error:  0.7826162576675415  and  1.2856556177139282  and  0.9782871007919312\n",
      "total error:  3.046558976173401\n",
      "training error:  0.7789632081985474  and  1.2881224155426025  and  1.032773494720459\n",
      "total error:  3.099859118461609\n",
      "training error:  0.7933609485626221  and  1.2357308864593506  and  1.044105052947998\n",
      "total error:  3.0731968879699707\n",
      "training error:  0.7935960292816162  and  1.231461763381958  and  1.010956883430481\n",
      "total error:  3.036014676094055\n",
      "training error:  0.8110780715942383  and  1.3059660196304321  and  1.1022534370422363\n",
      "total error:  3.2192975282669067\n",
      "training error:  0.7906185984611511  and  1.3340855836868286  and  0.9832723140716553\n",
      "total error:  3.107976496219635\n",
      "training error:  0.7909114956855774  and  1.2636067867279053  and  0.9771624803543091\n",
      "total error:  3.0316807627677917\n",
      "training error:  0.767261266708374  and  1.2604281902313232  and  1.0588548183441162\n",
      "total error:  3.0865442752838135\n",
      "training error:  0.8002816438674927  and  1.3598761558532715  and  1.0328788757324219\n",
      "total error:  3.193036675453186\n",
      "training error:  0.8055654168128967  and  1.39988112449646  and  1.1024810075759888\n",
      "total error:  3.3079275488853455\n",
      "training error:  0.775686502456665  and  1.2490968704223633  and  1.054685354232788\n",
      "total error:  3.0794687271118164\n",
      "training error:  0.7839837074279785  and  1.2908027172088623  and  1.1204432249069214\n",
      "total error:  3.195229649543762\n",
      "training error:  0.7840861082077026  and  1.2440998554229736  and  1.0073388814926147\n",
      "total error:  3.035524845123291\n",
      "training error:  0.7909873723983765  and  1.2325348854064941  and  0.9784857034683228\n",
      "total error:  3.0020079612731934\n",
      "training error:  0.7664602994918823  and  1.2766982316970825  and  1.0441571474075317\n",
      "total error:  3.0873156785964966\n",
      "training error:  0.803357720375061  and  1.2583057880401611  and  1.0425645112991333\n",
      "total error:  3.1042280197143555\n",
      "training error:  0.7980105876922607  and  1.3697094917297363  and  1.1445960998535156\n",
      "total error:  3.3123161792755127\n",
      "training error:  0.7817556262016296  and  1.2592710256576538  and  1.0191991329193115\n",
      "total error:  3.060225784778595\n",
      "training error:  0.8075473308563232  and  1.2268580198287964  and  1.0302395820617676\n",
      "total error:  3.064644932746887\n",
      "training error:  0.7889437675476074  and  1.2672264575958252  and  1.0877134799957275\n",
      "total error:  3.14388370513916\n",
      "training error:  0.7698885798454285  and  1.3596394062042236  and  1.1122701168060303\n",
      "total error:  3.2417981028556824\n",
      "training error:  0.7657960653305054  and  1.2665294408798218  and  1.0899338722229004\n",
      "total error:  3.1222593784332275\n",
      "training error:  0.8015015125274658  and  1.2926177978515625  and  1.0262993574142456\n",
      "total error:  3.120418667793274\n",
      "training error:  0.8300763368606567  and  1.2772352695465088  and  1.07339346408844\n",
      "total error:  3.1807050704956055\n",
      "training error:  0.7734259366989136  and  1.2543166875839233  and  1.0756993293762207\n",
      "total error:  3.1034419536590576\n",
      "training error:  0.799670398235321  and  1.202805995941162  and  1.009390115737915\n",
      "total error:  3.011866509914398\n",
      "training error:  0.7995749115943909  and  1.3141933679580688  and  1.1361892223358154\n",
      "total error:  3.249957501888275\n",
      "training error:  0.8065198063850403  and  1.2002482414245605  and  0.9698877334594727\n",
      "total error:  2.9766557812690735\n",
      "training error:  0.7683027982711792  and  1.2121334075927734  and  1.0174751281738281\n",
      "total error:  2.9979113340377808\n",
      "training error:  0.7965081334114075  and  1.2574303150177002  and  0.9915777444839478\n",
      "total error:  3.0455161929130554\n",
      "training error:  0.7978848218917847  and  1.3057794570922852  and  1.1941627264022827\n",
      "total error:  3.2978270053863525\n",
      "training error:  0.812131404876709  and  1.3102270364761353  and  1.0866374969482422\n",
      "total error:  3.2089959383010864\n",
      "training error:  0.8004516959190369  and  1.322404146194458  and  1.1096529960632324\n",
      "total error:  3.2325088381767273\n",
      "training error:  0.7897747159004211  and  1.2435753345489502  and  1.0524684190750122\n",
      "total error:  3.0858184695243835\n",
      "training error:  0.7916152477264404  and  1.2616865634918213  and  1.029577374458313\n",
      "total error:  3.0828791856765747\n",
      "training error:  0.7720808982849121  and  1.2205195426940918  and  0.9936383962631226\n",
      "total error:  2.9862388372421265\n",
      "training error:  0.7705633044242859  and  1.2384165525436401  and  1.0073614120483398\n",
      "total error:  3.016341269016266\n",
      "training error:  0.8201866149902344  and  1.2866849899291992  and  1.0823884010314941\n",
      "total error:  3.1892600059509277\n",
      "training error:  0.7628625631332397  and  1.3024120330810547  and  1.025977373123169\n",
      "total error:  3.0912519693374634\n",
      "training error:  0.7794473767280579  and  1.4305059909820557  and  1.055856466293335\n",
      "total error:  3.2658098340034485\n",
      "training error:  0.7628598809242249  and  1.2593438625335693  and  0.9895755052566528\n",
      "total error:  3.011779248714447\n",
      "training error:  0.7580091953277588  and  1.2358049154281616  and  1.0105023384094238\n",
      "total error:  3.0043164491653442\n",
      "training error:  0.7619833946228027  and  1.220226764678955  and  1.0223636627197266\n",
      "total error:  3.0045738220214844\n",
      "training error:  0.7731342315673828  and  1.217484474182129  and  1.0481287240982056\n",
      "total error:  3.0387474298477173\n",
      "training error:  0.7744606733322144  and  1.4259648323059082  and  1.2388722896575928\n",
      "total error:  3.4392977952957153\n",
      "training error:  0.783202052116394  and  1.4678173065185547  and  1.0884190797805786\n",
      "total error:  3.3394384384155273\n",
      "training error:  0.7757350206375122  and  1.2986263036727905  and  0.9783157110214233\n",
      "total error:  3.052677035331726\n",
      "training error:  0.7843377590179443  and  1.2493953704833984  and  0.972557783126831\n",
      "total error:  3.006290912628174\n",
      "training error:  0.8140300512313843  and  1.3588987588882446  and  1.0679759979248047\n",
      "total error:  3.2409048080444336\n",
      "training error:  0.8008430600166321  and  1.4364752769470215  and  1.1117489337921143\n",
      "total error:  3.349067270755768\n",
      "training error:  0.745252251625061  and  1.2604600191116333  and  1.0181604623794556\n",
      "total error:  3.02387273311615\n",
      "training error:  0.8129266500473022  and  1.2217305898666382  and  1.045615315437317\n",
      "total error:  3.0802725553512573\n",
      "training error:  0.7673531174659729  and  1.2476383447647095  and  1.0165473222732544\n",
      "total error:  3.0315387845039368\n",
      "training error:  0.7707495093345642  and  1.2407057285308838  and  0.9840681552886963\n",
      "total error:  2.9955233931541443\n",
      "training error:  0.7592216730117798  and  1.253112554550171  and  0.9829617738723755\n",
      "total error:  2.995296001434326\n",
      "training error:  0.7748585939407349  and  1.246692180633545  and  0.990542471408844\n",
      "total error:  3.012093245983124\n",
      "training error:  0.7562960386276245  and  1.3191156387329102  and  0.9945123195648193\n",
      "total error:  3.069923996925354\n",
      "training error:  0.7694897651672363  and  1.2417954206466675  and  1.0112152099609375\n",
      "total error:  3.0225003957748413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7938895225524902  and  1.2651375532150269  and  1.0066745281219482\n",
      "total error:  3.0657016038894653\n",
      "training error:  0.7784528732299805  and  1.304269552230835  and  1.0272576808929443\n",
      "total error:  3.1099801063537598\n",
      "training error:  0.7715979814529419  and  1.2569568157196045  and  1.025928020477295\n",
      "total error:  3.0544828176498413\n",
      "training error:  0.7880673408508301  and  1.2332899570465088  and  1.022209882736206\n",
      "total error:  3.043567180633545\n",
      "training error:  0.7726867198944092  and  1.2417716979980469  and  0.9929215908050537\n",
      "total error:  3.0073800086975098\n",
      "training error:  0.7796928882598877  and  1.2410051822662354  and  1.0918078422546387\n",
      "total error:  3.1125059127807617\n",
      "training error:  0.7716183066368103  and  1.2621980905532837  and  1.0302025079727173\n",
      "total error:  3.0640189051628113\n",
      "training error:  0.7746267318725586  and  1.3538544178009033  and  1.0236198902130127\n",
      "total error:  3.1521010398864746\n",
      "training error:  0.7620975375175476  and  1.2974507808685303  and  0.9743083715438843\n",
      "total error:  3.033856689929962\n",
      "training error:  0.7707703113555908  and  1.262027621269226  and  0.9689348340034485\n",
      "total error:  3.0017327666282654\n",
      "training error:  0.7925973534584045  and  1.2847578525543213  and  1.0013418197631836\n",
      "total error:  3.0786970257759094\n",
      "training error:  0.7743649482727051  and  1.317885398864746  and  1.1120235919952393\n",
      "total error:  3.2042739391326904\n",
      "training error:  0.7695016860961914  and  1.3699225187301636  and  1.0336272716522217\n",
      "total error:  3.1730514764785767\n",
      "training error:  0.7423179745674133  and  1.2001044750213623  and  1.0080302953720093\n",
      "total error:  2.950452744960785\n",
      "training error:  0.7707504034042358  and  1.2419016361236572  and  1.050802230834961\n",
      "total error:  3.063454270362854\n",
      "training error:  0.76377272605896  and  1.2752480506896973  and  1.0403860807418823\n",
      "total error:  3.0794068574905396\n",
      "training error:  0.7675125598907471  and  1.3879485130310059  and  1.0924323797225952\n",
      "total error:  3.247893452644348\n",
      "training error:  0.7669376134872437  and  1.213652491569519  and  1.0239946842193604\n",
      "total error:  3.004584789276123\n",
      "training error:  0.7617465257644653  and  1.3144737482070923  and  1.0600415468215942\n",
      "total error:  3.136261820793152\n",
      "training error:  0.7473815679550171  and  1.2009543180465698  and  0.9851243495941162\n",
      "total error:  2.933460235595703\n",
      "training error:  0.7718895673751831  and  1.229386806488037  and  0.984775185585022\n",
      "total error:  2.986051559448242\n",
      "training error:  0.749720573425293  and  1.2830730676651  and  1.064084768295288\n",
      "total error:  3.096878409385681\n",
      "training error:  0.7454286217689514  and  1.2467303276062012  and  1.0723096132278442\n",
      "total error:  3.064468562602997\n",
      "training error:  0.7529676556587219  and  1.2639795541763306  and  1.0261309146881104\n",
      "total error:  3.043078124523163\n",
      "training error:  0.757477343082428  and  1.2208530902862549  and  1.0162177085876465\n",
      "total error:  2.9945481419563293\n",
      "training error:  0.7548058032989502  and  1.2546610832214355  and  1.0550950765609741\n",
      "total error:  3.06456196308136\n",
      "training error:  0.7660355567932129  and  1.2502906322479248  and  0.9900269508361816\n",
      "total error:  3.0063531398773193\n",
      "training error:  0.7615822553634644  and  1.2559561729431152  and  0.9956059455871582\n",
      "total error:  3.013144373893738\n",
      "training error:  0.7780367732048035  and  1.222651481628418  and  1.0173667669296265\n",
      "total error:  3.018055021762848\n",
      "training error:  0.77259361743927  and  1.3430676460266113  and  1.1219706535339355\n",
      "total error:  3.237631916999817\n",
      "training error:  0.7792251706123352  and  1.293528437614441  and  1.0044224262237549\n",
      "total error:  3.077176034450531\n",
      "training error:  0.7538154125213623  and  1.2066670656204224  and  0.997509777545929\n",
      "total error:  2.9579922556877136\n",
      "training error:  0.7411559820175171  and  1.2045214176177979  and  1.0221350193023682\n",
      "total error:  2.967812418937683\n",
      "training error:  0.7560368776321411  and  1.3030253648757935  and  1.0015413761138916\n",
      "total error:  3.060603618621826\n",
      "training error:  0.7903748154640198  and  1.2738691568374634  and  1.0177847146987915\n",
      "total error:  3.0820286870002747\n",
      "training error:  0.7487349510192871  and  1.2278721332550049  and  0.9789326190948486\n",
      "total error:  2.9555397033691406\n",
      "training error:  0.748113214969635  and  1.2386736869812012  and  1.04347825050354\n",
      "total error:  3.030265152454376\n",
      "training error:  0.7581692934036255  and  1.2213022708892822  and  0.9755598306655884\n",
      "total error:  2.955031394958496\n",
      "training error:  0.7689300775527954  and  1.235082983970642  and  1.0222713947296143\n",
      "total error:  3.0262844562530518\n",
      "training error:  0.7726833820343018  and  1.2861382961273193  and  1.0856587886810303\n",
      "total error:  3.1444804668426514\n",
      "training error:  0.7582250833511353  and  1.2589114904403687  and  1.0232396125793457\n",
      "total error:  3.0403761863708496\n",
      "training error:  0.8020503520965576  and  1.3187754154205322  and  1.0648525953292847\n",
      "total error:  3.1856783628463745\n",
      "training error:  0.7383411526679993  and  1.2331783771514893  and  1.0474886894226074\n",
      "total error:  3.019008219242096\n",
      "training error:  0.7809265851974487  and  1.2547962665557861  and  1.0214754343032837\n",
      "total error:  3.0571982860565186\n",
      "training error:  0.7288209199905396  and  1.2382819652557373  and  1.0052088499069214\n",
      "total error:  2.9723117351531982\n",
      "training error:  0.765874981880188  and  1.2084109783172607  and  1.0156097412109375\n",
      "total error:  2.9898957014083862\n",
      "training error:  0.7552727460861206  and  1.2286145687103271  and  0.9990018010139465\n",
      "total error:  2.9828891158103943\n",
      "training error:  0.7714784145355225  and  1.260101079940796  and  1.0071535110473633\n",
      "total error:  3.0387330055236816\n",
      "training error:  0.7384771108627319  and  1.3997403383255005  and  1.0970442295074463\n",
      "total error:  3.2352616786956787\n",
      "training error:  0.744394838809967  and  1.1940782070159912  and  0.9775577783584595\n",
      "total error:  2.9160308241844177\n",
      "training error:  0.738002598285675  and  1.213172197341919  and  0.9928798675537109\n",
      "total error:  2.944054663181305\n",
      "training error:  0.7415478229522705  and  1.1983006000518799  and  1.0033692121505737\n",
      "total error:  2.943217635154724\n",
      "training error:  0.7672043442726135  and  1.243849754333496  and  1.0294039249420166\n",
      "total error:  3.040458023548126\n",
      "training error:  0.7428039908409119  and  1.29879891872406  and  0.973755419254303\n",
      "total error:  3.015358328819275\n",
      "training error:  0.7419018149375916  and  1.2132601737976074  and  1.0304746627807617\n",
      "total error:  2.9856366515159607\n",
      "training error:  0.7611799240112305  and  1.2551524639129639  and  1.0187962055206299\n",
      "total error:  3.035128593444824\n",
      "training error:  0.7438284158706665  and  1.250383973121643  and  1.056748867034912\n",
      "total error:  3.0509612560272217\n",
      "training error:  0.7817444205284119  and  1.2738449573516846  and  1.0162279605865479\n",
      "total error:  3.0718173384666443\n",
      "training error:  0.7572742700576782  and  1.2131202220916748  and  1.0386840105056763\n",
      "total error:  3.0090785026550293\n",
      "training error:  0.7367824912071228  and  1.2950271368026733  and  1.0538172721862793\n",
      "total error:  3.0856269001960754\n",
      "training error:  0.8105595707893372  and  1.259230136871338  and  0.9728127121925354\n",
      "total error:  3.0426024198532104\n",
      "training error:  0.7609426975250244  and  1.405541181564331  and  1.1303528547286987\n",
      "total error:  3.296836733818054\n",
      "training error:  0.7440601587295532  and  1.20987868309021  and  0.967282772064209\n",
      "total error:  2.921221613883972\n",
      "training error:  0.745002269744873  and  1.2490603923797607  and  1.0056912899017334\n",
      "total error:  2.999753952026367\n",
      "training error:  0.7509812116622925  and  1.2092814445495605  and  0.9858717918395996\n",
      "total error:  2.9461344480514526\n",
      "training error:  0.75913405418396  and  1.2223517894744873  and  1.0109224319458008\n",
      "total error:  2.992408275604248\n",
      "training error:  0.7354981899261475  and  1.2577784061431885  and  1.0236310958862305\n",
      "total error:  3.0169076919555664\n",
      "training error:  0.715478777885437  and  1.2454159259796143  and  1.0067613124847412\n",
      "total error:  2.9676560163497925\n",
      "training error:  0.7234369516372681  and  1.2109817266464233  and  0.9941064715385437\n",
      "total error:  2.928525149822235\n",
      "training error:  0.7563285827636719  and  1.2064024209976196  and  0.9557275176048279\n",
      "total error:  2.9184585213661194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7348832488059998  and  1.2141796350479126  and  0.9704897403717041\n",
      "total error:  2.9195526242256165\n",
      "training error:  0.7746350169181824  and  1.2336339950561523  and  0.9452230930328369\n",
      "total error:  2.9534921050071716\n",
      "training error:  0.7507161498069763  and  1.290914535522461  and  1.2111635208129883\n",
      "total error:  3.2527942061424255\n",
      "training error:  0.7731152772903442  and  1.3159255981445312  and  1.231755256652832\n",
      "total error:  3.3207961320877075\n",
      "training error:  0.7744698524475098  and  1.2486809492111206  and  1.057438611984253\n",
      "total error:  3.0805894136428833\n",
      "training error:  0.7568436861038208  and  1.1956672668457031  and  0.9849082827568054\n",
      "total error:  2.9374192357063293\n",
      "training error:  0.7379083633422852  and  1.2324882745742798  and  1.0257530212402344\n",
      "total error:  2.9961496591567993\n",
      "training error:  0.7217824459075928  and  1.2152082920074463  and  0.9692016839981079\n",
      "total error:  2.906192421913147\n",
      "training error:  0.7246823906898499  and  1.3304729461669922  and  0.9687039852142334\n",
      "total error:  3.0238593220710754\n",
      "training error:  0.7762698531150818  and  1.2928385734558105  and  0.9730027318000793\n",
      "total error:  3.0421111583709717\n",
      "training error:  0.767742395401001  and  1.2343153953552246  and  1.0903384685516357\n",
      "total error:  3.0923962593078613\n",
      "training error:  0.7444136142730713  and  1.2267649173736572  and  0.9890700578689575\n",
      "total error:  2.960248589515686\n",
      "training error:  0.745812177658081  and  1.2835519313812256  and  1.0081589221954346\n",
      "total error:  3.037523031234741\n",
      "training error:  0.7289400696754456  and  1.2460901737213135  and  0.9485073685646057\n",
      "total error:  2.9235376119613647\n",
      "training error:  0.7737051248550415  and  1.3609843254089355  and  1.0654150247573853\n",
      "total error:  3.2001044750213623\n",
      "training error:  0.7576619982719421  and  1.3334019184112549  and  1.02871835231781\n",
      "total error:  3.119782269001007\n",
      "training error:  0.7322428226470947  and  1.253179669380188  and  1.0329350233078003\n",
      "total error:  3.018357515335083\n",
      "training error:  0.7336506843566895  and  1.1806998252868652  and  0.9775142073631287\n",
      "total error:  2.8918647170066833\n",
      "training error:  0.7157270908355713  and  1.2092596292495728  and  0.9845101237297058\n",
      "total error:  2.90949684381485\n",
      "training error:  0.7400177121162415  and  1.226820468902588  and  0.944794774055481\n",
      "total error:  2.9116329550743103\n",
      "training error:  0.7418193817138672  and  1.186143159866333  and  0.9399396777153015\n",
      "total error:  2.8679022192955017\n",
      "training error:  0.7211257815361023  and  1.1634132862091064  and  0.9583771228790283\n",
      "total error:  2.842916190624237\n",
      "training error:  0.7611273527145386  and  1.2429218292236328  and  1.0161707401275635\n",
      "total error:  3.020219922065735\n",
      "training error:  0.7520157098770142  and  1.2735488414764404  and  1.0521128177642822\n",
      "total error:  3.077677369117737\n",
      "training error:  0.7456398606300354  and  1.2513811588287354  and  0.9973862171173096\n",
      "total error:  2.9944072365760803\n",
      "training error:  0.7415780425071716  and  1.2184977531433105  and  0.9735159277915955\n",
      "total error:  2.9335917234420776\n",
      "training error:  0.7653313279151917  and  1.2324124574661255  and  1.0073540210723877\n",
      "total error:  3.005097806453705\n",
      "training error:  0.7690515518188477  and  1.2633306980133057  and  1.045716404914856\n",
      "total error:  3.0780986547470093\n",
      "training error:  0.7342698574066162  and  1.1931977272033691  and  0.9535242319107056\n",
      "total error:  2.880991816520691\n",
      "training error:  0.7468833923339844  and  1.3583741188049316  and  0.9938765168190002\n",
      "total error:  3.0991340279579163\n",
      "training error:  0.7290008664131165  and  1.18766450881958  and  0.942054271697998\n",
      "total error:  2.8587196469306946\n",
      "training error:  0.7246218323707581  and  1.2597309350967407  and  1.0029876232147217\n",
      "total error:  2.9873403906822205\n",
      "training error:  0.7344831824302673  and  1.1819934844970703  and  0.9507541656494141\n",
      "total error:  2.8672308325767517\n",
      "training error:  0.7348892688751221  and  1.209797739982605  and  1.036820888519287\n",
      "total error:  2.981507897377014\n",
      "training error:  0.7023220062255859  and  1.1531651020050049  and  1.0023508071899414\n",
      "total error:  2.8578379154205322\n",
      "training error:  0.7367492914199829  and  1.3058879375457764  and  1.059269905090332\n",
      "total error:  3.1019071340560913\n",
      "training error:  0.7451050281524658  and  1.240782618522644  and  1.1515170335769653\n",
      "total error:  3.137404680252075\n",
      "training error:  0.748193621635437  and  1.2808295488357544  and  0.9719986319541931\n",
      "total error:  3.0010218024253845\n",
      "training error:  0.7213891744613647  and  1.2377383708953857  and  0.9575426578521729\n",
      "total error:  2.9166702032089233\n",
      "training error:  0.7291601896286011  and  1.2532297372817993  and  1.0056266784667969\n",
      "total error:  2.9880166053771973\n",
      "training error:  0.7257634997367859  and  1.222379446029663  and  1.0220582485198975\n",
      "total error:  2.9702011942863464\n",
      "training error:  0.7339016199111938  and  1.250939965248108  and  1.090666651725769\n",
      "total error:  3.075508236885071\n",
      "training error:  0.7605340480804443  and  1.3105634450912476  and  0.9563863277435303\n",
      "total error:  3.027483820915222\n",
      "training error:  0.7303190231323242  and  1.2775232791900635  and  1.0030913352966309\n",
      "total error:  3.0109336376190186\n",
      "training error:  0.7184008359909058  and  1.2558293342590332  and  0.9683506488800049\n",
      "total error:  2.942580819129944\n",
      "training error:  0.7198742628097534  and  1.2169692516326904  and  1.0466949939727783\n",
      "total error:  2.983538508415222\n",
      "training error:  0.7268531322479248  and  1.3387056589126587  and  1.0662859678268433\n",
      "total error:  3.1318447589874268\n",
      "training error:  0.7252062559127808  and  1.2074074745178223  and  1.0079429149627686\n",
      "total error:  2.9405566453933716\n",
      "training error:  0.7358123660087585  and  1.219625473022461  and  1.0259617567062378\n",
      "total error:  2.9813995957374573\n",
      "training error:  0.7457840442657471  and  1.3488441705703735  and  1.0142714977264404\n",
      "total error:  3.108899712562561\n",
      "training error:  0.7225654125213623  and  1.2207785844802856  and  0.9951032400131226\n",
      "total error:  2.9384472370147705\n",
      "training error:  0.7231169939041138  and  1.1995775699615479  and  0.9748955368995667\n",
      "total error:  2.8975901007652283\n",
      "training error:  0.7322033047676086  and  1.242175817489624  and  0.9599157571792603\n",
      "total error:  2.934294879436493\n",
      "training error:  0.7248356342315674  and  1.2386345863342285  and  1.0461264848709106\n",
      "total error:  3.0095967054367065\n",
      "training error:  0.7334991097450256  and  1.2472920417785645  and  0.9469072818756104\n",
      "total error:  2.9276984333992004\n",
      "training error:  0.7059338092803955  and  1.2059450149536133  and  1.0051898956298828\n",
      "total error:  2.9170687198638916\n",
      "training error:  0.7315948605537415  and  1.4370031356811523  and  1.0901875495910645\n",
      "total error:  3.2587855458259583\n",
      "training error:  0.7409350872039795  and  1.3049333095550537  and  0.9456316232681274\n",
      "total error:  2.9915000200271606\n",
      "training error:  0.6868520379066467  and  1.1796679496765137  and  0.9472551345825195\n",
      "total error:  2.81377512216568\n",
      "training error:  0.7238755822181702  and  1.2596994638442993  and  0.9543032646179199\n",
      "total error:  2.9378783106803894\n",
      "training error:  0.7031857371330261  and  1.185633897781372  and  0.9820769429206848\n",
      "total error:  2.870896577835083\n",
      "training error:  0.7186694145202637  and  1.1743614673614502  and  0.9545249342918396\n",
      "total error:  2.8475558161735535\n",
      "training error:  0.7293995022773743  and  1.29989755153656  and  0.9821726083755493\n",
      "total error:  3.0114696621894836\n",
      "training error:  0.71535325050354  and  1.2508466243743896  and  0.9614510536193848\n",
      "total error:  2.9276509284973145\n",
      "training error:  0.7367360591888428  and  1.2190468311309814  and  0.9785071611404419\n",
      "total error:  2.934290051460266\n",
      "training error:  0.7248139381408691  and  1.1780171394348145  and  1.0007236003875732\n",
      "total error:  2.903554677963257\n",
      "training error:  0.7362101674079895  and  1.3004571199417114  and  1.0540119409561157\n",
      "total error:  3.0906792283058167\n",
      "training error:  0.7296203970909119  and  1.2895517349243164  and  0.9835183620452881\n",
      "total error:  3.0026904940605164\n",
      "training error:  0.722023069858551  and  1.204402208328247  and  0.9741048216819763\n",
      "total error:  2.9005300998687744\n",
      "training error:  0.6971172094345093  and  1.158033847808838  and  0.988092303276062\n",
      "total error:  2.843243360519409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.7532034516334534  and  1.2718703746795654  and  1.072311520576477\n",
      "total error:  3.097385346889496\n",
      "training error:  0.7021301984786987  and  1.2576329708099365  and  0.9816451668739319\n",
      "total error:  2.941408336162567\n",
      "training error:  0.7351552248001099  and  1.3407070636749268  and  1.0910520553588867\n",
      "total error:  3.1669143438339233\n",
      "training error:  0.7605706453323364  and  1.2524352073669434  and  1.049459457397461\n",
      "total error:  3.0624653100967407\n",
      "training error:  0.7294942736625671  and  1.188984751701355  and  0.9930278658866882\n",
      "total error:  2.9115068912506104\n",
      "training error:  0.6911059021949768  and  1.2197957038879395  and  0.9581719040870667\n",
      "total error:  2.869073510169983\n",
      "training error:  0.7320231199264526  and  1.2710025310516357  and  0.987063467502594\n",
      "total error:  2.9900891184806824\n",
      "training error:  0.7240985631942749  and  1.2315380573272705  and  0.9676340818405151\n",
      "total error:  2.9232707023620605\n",
      "training error:  0.7352137565612793  and  1.246317982673645  and  0.97239750623703\n",
      "total error:  2.9539292454719543\n",
      "training error:  0.6856285333633423  and  1.188930869102478  and  0.9461288452148438\n",
      "total error:  2.820688247680664\n",
      "training error:  0.7129405736923218  and  1.2578401565551758  and  0.9947278499603271\n",
      "total error:  2.9655085802078247\n",
      "training error:  0.7176288366317749  and  1.1951861381530762  and  0.9777008295059204\n",
      "total error:  2.8905158042907715\n",
      "training error:  0.7181816697120667  and  1.2521963119506836  and  1.0044856071472168\n",
      "total error:  2.974863588809967\n",
      "training error:  0.6958750486373901  and  1.182720422744751  and  0.940621018409729\n",
      "total error:  2.81921648979187\n",
      "training error:  0.7082356214523315  and  1.3734222650527954  and  1.0021981000900269\n",
      "total error:  3.083855986595154\n",
      "training error:  0.7121285796165466  and  1.163482427597046  and  0.9841417670249939\n",
      "total error:  2.8597527742385864\n",
      "training error:  0.7243252992630005  and  1.1739165782928467  and  0.9955447316169739\n",
      "total error:  2.893786609172821\n",
      "training error:  0.7391102313995361  and  1.1785836219787598  and  1.0638360977172852\n",
      "total error:  2.981529951095581\n",
      "training error:  0.7414422631263733  and  1.3083306550979614  and  1.039473056793213\n",
      "total error:  3.0892459750175476\n",
      "training error:  0.7136853933334351  and  1.2372679710388184  and  1.0160318613052368\n",
      "total error:  2.9669852256774902\n",
      "training error:  0.7139772772789001  and  1.1451518535614014  and  0.9569993019104004\n",
      "total error:  2.816128432750702\n",
      "training error:  0.7125794291496277  and  1.2141876220703125  and  1.0343525409698486\n",
      "total error:  2.961119592189789\n",
      "training error:  0.7172777056694031  and  1.1647837162017822  and  0.9383646845817566\n",
      "total error:  2.820426106452942\n",
      "training error:  0.7164636850357056  and  1.240725040435791  and  0.9689372777938843\n",
      "total error:  2.926126003265381\n",
      "training error:  0.7068768739700317  and  1.1678398847579956  and  1.0143264532089233\n",
      "total error:  2.8890432119369507\n",
      "training error:  0.7045880556106567  and  1.1526427268981934  and  0.9726014137268066\n",
      "total error:  2.8298321962356567\n",
      "training error:  0.7102236747741699  and  1.2064881324768066  and  0.9859503507614136\n",
      "total error:  2.90266215801239\n",
      "training error:  0.7253891229629517  and  1.1871272325515747  and  0.9929167032241821\n",
      "total error:  2.9054330587387085\n",
      "training error:  0.7342948913574219  and  1.1766630411148071  and  0.9431668519973755\n",
      "total error:  2.8541247844696045\n",
      "training error:  0.6939665079116821  and  1.152186393737793  and  0.9414032697677612\n",
      "total error:  2.7875561714172363\n",
      "training error:  0.7030377984046936  and  1.1924569606781006  and  0.9397823810577393\n",
      "total error:  2.8352771401405334\n",
      "training error:  0.69881272315979  and  1.2333226203918457  and  0.9650604724884033\n",
      "total error:  2.897195816040039\n",
      "training error:  0.694122314453125  and  1.1619000434875488  and  0.9835611581802368\n",
      "total error:  2.8395835161209106\n",
      "training error:  0.695830762386322  and  1.2049765586853027  and  0.9552406072616577\n",
      "total error:  2.8560479283332825\n",
      "training error:  0.7074435949325562  and  1.2035515308380127  and  1.053891658782959\n",
      "total error:  2.964886784553528\n",
      "training error:  0.7301485538482666  and  1.3175333738327026  and  1.0272505283355713\n",
      "total error:  3.0749324560165405\n",
      "training error:  0.7293522357940674  and  1.2204279899597168  and  1.0196863412857056\n",
      "total error:  2.9694665670394897\n",
      "training error:  0.7115349173545837  and  1.171478509902954  and  0.9657036066055298\n",
      "total error:  2.8487170338630676\n",
      "training error:  0.7649381160736084  and  1.444784164428711  and  1.0720024108886719\n",
      "total error:  3.281724691390991\n",
      "training error:  0.7169858813285828  and  1.229109764099121  and  0.9666799902915955\n",
      "total error:  2.9127756357192993\n",
      "training error:  0.711905300617218  and  1.253698706626892  and  1.0611878633499146\n",
      "total error:  3.0267918705940247\n",
      "training error:  0.715749979019165  and  1.3673768043518066  and  1.166308879852295\n",
      "total error:  3.2494356632232666\n",
      "training error:  0.7157566547393799  and  1.1849364042282104  and  1.0849382877349854\n",
      "total error:  2.9856313467025757\n",
      "training error:  0.7173015475273132  and  1.1841936111450195  and  0.9254573583602905\n",
      "total error:  2.8269525170326233\n",
      "training error:  0.6980770230293274  and  1.1771355867385864  and  0.9716833829879761\n",
      "total error:  2.84689599275589\n",
      "training error:  0.7007616758346558  and  1.1857889890670776  and  1.1003962755203247\n",
      "total error:  2.986946940422058\n",
      "training error:  0.712786078453064  and  1.1955753564834595  and  0.977629542350769\n",
      "total error:  2.8859909772872925\n",
      "training error:  0.7099156975746155  and  1.1957423686981201  and  1.0014674663543701\n",
      "total error:  2.9071255326271057\n",
      "training error:  0.7192009687423706  and  1.343246579170227  and  1.045181155204773\n",
      "total error:  3.1076287031173706\n",
      "training error:  0.706074595451355  and  1.2502648830413818  and  0.9928687810897827\n",
      "total error:  2.9492082595825195\n",
      "training error:  0.7041720151901245  and  1.1813554763793945  and  0.9866190552711487\n",
      "total error:  2.8721465468406677\n",
      "training error:  0.6858752369880676  and  1.1672570705413818  and  1.0070531368255615\n",
      "total error:  2.860185444355011\n",
      "training error:  0.7176758050918579  and  1.3328083753585815  and  1.017326831817627\n",
      "total error:  3.0678110122680664\n",
      "training error:  0.716153621673584  and  1.1481037139892578  and  0.9247816801071167\n",
      "total error:  2.7890390157699585\n",
      "training error:  0.7250792980194092  and  1.2044923305511475  and  0.9920598864555359\n",
      "total error:  2.9216315150260925\n",
      "training error:  0.7109930515289307  and  1.1923422813415527  and  1.0367404222488403\n",
      "total error:  2.9400757551193237\n",
      "training error:  0.7310745716094971  and  1.240431308746338  and  1.1589868068695068\n",
      "total error:  3.130492687225342\n",
      "training error:  0.7212502360343933  and  1.2152496576309204  and  0.9717841744422913\n",
      "total error:  2.908284068107605\n",
      "training error:  0.719596803188324  and  1.3446472883224487  and  1.0074764490127563\n",
      "total error:  3.071720540523529\n",
      "training error:  0.7144693732261658  and  1.16972017288208  and  0.9545571804046631\n",
      "total error:  2.838746726512909\n",
      "training error:  0.6812157034873962  and  1.2265915870666504  and  0.9984217286109924\n",
      "total error:  2.906229019165039\n",
      "training error:  0.7098770141601562  and  1.216989517211914  and  0.9342936873435974\n",
      "total error:  2.8611602187156677\n",
      "training error:  0.7193892002105713  and  1.3010358810424805  and  1.1156160831451416\n",
      "total error:  3.1360411643981934\n",
      "training error:  0.7101097106933594  and  1.224653720855713  and  1.1100071668624878\n",
      "total error:  3.04477059841156\n",
      "training error:  0.7125158309936523  and  1.1451548337936401  and  0.9489904046058655\n",
      "total error:  2.806661069393158\n",
      "training error:  0.7051918506622314  and  1.1742444038391113  and  0.9603174924850464\n",
      "total error:  2.839753746986389\n",
      "training error:  0.7068923115730286  and  1.1343679428100586  and  0.9627370238304138\n",
      "total error:  2.803997278213501\n",
      "training error:  0.706517219543457  and  1.2022428512573242  and  1.0160691738128662\n",
      "total error:  2.9248292446136475\n",
      "training error:  0.6971325278282166  and  1.1897205114364624  and  0.9675176739692688\n",
      "total error:  2.8543707132339478\n",
      "training error:  0.6862884759902954  and  1.1453094482421875  and  0.9472200870513916\n",
      "total error:  2.7788180112838745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6848692893981934  and  1.1448274850845337  and  0.8983487486839294\n",
      "total error:  2.7280455231666565\n",
      "training error:  0.6783990859985352  and  1.1704447269439697  and  0.9696337580680847\n",
      "total error:  2.8184775710105896\n",
      "training error:  0.6894669532775879  and  1.1667948961257935  and  0.9951034784317017\n",
      "total error:  2.851365327835083\n",
      "training error:  0.6918032169342041  and  1.1534779071807861  and  1.0051100254058838\n",
      "total error:  2.850391149520874\n",
      "training error:  0.6805319786071777  and  1.1639091968536377  and  0.9785377383232117\n",
      "total error:  2.822978913784027\n",
      "training error:  0.6958842277526855  and  1.1850404739379883  and  1.0405935049057007\n",
      "total error:  2.9215182065963745\n",
      "training error:  0.7036113142967224  and  1.221641182899475  and  1.0151909589767456\n",
      "total error:  2.940443456172943\n",
      "training error:  0.6865917444229126  and  1.1603293418884277  and  1.0168005228042603\n",
      "total error:  2.8637216091156006\n",
      "training error:  0.6767810583114624  and  1.2717552185058594  and  0.961709201335907\n",
      "total error:  2.9102454781532288\n",
      "training error:  0.6849151849746704  and  1.2327826023101807  and  0.9518109560012817\n",
      "total error:  2.869508743286133\n",
      "training error:  0.6977595090866089  and  1.1881403923034668  and  0.9830808043479919\n",
      "total error:  2.8689807057380676\n",
      "training error:  0.6815189123153687  and  1.1714380979537964  and  0.9427495002746582\n",
      "total error:  2.7957065105438232\n",
      "training error:  0.6792705655097961  and  1.1981666088104248  and  0.9579118490219116\n",
      "total error:  2.8353490233421326\n",
      "training error:  0.6743744015693665  and  1.1350141763687134  and  0.9469160437583923\n",
      "total error:  2.756304621696472\n",
      "training error:  0.6869102120399475  and  1.2136821746826172  and  0.9613164067268372\n",
      "total error:  2.861908793449402\n",
      "training error:  0.7017130255699158  and  1.1607115268707275  and  0.9450182914733887\n",
      "total error:  2.807442843914032\n",
      "training error:  0.6937499046325684  and  1.5429195165634155  and  1.25999116897583\n",
      "total error:  3.496660590171814\n",
      "training error:  0.7197684049606323  and  1.1749281883239746  and  0.9740210771560669\n",
      "total error:  2.868717670440674\n",
      "training error:  0.7114988565444946  and  1.2872073650360107  and  1.124610424041748\n",
      "total error:  3.1233166456222534\n",
      "training error:  0.7187952995300293  and  1.2118874788284302  and  0.9761327505111694\n",
      "total error:  2.906815528869629\n",
      "training error:  0.699116051197052  and  1.1425238847732544  and  0.9683612585067749\n",
      "total error:  2.8100011944770813\n",
      "training error:  0.6836337447166443  and  1.2273651361465454  and  0.9803937673568726\n",
      "total error:  2.8913926482200623\n",
      "training error:  0.6737895011901855  and  1.2113972902297974  and  0.9866275787353516\n",
      "total error:  2.8718143701553345\n",
      "training error:  0.7028893232345581  and  1.200991153717041  and  0.9928480386734009\n",
      "total error:  2.896728515625\n",
      "training error:  0.696363091468811  and  1.2272816896438599  and  1.016139030456543\n",
      "total error:  2.939783811569214\n",
      "training error:  0.6912378668785095  and  1.1807962656021118  and  1.009171724319458\n",
      "total error:  2.8812058568000793\n",
      "training error:  0.6878723502159119  and  1.1880900859832764  and  0.9704979658126831\n",
      "total error:  2.8464604020118713\n",
      "training error:  0.6988726258277893  and  1.2106314897537231  and  0.9254523515701294\n",
      "total error:  2.834956467151642\n",
      "training error:  0.6847633123397827  and  1.1921608448028564  and  0.9743813872337341\n",
      "total error:  2.8513055443763733\n",
      "training error:  0.676378071308136  and  1.1575169563293457  and  0.9601534605026245\n",
      "total error:  2.794048488140106\n",
      "training error:  0.7196119427680969  and  1.1660163402557373  and  0.963708221912384\n",
      "total error:  2.8493365049362183\n",
      "training error:  0.6693539619445801  and  1.2083311080932617  and  0.9611845016479492\n",
      "total error:  2.838869571685791\n",
      "training error:  0.6941273808479309  and  1.184019923210144  and  0.9458761215209961\n",
      "total error:  2.824023425579071\n",
      "training error:  0.6935046911239624  and  1.2427074909210205  and  0.9592310786247253\n",
      "total error:  2.8954432606697083\n",
      "training error:  0.6930823922157288  and  1.187438726425171  and  0.963114857673645\n",
      "total error:  2.8436359763145447\n",
      "training error:  0.676749587059021  and  1.1573094129562378  and  0.9116553664207458\n",
      "total error:  2.7457143664360046\n",
      "training error:  0.7121877074241638  and  1.2024744749069214  and  0.9677925705909729\n",
      "total error:  2.882454752922058\n",
      "training error:  0.7012593150138855  and  1.2098678350448608  and  0.9879174828529358\n",
      "total error:  2.899044632911682\n",
      "training error:  0.6708428859710693  and  1.1834678649902344  and  0.9532774686813354\n",
      "total error:  2.807588219642639\n",
      "training error:  0.6936015486717224  and  1.2053152322769165  and  0.9442623853683472\n",
      "total error:  2.843179166316986\n",
      "training error:  0.7066448926925659  and  1.324489712715149  and  1.0365617275238037\n",
      "total error:  3.0676963329315186\n",
      "training error:  0.6994557976722717  and  1.2573816776275635  and  1.0315505266189575\n",
      "total error:  2.9883880019187927\n",
      "training error:  0.6946069002151489  and  1.220578908920288  and  0.9431235790252686\n",
      "total error:  2.8583093881607056\n",
      "training error:  0.6735496520996094  and  1.191892385482788  and  1.0412579774856567\n",
      "total error:  2.906700015068054\n",
      "training error:  0.6770081520080566  and  1.175248146057129  and  0.9658056497573853\n",
      "total error:  2.818061947822571\n",
      "training error:  0.6766096949577332  and  1.1937719583511353  and  0.9629746675491333\n",
      "total error:  2.8333563208580017\n",
      "training error:  0.6986393332481384  and  1.1435930728912354  and  0.9215516448020935\n",
      "total error:  2.7637840509414673\n",
      "training error:  0.6754560470581055  and  1.2138001918792725  and  0.9956660270690918\n",
      "total error:  2.8849222660064697\n",
      "training error:  0.6747491955757141  and  1.136153221130371  and  0.92637699842453\n",
      "total error:  2.7372794151306152\n",
      "training error:  0.6894397735595703  and  1.184399962425232  and  0.9778363704681396\n",
      "total error:  2.851676106452942\n",
      "training error:  0.6768879890441895  and  1.1449851989746094  and  0.9072789549827576\n",
      "total error:  2.7291521430015564\n",
      "training error:  0.6852020025253296  and  1.1627062559127808  and  0.9759425520896912\n",
      "total error:  2.8238508105278015\n",
      "training error:  0.6861284971237183  and  1.2156267166137695  and  0.9978571534156799\n",
      "total error:  2.8996123671531677\n",
      "training error:  0.6820356845855713  and  1.1727564334869385  and  0.9781346321105957\n",
      "total error:  2.8329267501831055\n",
      "training error:  0.6864714622497559  and  1.1686463356018066  and  0.9585847854614258\n",
      "total error:  2.8137025833129883\n",
      "training error:  0.6754188537597656  and  1.334134817123413  and  1.0330123901367188\n",
      "total error:  3.0425660610198975\n",
      "training error:  0.7126702070236206  and  1.2048819065093994  and  0.9962208867073059\n",
      "total error:  2.913773000240326\n",
      "training error:  0.707984983921051  and  1.1855658292770386  and  1.0003383159637451\n",
      "total error:  2.8938891291618347\n",
      "training error:  0.665522575378418  and  1.1432292461395264  and  0.9383394718170166\n",
      "total error:  2.747091293334961\n",
      "training error:  0.6946137547492981  and  1.196082592010498  and  0.9005364179611206\n",
      "total error:  2.7912327647209167\n",
      "training error:  0.6825985908508301  and  1.1346251964569092  and  0.9256963133811951\n",
      "total error:  2.7429201006889343\n",
      "training error:  0.6586184501647949  and  1.1114445924758911  and  0.9323234558105469\n",
      "total error:  2.702386498451233\n",
      "training error:  0.7297834157943726  and  1.2226159572601318  and  0.9936628341674805\n",
      "total error:  2.946062207221985\n",
      "training error:  0.6722216606140137  and  1.1839704513549805  and  0.8980975151062012\n",
      "total error:  2.7542896270751953\n",
      "training error:  0.6749202013015747  and  1.1293030977249146  and  0.9438204765319824\n",
      "total error:  2.7480437755584717\n",
      "training error:  0.6650297045707703  and  1.1221580505371094  and  0.9314669966697693\n",
      "total error:  2.718654751777649\n",
      "training error:  0.6896657347679138  and  1.170090913772583  and  0.9581018090248108\n",
      "total error:  2.8178584575653076\n",
      "training error:  0.6797268390655518  and  1.1416144371032715  and  0.9644432067871094\n",
      "total error:  2.7857844829559326\n",
      "training error:  0.6741142868995667  and  1.1317851543426514  and  0.9656034708023071\n",
      "total error:  2.771502912044525\n",
      "training error:  0.7057938575744629  and  1.1797077655792236  and  0.9637549519538879\n",
      "total error:  2.8492565751075745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6521326899528503  and  1.1071079969406128  and  0.9186083078384399\n",
      "total error:  2.677848994731903\n",
      "training error:  0.7071826457977295  and  1.1767096519470215  and  0.976219654083252\n",
      "total error:  2.860111951828003\n",
      "training error:  0.6671779155731201  and  1.162737250328064  and  0.9391658902168274\n",
      "total error:  2.7690810561180115\n",
      "training error:  0.6791041493415833  and  1.1727848052978516  and  1.0083763599395752\n",
      "total error:  2.86026531457901\n",
      "training error:  0.6570221185684204  and  1.1750023365020752  and  0.9192061424255371\n",
      "total error:  2.7512305974960327\n",
      "training error:  0.6795530319213867  and  1.136615514755249  and  0.9213747978210449\n",
      "total error:  2.7375433444976807\n",
      "training error:  0.655441164970398  and  1.178556203842163  and  1.0867505073547363\n",
      "total error:  2.9207478761672974\n",
      "training error:  0.6702800989151001  and  1.1331431865692139  and  0.9075943231582642\n",
      "total error:  2.711017608642578\n",
      "training error:  0.6648313999176025  and  1.162743330001831  and  0.9558929800987244\n",
      "total error:  2.783467710018158\n",
      "training error:  0.6873789429664612  and  1.2142791748046875  and  0.9674471616744995\n",
      "total error:  2.869105279445648\n",
      "training error:  0.6992673873901367  and  1.256299376487732  and  1.0506583452224731\n",
      "total error:  3.006225109100342\n",
      "training error:  0.6963781118392944  and  1.1596097946166992  and  0.9371667504310608\n",
      "total error:  2.7931546568870544\n",
      "training error:  0.6648839712142944  and  1.2712359428405762  and  0.9816896319389343\n",
      "total error:  2.917809545993805\n",
      "training error:  0.6796356439590454  and  1.1518261432647705  and  0.9117099046707153\n",
      "total error:  2.7431716918945312\n",
      "training error:  0.6737334132194519  and  1.1692824363708496  and  0.9798977375030518\n",
      "total error:  2.8229135870933533\n",
      "training error:  0.6717380285263062  and  1.150343894958496  and  0.9035754203796387\n",
      "total error:  2.725657343864441\n",
      "training error:  0.6672513484954834  and  1.150923490524292  and  0.9190899133682251\n",
      "total error:  2.7372647523880005\n",
      "training error:  0.677280068397522  and  1.1479194164276123  and  0.9481625556945801\n",
      "total error:  2.7733620405197144\n",
      "training error:  0.655166506767273  and  1.157710313796997  and  1.0434719324111938\n",
      "total error:  2.856348752975464\n",
      "training error:  0.6764006018638611  and  1.1214845180511475  and  1.000483512878418\n",
      "total error:  2.7983686327934265\n",
      "training error:  0.6757395267486572  and  1.1517964601516724  and  0.9178962707519531\n",
      "total error:  2.7454322576522827\n",
      "training error:  0.6696938276290894  and  1.1870005130767822  and  0.9514286518096924\n",
      "total error:  2.808122992515564\n",
      "training error:  0.6817678213119507  and  1.2180652618408203  and  0.9403045773506165\n",
      "total error:  2.8401376605033875\n",
      "training error:  0.6744140386581421  and  1.157303810119629  and  0.9683993458747864\n",
      "total error:  2.8001171946525574\n",
      "training error:  0.6786410808563232  and  1.1674139499664307  and  1.0281500816345215\n",
      "total error:  2.8742051124572754\n",
      "training error:  0.695885181427002  and  1.274684190750122  and  1.0009526014328003\n",
      "total error:  2.9715219736099243\n",
      "training error:  0.6597201228141785  and  1.1925657987594604  and  0.9201388955116272\n",
      "total error:  2.772424817085266\n",
      "training error:  0.6807936429977417  and  1.1102677583694458  and  0.9210814237594604\n",
      "total error:  2.712142825126648\n",
      "training error:  0.6590925455093384  and  1.1874516010284424  and  0.9282811284065247\n",
      "total error:  2.7748252749443054\n",
      "training error:  0.6820501685142517  and  1.24495267868042  and  0.9424667954444885\n",
      "total error:  2.86946964263916\n",
      "training error:  0.6899091601371765  and  1.1835148334503174  and  0.9402424693107605\n",
      "total error:  2.8136664628982544\n",
      "training error:  0.6811630129814148  and  1.183605432510376  and  0.9735034704208374\n",
      "total error:  2.838271915912628\n",
      "training error:  0.65660560131073  and  1.1756911277770996  and  0.9300394058227539\n",
      "total error:  2.7623361349105835\n",
      "training error:  0.6691713333129883  and  1.1165603399276733  and  0.9623773097991943\n",
      "total error:  2.748108983039856\n",
      "training error:  0.6830015182495117  and  1.1686441898345947  and  0.992680013179779\n",
      "total error:  2.8443257212638855\n",
      "training error:  0.6564322710037231  and  1.103149175643921  and  0.9421276450157166\n",
      "total error:  2.7017090916633606\n",
      "training error:  0.6576188802719116  and  1.221269130706787  and  0.9864749908447266\n",
      "total error:  2.8653630018234253\n",
      "training error:  0.670025110244751  and  1.415284514427185  and  1.1242250204086304\n",
      "total error:  3.2095346450805664\n",
      "training error:  0.669808030128479  and  1.1806626319885254  and  0.9767693281173706\n",
      "total error:  2.827239990234375\n",
      "training error:  0.67466801404953  and  1.1229585409164429  and  0.912250280380249\n",
      "total error:  2.709876835346222\n",
      "training error:  0.6762915849685669  and  1.2413791418075562  and  0.9737578630447388\n",
      "total error:  2.891428589820862\n",
      "training error:  0.6932435035705566  and  1.2298399209976196  and  0.9255545139312744\n",
      "total error:  2.8486379384994507\n",
      "training error:  0.6416244506835938  and  1.1398229598999023  and  0.9475810527801514\n",
      "total error:  2.7290284633636475\n",
      "training error:  0.6648061275482178  and  1.2593821287155151  and  1.0585596561431885\n",
      "total error:  2.9827479124069214\n",
      "training error:  0.7322617769241333  and  1.2936278581619263  and  1.0777051448822021\n",
      "total error:  3.1035947799682617\n",
      "training error:  0.6923287510871887  and  1.2975714206695557  and  1.0118117332458496\n",
      "total error:  3.001711905002594\n",
      "training error:  0.6827563047409058  and  1.2975680828094482  and  0.9863194227218628\n",
      "total error:  2.966643810272217\n",
      "training error:  0.6743897199630737  and  1.1997113227844238  and  0.9262288808822632\n",
      "total error:  2.8003299236297607\n",
      "training error:  0.6735200881958008  and  1.1924705505371094  and  0.9445585608482361\n",
      "total error:  2.8105491995811462\n",
      "training error:  0.6547849774360657  and  1.1279826164245605  and  0.9724996089935303\n",
      "total error:  2.7552672028541565\n",
      "training error:  0.6567715406417847  and  1.1962751150131226  and  0.988742470741272\n",
      "total error:  2.841789126396179\n",
      "training error:  0.6600056886672974  and  1.1350470781326294  and  0.9808372259140015\n",
      "total error:  2.7758899927139282\n",
      "training error:  0.647148847579956  and  1.1278012990951538  and  0.9098029732704163\n",
      "total error:  2.684753119945526\n",
      "training error:  0.6632757782936096  and  1.147933840751648  and  0.8918712735176086\n",
      "total error:  2.703080892562866\n",
      "training error:  0.6752909421920776  and  1.1468775272369385  and  0.9496735334396362\n",
      "total error:  2.7718420028686523\n",
      "training error:  0.6654062271118164  and  1.1151198148727417  and  0.9593708515167236\n",
      "total error:  2.7398968935012817\n",
      "training error:  0.648384153842926  and  1.1437327861785889  and  0.9074226021766663\n",
      "total error:  2.699539542198181\n",
      "training error:  0.6658984422683716  and  1.106985330581665  and  0.9144524335861206\n",
      "total error:  2.6873362064361572\n",
      "training error:  0.6668906211853027  and  1.1276991367340088  and  0.9157867431640625\n",
      "total error:  2.710376501083374\n",
      "training error:  0.6556535959243774  and  1.1632927656173706  and  0.8918988704681396\n",
      "total error:  2.7108452320098877\n",
      "training error:  0.678452730178833  and  1.2480251789093018  and  0.9053395986557007\n",
      "total error:  2.8318175077438354\n",
      "training error:  0.6866319179534912  and  1.169811725616455  and  0.9598154425621033\n",
      "total error:  2.8162590861320496\n",
      "training error:  0.6664636135101318  and  1.1471117734909058  and  0.8972303867340088\n",
      "total error:  2.7108057737350464\n",
      "training error:  0.6359952092170715  and  1.0939618349075317  and  0.9022126197814941\n",
      "total error:  2.6321696639060974\n",
      "training error:  0.6821845769882202  and  1.1894230842590332  and  0.9475244879722595\n",
      "total error:  2.819132149219513\n",
      "training error:  0.6716130375862122  and  1.1380283832550049  and  0.9440726637840271\n",
      "total error:  2.753714084625244\n",
      "training error:  0.6616641283035278  and  1.184107780456543  and  0.9210481643676758\n",
      "total error:  2.7668200731277466\n",
      "training error:  0.6589890122413635  and  1.1634918451309204  and  0.9387142658233643\n",
      "total error:  2.761195123195648\n",
      "training error:  0.7274909615516663  and  1.5533485412597656  and  0.9554589986801147\n",
      "total error:  3.2362985014915466\n",
      "training error:  0.6427501440048218  and  1.0887138843536377  and  0.9216369986534119\n",
      "total error:  2.6531010270118713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6729460954666138  and  1.1487395763397217  and  0.9916409254074097\n",
      "total error:  2.813326597213745\n",
      "training error:  0.6402976512908936  and  1.1000275611877441  and  0.921676516532898\n",
      "total error:  2.6620017290115356\n",
      "training error:  0.6797587871551514  and  1.1989924907684326  and  0.9753572940826416\n",
      "total error:  2.8541085720062256\n",
      "training error:  0.6522995233535767  and  1.2262341976165771  and  0.9944720268249512\n",
      "total error:  2.873005747795105\n",
      "training error:  0.6835119128227234  and  1.1463778018951416  and  0.8701382875442505\n",
      "total error:  2.7000280022621155\n",
      "training error:  0.6455821394920349  and  1.162536859512329  and  0.9037045836448669\n",
      "total error:  2.711823582649231\n",
      "training error:  0.6550247073173523  and  1.1480371952056885  and  0.9207537770271301\n",
      "total error:  2.723815679550171\n",
      "training error:  0.6664907336235046  and  1.1756995916366577  and  0.8918231725692749\n",
      "total error:  2.7340134978294373\n",
      "training error:  0.6569106578826904  and  1.3751219511032104  and  1.0043108463287354\n",
      "total error:  3.0363434553146362\n",
      "training error:  0.6350530385971069  and  1.134908676147461  and  0.9404138326644897\n",
      "total error:  2.7103755474090576\n",
      "training error:  0.6417614817619324  and  1.1514145135879517  and  1.0110490322113037\n",
      "total error:  2.8042250275611877\n",
      "training error:  0.6362963914871216  and  1.123472809791565  and  0.924695611000061\n",
      "total error:  2.6844648122787476\n",
      "training error:  0.6386584639549255  and  1.109978199005127  and  0.9437195062637329\n",
      "total error:  2.6923561692237854\n",
      "training error:  0.6541757583618164  and  1.1715112924575806  and  0.9092264771461487\n",
      "total error:  2.7349135279655457\n",
      "training error:  0.6518508195877075  and  1.1334660053253174  and  0.9104368686676025\n",
      "total error:  2.6957536935806274\n",
      "training error:  0.650235652923584  and  1.1600120067596436  and  1.0106043815612793\n",
      "total error:  2.820852041244507\n",
      "training error:  0.7130313515663147  and  1.359099268913269  and  1.1338303089141846\n",
      "total error:  3.2059609293937683\n",
      "training error:  0.6673632860183716  and  1.2298400402069092  and  0.9593595862388611\n",
      "total error:  2.856562912464142\n",
      "training error:  0.7061034440994263  and  1.1964439153671265  and  1.0284676551818848\n",
      "total error:  2.9310150146484375\n",
      "training error:  0.6563358306884766  and  1.1648532152175903  and  1.0073719024658203\n",
      "total error:  2.828560948371887\n",
      "training error:  0.6528725028038025  and  1.1223912239074707  and  0.8916072845458984\n",
      "total error:  2.6668710112571716\n",
      "training error:  0.6594678163528442  and  1.1564104557037354  and  0.935474693775177\n",
      "total error:  2.7513529658317566\n",
      "training error:  0.6590074300765991  and  1.33760404586792  and  1.0023272037506104\n",
      "total error:  2.9989386796951294\n",
      "training error:  0.6639338731765747  and  1.1649401187896729  and  0.9592746496200562\n",
      "total error:  2.7881486415863037\n",
      "training error:  0.656251847743988  and  1.1891905069351196  and  0.9604577422142029\n",
      "total error:  2.8059000968933105\n",
      "training error:  0.6400120258331299  and  1.1014032363891602  and  0.9397144317626953\n",
      "total error:  2.6811296939849854\n",
      "training error:  0.6464196443557739  and  1.144846796989441  and  0.9831550121307373\n",
      "total error:  2.774421453475952\n",
      "training error:  0.6818211078643799  and  1.182283878326416  and  0.9334309101104736\n",
      "total error:  2.7975358963012695\n",
      "training error:  0.6541389226913452  and  1.1471166610717773  and  0.9231019616127014\n",
      "total error:  2.724357545375824\n",
      "training error:  0.6487177014350891  and  1.1278539896011353  and  0.946078896522522\n",
      "total error:  2.7226505875587463\n",
      "training error:  0.66107177734375  and  1.1883959770202637  and  1.0203099250793457\n",
      "total error:  2.8697776794433594\n",
      "training error:  0.6461873054504395  and  1.1496124267578125  and  0.9553284645080566\n",
      "total error:  2.7511281967163086\n",
      "training error:  0.6566460132598877  and  1.1319655179977417  and  0.9103387594223022\n",
      "total error:  2.6989502906799316\n",
      "training error:  0.6608549356460571  and  1.1156272888183594  and  0.9411383271217346\n",
      "total error:  2.717620551586151\n",
      "training error:  0.6422997117042542  and  1.0997600555419922  and  0.9349727630615234\n",
      "total error:  2.6770325303077698\n",
      "training error:  0.6773982644081116  and  1.2523081302642822  and  0.9811211824417114\n",
      "total error:  2.9108275771141052\n",
      "training error:  0.6599130034446716  and  1.1617201566696167  and  0.9373385906219482\n",
      "total error:  2.7589717507362366\n",
      "training error:  0.656217098236084  and  1.1525179147720337  and  0.9830238819122314\n",
      "total error:  2.791758894920349\n",
      "training error:  0.6452174782752991  and  1.0923821926116943  and  0.8931844234466553\n",
      "total error:  2.6307840943336487\n",
      "training error:  0.6467061638832092  and  1.112404704093933  and  0.9517605900764465\n",
      "total error:  2.710871458053589\n",
      "training error:  0.6567668914794922  and  1.1468040943145752  and  0.9672168493270874\n",
      "total error:  2.770787835121155\n",
      "training error:  0.6452616453170776  and  1.1068673133850098  and  0.9576234817504883\n",
      "total error:  2.7097524404525757\n",
      "training error:  0.6430095434188843  and  1.1373217105865479  and  0.9683322310447693\n",
      "total error:  2.7486634850502014\n",
      "training error:  0.6498452425003052  and  1.1601593494415283  and  1.0293492078781128\n",
      "total error:  2.8393537998199463\n",
      "training error:  0.6361006498336792  and  1.0940024852752686  and  0.8786152005195618\n",
      "total error:  2.6087183356285095\n",
      "training error:  0.6524866819381714  and  1.160175085067749  and  0.9525278806686401\n",
      "total error:  2.7651896476745605\n",
      "training error:  0.6455472111701965  and  1.1269288063049316  and  0.9137473106384277\n",
      "total error:  2.686223328113556\n",
      "training error:  0.664569616317749  and  1.1180589199066162  and  0.9132963418960571\n",
      "total error:  2.6959248781204224\n",
      "training error:  0.6413137316703796  and  1.1240262985229492  and  0.9155877828598022\n",
      "total error:  2.680927813053131\n",
      "training error:  0.6361410617828369  and  1.1013596057891846  and  0.9795523881912231\n",
      "total error:  2.7170530557632446\n",
      "training error:  0.6386516094207764  and  1.1472792625427246  and  0.9309957027435303\n",
      "total error:  2.7169265747070312\n",
      "training error:  0.6386671662330627  and  1.093894124031067  and  0.8978869915008545\n",
      "total error:  2.630448281764984\n",
      "training error:  0.6392422318458557  and  1.135722279548645  and  0.9470611810684204\n",
      "total error:  2.722025692462921\n",
      "training error:  0.6627380847930908  and  1.1635441780090332  and  0.9712045788764954\n",
      "total error:  2.7974868416786194\n",
      "training error:  0.6818413734436035  and  1.2273114919662476  and  0.9613237977027893\n",
      "total error:  2.8704766631126404\n",
      "training error:  0.6663162708282471  and  1.1039849519729614  and  1.0408904552459717\n",
      "total error:  2.81119167804718\n",
      "training error:  0.6260625123977661  and  1.1236995458602905  and  0.9797117710113525\n",
      "total error:  2.729473829269409\n",
      "training error:  0.691537618637085  and  1.166028380393982  and  0.9514226913452148\n",
      "total error:  2.8089886903762817\n",
      "training error:  0.6739532947540283  and  1.118100643157959  and  0.9691015481948853\n",
      "total error:  2.7611554861068726\n",
      "training error:  0.6315857172012329  and  1.1302404403686523  and  0.9279797077178955\n",
      "total error:  2.6898058652877808\n",
      "training error:  0.6704632043838501  and  1.1045019626617432  and  0.9681450128555298\n",
      "total error:  2.743110179901123\n",
      "training error:  0.6402573585510254  and  1.15205979347229  and  0.9331233501434326\n",
      "total error:  2.725440502166748\n",
      "training error:  0.6563315987586975  and  1.186551570892334  and  0.9462197422981262\n",
      "total error:  2.7891029119491577\n",
      "training error:  0.6304787397384644  and  1.11190664768219  and  0.8929125070571899\n",
      "total error:  2.6352978944778442\n",
      "training error:  0.6727607250213623  and  1.2257124185562134  and  1.0072693824768066\n",
      "total error:  2.9057425260543823\n",
      "training error:  0.6330629587173462  and  1.1415112018585205  and  0.9081931114196777\n",
      "total error:  2.6827672719955444\n",
      "training error:  0.6331617832183838  and  1.098189353942871  and  0.9409963488578796\n",
      "total error:  2.6723474860191345\n",
      "training error:  0.6422226428985596  and  1.1853413581848145  and  0.932966947555542\n",
      "total error:  2.760530948638916\n",
      "training error:  0.6424511075019836  and  1.0888956785202026  and  1.0247220993041992\n",
      "total error:  2.7560688853263855\n",
      "training error:  0.6535016298294067  and  1.1288632154464722  and  1.1453059911727905\n",
      "total error:  2.9276708364486694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6641684174537659  and  1.1807677745819092  and  0.9144865274429321\n",
      "total error:  2.759422719478607\n",
      "training error:  0.6336236000061035  and  1.0969998836517334  and  0.9388994574546814\n",
      "total error:  2.6695229411125183\n",
      "training error:  0.6154282689094543  and  1.2413487434387207  and  0.9403674602508545\n",
      "total error:  2.7971444725990295\n",
      "training error:  0.635056734085083  and  1.2275357246398926  and  0.8842095136642456\n",
      "total error:  2.746801972389221\n",
      "training error:  0.6360870599746704  and  1.093353509902954  and  0.8903293013572693\n",
      "total error:  2.619769871234894\n",
      "training error:  0.6434231400489807  and  1.1235954761505127  and  0.8927925825119019\n",
      "total error:  2.6598111987113953\n",
      "training error:  0.6581672430038452  and  1.1026722192764282  and  0.9265464544296265\n",
      "total error:  2.6873859167099\n",
      "training error:  0.6409928202629089  and  1.1267788410186768  and  1.0431808233261108\n",
      "total error:  2.8109524846076965\n",
      "training error:  0.6273969411849976  and  1.093453049659729  and  0.8702120780944824\n",
      "total error:  2.591062068939209\n",
      "training error:  0.6247994899749756  and  1.0615618228912354  and  0.8589562177658081\n",
      "total error:  2.545317530632019\n",
      "training error:  0.6238999962806702  and  1.1156384944915771  and  0.9274473190307617\n",
      "total error:  2.666985809803009\n",
      "training error:  0.6298549175262451  and  1.1049692630767822  and  0.9633309841156006\n",
      "total error:  2.698155164718628\n",
      "training error:  0.6409793496131897  and  1.1350622177124023  and  0.8864089250564575\n",
      "total error:  2.6624504923820496\n",
      "training error:  0.61421799659729  and  1.0718770027160645  and  0.9061042070388794\n",
      "total error:  2.592199206352234\n",
      "training error:  0.6471619606018066  and  1.1593809127807617  and  0.980414867401123\n",
      "total error:  2.7869577407836914\n",
      "training error:  0.6432880163192749  and  1.1062418222427368  and  0.9349836111068726\n",
      "total error:  2.6845134496688843\n",
      "training error:  0.6219238042831421  and  1.1075212955474854  and  0.9808412194252014\n",
      "total error:  2.710286319255829\n",
      "training error:  0.6499972343444824  and  1.1420013904571533  and  0.920541524887085\n",
      "total error:  2.7125401496887207\n",
      "training error:  0.6291823387145996  and  1.127352237701416  and  0.9643179774284363\n",
      "total error:  2.720852553844452\n",
      "training error:  0.6421833038330078  and  1.1003773212432861  and  0.936598539352417\n",
      "total error:  2.679159164428711\n",
      "training error:  0.6561448574066162  and  1.21453857421875  and  0.9962446689605713\n",
      "total error:  2.8669281005859375\n",
      "training error:  0.6467654705047607  and  1.150099515914917  and  0.932096540927887\n",
      "total error:  2.7289615273475647\n",
      "training error:  0.6073474884033203  and  1.1644556522369385  and  0.909641444683075\n",
      "total error:  2.6814445853233337\n",
      "training error:  0.6128093600273132  and  1.1863982677459717  and  0.9343926310539246\n",
      "total error:  2.7336002588272095\n",
      "training error:  0.663144588470459  and  1.132297396659851  and  0.9465011358261108\n",
      "total error:  2.741943120956421\n",
      "training error:  0.6485005617141724  and  1.3167619705200195  and  0.984144926071167\n",
      "total error:  2.949407458305359\n",
      "training error:  0.6444377899169922  and  1.167952299118042  and  0.9595651626586914\n",
      "total error:  2.7719552516937256\n",
      "training error:  0.6331297755241394  and  1.1113877296447754  and  0.9231668710708618\n",
      "total error:  2.6676843762397766\n",
      "training error:  0.6446815729141235  and  1.156297206878662  and  0.9014462232589722\n",
      "total error:  2.702425003051758\n",
      "training error:  0.6238588690757751  and  1.1954549551010132  and  0.9281836748123169\n",
      "total error:  2.7474974989891052\n",
      "training error:  0.6097338199615479  and  1.1298415660858154  and  0.9393845200538635\n",
      "total error:  2.678959906101227\n",
      "training error:  0.642261266708374  and  1.2044340372085571  and  0.9834822416305542\n",
      "total error:  2.8301775455474854\n",
      "training error:  0.634102463722229  and  1.1206996440887451  and  0.9526855945587158\n",
      "total error:  2.70748770236969\n",
      "training error:  0.6433435678482056  and  1.1427181959152222  and  0.9713780879974365\n",
      "total error:  2.7574398517608643\n",
      "training error:  0.633034348487854  and  1.1295504570007324  and  0.9601317644119263\n",
      "total error:  2.7227165699005127\n",
      "training error:  0.6320560574531555  and  1.1120080947875977  and  0.8946526646614075\n",
      "total error:  2.6387168169021606\n",
      "training error:  0.6285035610198975  and  1.1175506114959717  and  0.8942135572433472\n",
      "total error:  2.6402677297592163\n",
      "training error:  0.6268583536148071  and  1.195468783378601  and  0.9788875579833984\n",
      "total error:  2.8012146949768066\n",
      "training error:  0.6238914132118225  and  1.1032726764678955  and  0.8954923152923584\n",
      "total error:  2.6226564049720764\n",
      "training error:  0.6708977222442627  and  1.1596308946609497  and  0.9268254041671753\n",
      "total error:  2.7573540210723877\n",
      "training error:  0.6466473340988159  and  1.1561789512634277  and  0.8735640048980713\n",
      "total error:  2.676390290260315\n",
      "training error:  0.616534411907196  and  1.0762250423431396  and  0.8596106171607971\n",
      "total error:  2.552370071411133\n",
      "training error:  0.6111063957214355  and  1.0766962766647339  and  0.9315823912620544\n",
      "total error:  2.619385063648224\n",
      "training error:  0.6289927959442139  and  1.0565719604492188  and  0.8889533877372742\n",
      "total error:  2.574518144130707\n",
      "training error:  0.6199255585670471  and  1.1379317045211792  and  0.9855384230613708\n",
      "total error:  2.743395686149597\n",
      "training error:  0.6364647746086121  and  1.133467674255371  and  0.9914318323135376\n",
      "total error:  2.7613642811775208\n",
      "training error:  0.6246508359909058  and  1.1338595151901245  and  0.9057290554046631\n",
      "total error:  2.6642394065856934\n",
      "training error:  0.6875563859939575  and  1.2103042602539062  and  0.9648253917694092\n",
      "total error:  2.862686038017273\n",
      "training error:  0.6420871019363403  and  1.120521068572998  and  0.9654093980789185\n",
      "total error:  2.728017568588257\n",
      "training error:  0.6284447908401489  and  1.1089510917663574  and  0.953208327293396\n",
      "total error:  2.6906042098999023\n",
      "training error:  0.6281883120536804  and  1.0819675922393799  and  0.9576702117919922\n",
      "total error:  2.6678261160850525\n",
      "training error:  0.6164542436599731  and  1.0791462659835815  and  0.927942156791687\n",
      "total error:  2.6235426664352417\n",
      "training error:  0.6416450142860413  and  1.259168267250061  and  0.9448728561401367\n",
      "total error:  2.845686137676239\n",
      "training error:  0.6623039245605469  and  1.182570457458496  and  0.9770742058753967\n",
      "total error:  2.8219485878944397\n",
      "training error:  0.6334540247917175  and  1.0960420370101929  and  0.9184920787811279\n",
      "total error:  2.6479881405830383\n",
      "training error:  0.6200858354568481  and  1.1218605041503906  and  0.9746504426002502\n",
      "total error:  2.716596782207489\n",
      "training error:  0.6391773223876953  and  1.1513973474502563  and  1.0465750694274902\n",
      "total error:  2.837149739265442\n",
      "training error:  0.6404684782028198  and  1.0826952457427979  and  0.8748955726623535\n",
      "total error:  2.598059296607971\n",
      "training error:  0.6208873987197876  and  1.1488463878631592  and  0.9888086318969727\n",
      "total error:  2.7585424184799194\n",
      "training error:  0.6291662454605103  and  1.1009974479675293  and  0.8576047420501709\n",
      "total error:  2.5877684354782104\n",
      "training error:  0.6204565763473511  and  1.0905909538269043  and  0.9254247546195984\n",
      "total error:  2.6364722847938538\n",
      "training error:  0.6521114706993103  and  1.136054515838623  and  0.932884693145752\n",
      "total error:  2.7210506796836853\n",
      "training error:  0.6306496858596802  and  1.1286357641220093  and  0.9080895781517029\n",
      "total error:  2.6673750281333923\n",
      "training error:  0.6518662571907043  and  1.1101696491241455  and  0.9500624537467957\n",
      "total error:  2.7120983600616455\n",
      "training error:  0.6531062722206116  and  1.1331799030303955  and  1.012132167816162\n",
      "total error:  2.798418343067169\n",
      "training error:  0.6618392467498779  and  1.1350951194763184  and  0.896095871925354\n",
      "total error:  2.6930302381515503\n",
      "training error:  0.6101837158203125  and  1.0991649627685547  and  0.9599437713623047\n",
      "total error:  2.669292449951172\n",
      "training error:  0.6446120738983154  and  1.2070372104644775  and  0.9295215606689453\n",
      "total error:  2.7811708450317383\n",
      "training error:  0.6506888270378113  and  1.1706914901733398  and  0.9489878416061401\n",
      "total error:  2.7703681588172913\n",
      "training error:  0.6344579458236694  and  1.1501529216766357  and  0.9489582180976868\n",
      "total error:  2.733569085597992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.6208950281143188  and  1.1797542572021484  and  0.917375385761261\n",
      "total error:  2.7180246710777283\n",
      "training error:  0.6229380965232849  and  1.1061586141586304  and  0.9468220472335815\n",
      "total error:  2.675918757915497\n",
      "training error:  0.6538971662521362  and  1.1650885343551636  and  0.9713515639305115\n",
      "total error:  2.7903372645378113\n",
      "training error:  0.6224360466003418  and  1.0789885520935059  and  0.8889142274856567\n",
      "total error:  2.5903388261795044\n",
      "training error:  0.6198209524154663  and  1.0914132595062256  and  0.8878932595252991\n",
      "total error:  2.599127471446991\n",
      "training error:  0.6234270334243774  and  1.252968668937683  and  1.0569431781768799\n",
      "total error:  2.9333388805389404\n",
      "training error:  0.6267377734184265  and  1.1154086589813232  and  0.8883962631225586\n",
      "total error:  2.6305426955223083\n",
      "training error:  0.6304716467857361  and  1.1464505195617676  and  0.9575619697570801\n",
      "total error:  2.7344841361045837\n",
      "training error:  0.6206470131874084  and  1.0865161418914795  and  0.8421581983566284\n",
      "total error:  2.5493213534355164\n",
      "training error:  0.6276144981384277  and  1.1004464626312256  and  1.0024605989456177\n",
      "total error:  2.730521559715271\n",
      "training error:  0.6111180186271667  and  1.0903241634368896  and  0.9209469556808472\n",
      "total error:  2.6223891377449036\n",
      "training error:  0.6272443532943726  and  1.0799846649169922  and  0.8914424180984497\n",
      "total error:  2.5986714363098145\n",
      "training error:  0.6442780494689941  and  1.1161932945251465  and  0.9464517831802368\n",
      "total error:  2.7069231271743774\n",
      "training error:  0.6267948746681213  and  1.0897235870361328  and  0.8577263355255127\n",
      "total error:  2.574244797229767\n",
      "training error:  0.6323146224021912  and  1.1132818460464478  and  0.9621127843856812\n",
      "total error:  2.70770925283432\n",
      "training error:  0.6256216764450073  and  1.1186254024505615  and  0.9344866871833801\n",
      "total error:  2.678733766078949\n",
      "training error:  0.6157569289207458  and  1.1120502948760986  and  0.9454967975616455\n",
      "total error:  2.67330402135849\n",
      "training error:  0.6085111498832703  and  1.088133692741394  and  0.8843256235122681\n",
      "total error:  2.5809704661369324\n",
      "training error:  0.6286483407020569  and  1.131988286972046  and  0.9461009502410889\n",
      "total error:  2.7067375779151917\n",
      "training error:  0.6573615670204163  and  1.6277031898498535  and  1.0836431980133057\n",
      "total error:  3.3687079548835754\n",
      "training error:  0.6371305584907532  and  1.079181432723999  and  0.9481848478317261\n",
      "total error:  2.6644968390464783\n",
      "training error:  0.6077583432197571  and  1.1015962362289429  and  0.8915346264839172\n",
      "total error:  2.600889205932617\n",
      "training error:  0.6261582970619202  and  1.0862414836883545  and  0.8793414235115051\n",
      "total error:  2.59174120426178\n",
      "training error:  0.6136301755905151  and  1.0994714498519897  and  0.9206024408340454\n",
      "total error:  2.6337040662765503\n",
      "training error:  0.6388959884643555  and  1.1534866094589233  and  0.882193922996521\n",
      "total error:  2.6745765209198\n",
      "training error:  0.6043426990509033  and  1.1202938556671143  and  0.8484311699867249\n",
      "total error:  2.5730677247047424\n",
      "training error:  0.6265703439712524  and  1.1470990180969238  and  0.9371744394302368\n",
      "total error:  2.710843801498413\n",
      "training error:  0.6784608364105225  and  1.1897892951965332  and  1.028734564781189\n",
      "total error:  2.8969846963882446\n",
      "training error:  0.6616597175598145  and  1.1216762065887451  and  0.9124215841293335\n",
      "total error:  2.695757508277893\n",
      "training error:  0.6044365763664246  and  1.040350079536438  and  0.9556921124458313\n",
      "total error:  2.600478768348694\n",
      "training error:  0.637133002281189  and  1.1026921272277832  and  1.0297623872756958\n",
      "total error:  2.769587516784668\n",
      "training error:  0.62972092628479  and  1.1027255058288574  and  0.8798290491104126\n",
      "total error:  2.61227548122406\n",
      "training error:  0.6009427309036255  and  1.0959692001342773  and  0.9754083752632141\n",
      "total error:  2.672320306301117\n",
      "training error:  0.608191967010498  and  1.0810596942901611  and  0.9138121008872986\n",
      "total error:  2.6030637621879578\n",
      "training error:  0.6338703036308289  and  1.1392945051193237  and  0.9687455892562866\n",
      "total error:  2.741910398006439\n",
      "training error:  0.6394109129905701  and  1.184173822402954  and  1.0465998649597168\n",
      "total error:  2.870184600353241\n",
      "training error:  0.6172550320625305  and  1.122514247894287  and  0.9195292592048645\n",
      "total error:  2.659298539161682\n",
      "training error:  0.6818629503250122  and  1.1940629482269287  and  0.941503643989563\n",
      "total error:  2.817429542541504\n",
      "training error:  0.646964430809021  and  1.2768157720565796  and  1.0450384616851807\n",
      "total error:  2.9688186645507812\n",
      "training error:  0.6169437170028687  and  1.081510305404663  and  0.871917724609375\n",
      "total error:  2.5703717470169067\n",
      "training error:  0.6037676334381104  and  1.0820987224578857  and  0.9487147927284241\n",
      "total error:  2.63458114862442\n",
      "training error:  0.653803288936615  and  1.0814540386199951  and  0.8493883609771729\n",
      "total error:  2.584645688533783\n",
      "training error:  0.6333802938461304  and  1.208298683166504  and  0.9355442523956299\n",
      "total error:  2.777223229408264\n",
      "training error:  0.647233784198761  and  1.149951457977295  and  0.9234175682067871\n",
      "total error:  2.720602810382843\n",
      "training error:  0.6307133436203003  and  1.0827069282531738  and  0.9255735874176025\n",
      "total error:  2.6389938592910767\n",
      "training error:  0.626949667930603  and  1.1668519973754883  and  0.9523476362228394\n",
      "total error:  2.7461493015289307\n",
      "training error:  0.6124668121337891  and  1.071811556816101  and  0.9041258096694946\n",
      "total error:  2.5884041786193848\n",
      "training error:  0.6091012954711914  and  1.0647214651107788  and  0.9075890779495239\n",
      "total error:  2.581411838531494\n",
      "training error:  0.6218559145927429  and  1.1195013523101807  and  0.8925647735595703\n",
      "total error:  2.633922040462494\n",
      "training error:  0.6052048802375793  and  1.1029959917068481  and  0.8585983514785767\n",
      "total error:  2.566799223423004\n",
      "training error:  0.6187036037445068  and  1.1089882850646973  and  0.8693865537643433\n",
      "total error:  2.5970784425735474\n",
      "training error:  0.6205188035964966  and  1.0908674001693726  and  0.8680307865142822\n",
      "total error:  2.5794169902801514\n",
      "training error:  0.6498923897743225  and  1.0929059982299805  and  0.8789323568344116\n",
      "total error:  2.6217307448387146\n",
      "training error:  0.6031582355499268  and  1.0666875839233398  and  0.894229531288147\n",
      "total error:  2.5640753507614136\n",
      "training error:  0.625707745552063  and  1.1419005393981934  and  0.9204208850860596\n",
      "total error:  2.688029170036316\n",
      "training error:  0.6134432554244995  and  1.109464406967163  and  0.8880348801612854\n",
      "total error:  2.610942542552948\n",
      "training error:  0.6223732233047485  and  1.1692553758621216  and  0.9153065085411072\n",
      "total error:  2.7069351077079773\n",
      "training error:  0.6063965559005737  and  1.118489384651184  and  0.918594241142273\n",
      "total error:  2.6434801816940308\n",
      "training error:  0.6065134406089783  and  1.1589667797088623  and  0.9824106693267822\n",
      "total error:  2.747890889644623\n",
      "training error:  0.6065081357955933  and  1.1335220336914062  and  0.9164907932281494\n",
      "total error:  2.656520962715149\n",
      "training error:  0.6086664199829102  and  1.0707683563232422  and  0.8998454809188843\n",
      "total error:  2.5792802572250366\n",
      "training error:  0.6009187698364258  and  1.0613560676574707  and  0.911857008934021\n",
      "total error:  2.5741318464279175\n",
      "training error:  0.5933586359024048  and  1.0522761344909668  and  0.8395751714706421\n",
      "total error:  2.4852099418640137\n",
      "training error:  0.5966736674308777  and  1.1366593837738037  and  0.9844483137130737\n",
      "total error:  2.717781364917755\n",
      "training error:  0.5984220504760742  and  1.053012490272522  and  0.8673098087310791\n",
      "total error:  2.5187443494796753\n",
      "training error:  0.5901103019714355  and  1.1186883449554443  and  0.9448485374450684\n",
      "total error:  2.6536471843719482\n",
      "training error:  0.5810895562171936  and  1.0826432704925537  and  0.8903913497924805\n",
      "total error:  2.554124176502228\n",
      "training error:  0.607884407043457  and  1.100536584854126  and  0.9089078307151794\n",
      "total error:  2.6173288226127625\n",
      "training error:  0.6161425709724426  and  1.0566184520721436  and  0.8924257755279541\n",
      "total error:  2.5651867985725403\n",
      "training error:  0.6134241223335266  and  1.0758426189422607  and  0.8908718824386597\n",
      "total error:  2.580138623714447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.616778552532196  and  1.074183464050293  and  0.8863476514816284\n",
      "total error:  2.5773096680641174\n",
      "training error:  0.6010257005691528  and  1.1126673221588135  and  0.8841454982757568\n",
      "total error:  2.597838521003723\n",
      "training error:  0.5903974771499634  and  1.0728521347045898  and  1.0115435123443604\n",
      "total error:  2.6747931241989136\n",
      "training error:  0.5904617309570312  and  1.0690412521362305  and  0.8753769993782043\n",
      "total error:  2.534879982471466\n",
      "training error:  0.6096457242965698  and  1.0966362953186035  and  0.8716248273849487\n",
      "total error:  2.577906847000122\n",
      "training error:  0.6123290061950684  and  1.143498420715332  and  1.0267784595489502\n",
      "total error:  2.7826058864593506\n",
      "training error:  0.6363744735717773  and  1.0727508068084717  and  0.9674950838088989\n",
      "total error:  2.676620364189148\n",
      "training error:  0.683756947517395  and  1.4054710865020752  and  1.3389204740524292\n",
      "total error:  3.4281485080718994\n",
      "training error:  0.6238699555397034  and  1.430182695388794  and  0.9819211959838867\n",
      "total error:  3.035973846912384\n",
      "training error:  0.6288638710975647  and  1.1547454595565796  and  0.9397445917129517\n",
      "total error:  2.723353922367096\n",
      "training error:  0.6261363625526428  and  1.197998285293579  and  0.9439510107040405\n",
      "total error:  2.7680856585502625\n",
      "training error:  0.598833441734314  and  1.0955665111541748  and  0.8612464070320129\n",
      "total error:  2.5556463599205017\n",
      "training error:  0.6084185838699341  and  1.102031946182251  and  0.9074518084526062\n",
      "total error:  2.6179023385047913\n",
      "training error:  0.5937516689300537  and  1.148451566696167  and  1.0049856901168823\n",
      "total error:  2.747188925743103\n",
      "training error:  0.5992159843444824  and  1.2501215934753418  and  0.8622684478759766\n",
      "total error:  2.711606025695801\n",
      "training error:  0.6056361794471741  and  1.160733938217163  and  0.8972073197364807\n",
      "total error:  2.663577437400818\n",
      "training error:  0.5857226848602295  and  1.073207974433899  and  0.8789725303649902\n",
      "total error:  2.5379031896591187\n",
      "training error:  0.6376315951347351  and  1.2154688835144043  and  0.9520055055618286\n",
      "total error:  2.805105984210968\n",
      "training error:  0.6066032648086548  and  1.1150548458099365  and  0.8709828853607178\n",
      "total error:  2.592640995979309\n",
      "training error:  0.5899426937103271  and  1.1033618450164795  and  0.9460517764091492\n",
      "total error:  2.639356315135956\n",
      "training error:  0.6343375444412231  and  1.132332682609558  and  0.9203221797943115\n",
      "total error:  2.6869924068450928\n",
      "training error:  0.5967510938644409  and  1.0564947128295898  and  0.8718708753585815\n",
      "total error:  2.5251166820526123\n",
      "training error:  0.5921403169631958  and  1.081897258758545  and  0.9003129005432129\n",
      "total error:  2.5743504762649536\n",
      "training error:  0.6140298247337341  and  1.0724437236785889  and  0.9466822743415833\n",
      "total error:  2.6331558227539062\n",
      "training error:  0.5947761535644531  and  1.0290697813034058  and  0.867379903793335\n",
      "total error:  2.491225838661194\n",
      "training error:  0.6125025749206543  and  1.2474455833435059  and  1.0271543264389038\n",
      "total error:  2.887102484703064\n",
      "training error:  0.6104292869567871  and  1.1195412874221802  and  0.8805585503578186\n",
      "total error:  2.610529124736786\n",
      "training error:  0.6270187497138977  and  1.1501089334487915  and  0.9801911115646362\n",
      "total error:  2.7573187947273254\n",
      "training error:  0.6093968749046326  and  1.065187931060791  and  0.8666405081748962\n",
      "total error:  2.54122531414032\n",
      "training error:  0.645391583442688  and  1.1511574983596802  and  1.066375494003296\n",
      "total error:  2.862924575805664\n",
      "training error:  0.6014842987060547  and  1.0835601091384888  and  0.9285575747489929\n",
      "total error:  2.6136019825935364\n",
      "training error:  0.5950751900672913  and  1.0971360206604004  and  0.9051881432533264\n",
      "total error:  2.597399353981018\n",
      "training error:  0.6136491298675537  and  1.0747548341751099  and  0.8810516595840454\n",
      "total error:  2.569455623626709\n",
      "training error:  0.5983089208602905  and  1.0815613269805908  and  0.9190074801445007\n",
      "total error:  2.598877727985382\n",
      "training error:  0.5924579501152039  and  1.055879831314087  and  0.9272928237915039\n",
      "total error:  2.5756306052207947\n",
      "training error:  0.5960831046104431  and  1.0827587842941284  and  0.8940283060073853\n",
      "total error:  2.572870194911957\n",
      "training error:  0.599358856678009  and  1.140550136566162  and  0.8855230808258057\n",
      "total error:  2.625432074069977\n",
      "training error:  0.5895978212356567  and  1.033613681793213  and  0.833740234375\n",
      "total error:  2.4569517374038696\n",
      "training error:  0.6471189260482788  and  1.135805368423462  and  0.9265915155410767\n",
      "total error:  2.7095158100128174\n",
      "training error:  0.6546177864074707  and  1.4359252452850342  and  1.009011149406433\n",
      "total error:  3.099554181098938\n",
      "training error:  0.6119412183761597  and  1.1192362308502197  and  0.933665931224823\n",
      "total error:  2.6648433804512024\n",
      "training error:  0.608871579170227  and  1.0654964447021484  and  0.883088231086731\n",
      "total error:  2.5574562549591064\n",
      "training error:  0.6067230701446533  and  1.1750335693359375  and  0.8835077285766602\n",
      "total error:  2.665264368057251\n",
      "training error:  0.6000329852104187  and  1.168737530708313  and  0.8373313546180725\n",
      "total error:  2.606101870536804\n",
      "training error:  0.615877628326416  and  1.0592739582061768  and  0.8776350617408752\n",
      "total error:  2.552786648273468\n",
      "training error:  0.62107253074646  and  1.1283376216888428  and  0.9998118281364441\n",
      "total error:  2.749221980571747\n",
      "training error:  0.6217024922370911  and  1.08292818069458  and  0.9520758390426636\n",
      "total error:  2.6567065119743347\n",
      "training error:  0.6103237867355347  and  1.1369030475616455  and  0.891995906829834\n",
      "total error:  2.639222741127014\n",
      "training error:  0.6036669611930847  and  1.1745907068252563  and  1.010857343673706\n",
      "total error:  2.789115011692047\n",
      "training error:  0.598496675491333  and  1.1507568359375  and  0.9164485335350037\n",
      "total error:  2.6657020449638367\n",
      "training error:  0.6073161363601685  and  1.1818740367889404  and  0.9202232956886292\n",
      "total error:  2.709413468837738\n",
      "training error:  0.6147288084030151  and  1.1196016073226929  and  0.96988445520401\n",
      "total error:  2.704214870929718\n",
      "training error:  0.5918556451797485  and  1.0513343811035156  and  0.83104008436203\n",
      "total error:  2.474230110645294\n",
      "training error:  0.5881244540214539  and  1.0614300966262817  and  0.8605924248695374\n",
      "total error:  2.510146975517273\n",
      "training error:  0.6197656393051147  and  1.1150341033935547  and  1.0095731019973755\n",
      "total error:  2.744372844696045\n",
      "training error:  0.6088560819625854  and  1.1370234489440918  and  0.8914032578468323\n",
      "total error:  2.6372827887535095\n",
      "training error:  0.5960161089897156  and  1.0554105043411255  and  0.8226610422134399\n",
      "total error:  2.474087655544281\n",
      "training error:  0.5950356721878052  and  1.0548447370529175  and  0.8597227931022644\n",
      "total error:  2.509603202342987\n",
      "training error:  0.5870288610458374  and  1.077420949935913  and  0.9218924045562744\n",
      "total error:  2.586342215538025\n",
      "training error:  0.5933666229248047  and  1.152773141860962  and  0.880709171295166\n",
      "total error:  2.6268489360809326\n",
      "training error:  0.5962277054786682  and  1.04248046875  and  0.9094272255897522\n",
      "total error:  2.5481353998184204\n",
      "training error:  0.6024927496910095  and  1.1046369075775146  and  1.0505599975585938\n",
      "total error:  2.757689654827118\n",
      "training error:  0.5881160497665405  and  1.077295184135437  and  0.889196515083313\n",
      "total error:  2.5546077489852905\n",
      "training error:  0.5887683629989624  and  1.0503456592559814  and  0.9109646081924438\n",
      "total error:  2.5500786304473877\n",
      "training error:  0.5942364931106567  and  1.0888392925262451  and  0.8859657645225525\n",
      "total error:  2.5690415501594543\n",
      "training error:  0.5904590487480164  and  1.064616322517395  and  0.9638395309448242\n",
      "total error:  2.6189149022102356\n",
      "training error:  0.5849162340164185  and  1.2118016481399536  and  0.9790845513343811\n",
      "total error:  2.775802433490753\n",
      "training error:  0.5780876874923706  and  1.0960582494735718  and  0.8819175958633423\n",
      "total error:  2.5560635328292847\n",
      "training error:  0.5951047539710999  and  1.058066964149475  and  0.8886357545852661\n",
      "total error:  2.541807472705841\n",
      "training error:  0.6207257509231567  and  1.0970768928527832  and  0.8831819295883179\n",
      "total error:  2.600984573364258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.5868736505508423  and  1.0327004194259644  and  0.856196939945221\n",
      "total error:  2.4757710099220276\n",
      "training error:  0.6117908954620361  and  1.1156730651855469  and  0.967422366142273\n",
      "total error:  2.694886326789856\n",
      "training error:  0.5870959162712097  and  1.0547388792037964  and  0.8965674638748169\n",
      "total error:  2.538402259349823\n",
      "training error:  0.6007768511772156  and  1.1104997396469116  and  0.8600520491600037\n",
      "total error:  2.571328639984131\n",
      "training error:  0.5926999449729919  and  1.0281925201416016  and  0.8437520265579224\n",
      "total error:  2.464644491672516\n",
      "training error:  0.579916775226593  and  1.0609657764434814  and  0.9119912385940552\n",
      "total error:  2.5528737902641296\n",
      "training error:  0.5922114849090576  and  1.088904857635498  and  0.9559053182601929\n",
      "total error:  2.6370216608047485\n",
      "training error:  0.6039060354232788  and  1.0776445865631104  and  0.8492053747177124\n",
      "total error:  2.5307559967041016\n",
      "training error:  0.6288250684738159  and  1.12833571434021  and  0.9538965225219727\n",
      "total error:  2.7110573053359985\n",
      "training error:  0.6112067699432373  and  1.0495110750198364  and  0.8521465063095093\n",
      "total error:  2.512864351272583\n",
      "training error:  0.5941458344459534  and  1.0560085773468018  and  0.8747152090072632\n",
      "total error:  2.5248696208000183\n",
      "training error:  0.5870144367218018  and  1.108101725578308  and  0.8707154989242554\n",
      "total error:  2.5658316612243652\n",
      "training error:  0.581961989402771  and  1.104583740234375  and  0.9217183589935303\n",
      "total error:  2.6082640886306763\n",
      "training error:  0.6657949686050415  and  1.332605242729187  and  0.8813635110855103\n",
      "total error:  2.8797637224197388\n",
      "training error:  0.6038472056388855  and  1.0905437469482422  and  0.9019666910171509\n",
      "total error:  2.5963576436042786\n",
      "training error:  0.5948100090026855  and  1.0826846361160278  and  1.0275417566299438\n",
      "total error:  2.7050364017486572\n",
      "training error:  0.6108025908470154  and  1.0517455339431763  and  0.8587027192115784\n",
      "total error:  2.52125084400177\n",
      "training error:  0.5963448882102966  and  1.1188726425170898  and  0.8889504671096802\n",
      "total error:  2.6041679978370667\n",
      "training error:  0.6498194932937622  and  1.2091116905212402  and  1.0020861625671387\n",
      "total error:  2.861017346382141\n",
      "training error:  0.6141039729118347  and  1.0994744300842285  and  0.952600359916687\n",
      "total error:  2.6661787629127502\n",
      "training error:  0.6169850826263428  and  1.0940618515014648  and  0.9157377481460571\n",
      "total error:  2.6267846822738647\n",
      "training error:  0.596362292766571  and  1.08250892162323  and  0.8873980045318604\n",
      "total error:  2.5662692189216614\n",
      "training error:  0.6034506559371948  and  1.147756814956665  and  0.9052045345306396\n",
      "total error:  2.6564120054244995\n",
      "training error:  0.6030259132385254  and  1.049507737159729  and  0.8697954416275024\n",
      "total error:  2.522329092025757\n",
      "training error:  0.6413534283638  and  1.0806388854980469  and  0.898999035358429\n",
      "total error:  2.620991349220276\n",
      "training error:  0.6139940023422241  and  1.143317461013794  and  0.8928521871566772\n",
      "total error:  2.6501636505126953\n",
      "training error:  0.6042816638946533  and  1.113082766532898  and  0.9702544212341309\n",
      "total error:  2.687618851661682\n",
      "training error:  0.59132981300354  and  1.1217455863952637  and  0.9223438501358032\n",
      "total error:  2.635419249534607\n",
      "training error:  0.6253798007965088  and  1.0633716583251953  and  0.8630478382110596\n",
      "total error:  2.5517992973327637\n",
      "training error:  0.5963746905326843  and  1.0406395196914673  and  0.8471028208732605\n",
      "total error:  2.484117031097412\n",
      "training error:  0.5991770029067993  and  1.1011708974838257  and  0.9495697617530823\n",
      "total error:  2.6499176621437073\n",
      "training error:  0.5964750051498413  and  1.190857172012329  and  0.86224365234375\n",
      "total error:  2.6495758295059204\n",
      "training error:  0.5831767916679382  and  1.0940192937850952  and  0.8645422458648682\n",
      "total error:  2.5417383313179016\n",
      "training error:  0.5830087065696716  and  1.0504862070083618  and  0.9592308402061462\n",
      "total error:  2.5927257537841797\n",
      "training error:  0.609103262424469  and  1.0729408264160156  and  0.8945919275283813\n",
      "total error:  2.576636016368866\n",
      "training error:  0.5762524008750916  and  1.1832454204559326  and  0.8998268842697144\n",
      "total error:  2.6593247056007385\n",
      "training error:  0.5748867988586426  and  1.0463483333587646  and  0.8402741551399231\n",
      "total error:  2.4615092873573303\n",
      "training error:  0.6148013472557068  and  1.0474045276641846  and  0.9246149659156799\n",
      "total error:  2.5868208408355713\n",
      "training error:  0.6078442931175232  and  1.145021915435791  and  0.8897020816802979\n",
      "total error:  2.642568290233612\n",
      "training error:  0.5832452178001404  and  1.0366076231002808  and  0.875298261642456\n",
      "total error:  2.495151102542877\n",
      "training error:  0.5953396558761597  and  1.2380006313323975  and  0.8773820400238037\n",
      "total error:  2.710722327232361\n",
      "training error:  0.5786588788032532  and  1.1316840648651123  and  0.9652000665664673\n",
      "total error:  2.6755430102348328\n",
      "training error:  0.6160982847213745  and  1.08134126663208  and  0.8284046649932861\n",
      "total error:  2.5258442163467407\n",
      "training error:  0.5742295980453491  and  1.0606603622436523  and  0.8906999826431274\n",
      "total error:  2.525589942932129\n",
      "training error:  0.5837963223457336  and  1.064998745918274  and  0.9171460866928101\n",
      "total error:  2.5659411549568176\n",
      "training error:  0.6120558381080627  and  1.1000897884368896  and  1.046586513519287\n",
      "total error:  2.7587321400642395\n",
      "training error:  0.6119229197502136  and  1.0405282974243164  and  0.9041892290115356\n",
      "total error:  2.5566404461860657\n",
      "training error:  0.5737638473510742  and  1.0446281433105469  and  0.8905194401741028\n",
      "total error:  2.508911430835724\n",
      "training error:  0.6513833999633789  and  1.1190929412841797  and  0.8507225513458252\n",
      "total error:  2.621198892593384\n",
      "training error:  0.6107535362243652  and  1.1013517379760742  and  0.9064715504646301\n",
      "total error:  2.6185768246650696\n",
      "training error:  0.599531352519989  and  1.0750980377197266  and  0.8708610534667969\n",
      "total error:  2.5454904437065125\n",
      "training error:  0.5794338583946228  and  1.0659812688827515  and  0.8489480018615723\n",
      "total error:  2.4943631291389465\n",
      "training error:  0.5840116143226624  and  1.0622937679290771  and  0.8802968263626099\n",
      "total error:  2.5266022086143494\n",
      "training error:  0.5990103483200073  and  1.0927258729934692  and  0.8814854621887207\n",
      "total error:  2.5732216835021973\n",
      "training error:  0.6095688343048096  and  1.1506657600402832  and  1.002701997756958\n",
      "total error:  2.762936592102051\n",
      "training error:  0.5918660759925842  and  1.0761798620224  and  0.8453299403190613\n",
      "total error:  2.5133758783340454\n",
      "training error:  0.5944933295249939  and  1.1474056243896484  and  0.9212911128997803\n",
      "total error:  2.6631900668144226\n",
      "training error:  0.5941450595855713  and  1.1321539878845215  and  0.8711023926734924\n",
      "total error:  2.597401440143585\n",
      "training error:  0.5950515270233154  and  1.0854873657226562  and  0.851122260093689\n",
      "total error:  2.5316611528396606\n",
      "training error:  0.5836873054504395  and  1.1022624969482422  and  0.9115732908248901\n",
      "total error:  2.5975230932235718\n",
      "training error:  0.5869296789169312  and  1.072314977645874  and  0.8549211025238037\n",
      "total error:  2.514165759086609\n",
      "training error:  0.5842320919036865  and  1.2564483880996704  and  0.8828060626983643\n",
      "total error:  2.723486542701721\n",
      "training error:  0.5896297693252563  and  1.1366074085235596  and  0.9312953948974609\n",
      "total error:  2.657532572746277\n",
      "training error:  0.5882627964019775  and  1.0592899322509766  and  0.9386060833930969\n",
      "total error:  2.586158812046051\n",
      "training error:  0.604773998260498  and  1.238004207611084  and  0.8675367832183838\n",
      "total error:  2.710314989089966\n",
      "training error:  0.5867938995361328  and  1.0832724571228027  and  0.8696081042289734\n",
      "total error:  2.539674460887909\n",
      "training error:  0.5759948492050171  and  1.056653618812561  and  0.8466731309890747\n",
      "total error:  2.479321599006653\n",
      "training error:  0.5976113080978394  and  1.0689795017242432  and  0.8335385918617249\n",
      "total error:  2.5001294016838074\n",
      "training error:  0.5620035529136658  and  1.0270862579345703  and  0.8709151744842529\n",
      "total error:  2.460004985332489\n",
      "training error:  0.5721389651298523  and  1.107576608657837  and  1.063494086265564\n",
      "total error:  2.743209660053253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.5700758695602417  and  1.0548774003982544  and  0.8404555320739746\n",
      "total error:  2.4654088020324707\n",
      "training error:  0.5735599994659424  and  1.0478284358978271  and  0.8382349610328674\n",
      "total error:  2.459623396396637\n",
      "training error:  0.6026009917259216  and  1.0693466663360596  and  0.886950671672821\n",
      "total error:  2.5588983297348022\n",
      "training error:  0.5879980325698853  and  1.0598963499069214  and  0.8522490859031677\n",
      "total error:  2.5001434683799744\n",
      "training error:  0.5943492650985718  and  1.0891478061676025  and  0.9259555339813232\n",
      "total error:  2.6094526052474976\n",
      "training error:  0.6123688220977783  and  1.0796704292297363  and  0.835111141204834\n",
      "total error:  2.5271503925323486\n",
      "training error:  0.6143937110900879  and  1.1297452449798584  and  0.8930840492248535\n",
      "total error:  2.6372230052948\n",
      "training error:  0.5644432306289673  and  1.1080245971679688  and  0.8968919515609741\n",
      "total error:  2.56935977935791\n",
      "training error:  0.5733357667922974  and  1.068327784538269  and  0.8788492679595947\n",
      "total error:  2.520512819290161\n",
      "training error:  0.5804388523101807  and  1.1500194072723389  and  0.9053456783294678\n",
      "total error:  2.6358039379119873\n",
      "training error:  0.563270628452301  and  1.055354118347168  and  0.8481562733650208\n",
      "total error:  2.4667810201644897\n",
      "training error:  0.5701680779457092  and  1.0095586776733398  and  0.8257154226303101\n",
      "total error:  2.405442178249359\n",
      "training error:  0.580871045589447  and  1.092242956161499  and  0.9035245776176453\n",
      "total error:  2.5766385793685913\n",
      "training error:  0.5897664427757263  and  1.1386311054229736  and  0.9624844789505005\n",
      "total error:  2.6908820271492004\n",
      "training error:  0.5688489675521851  and  1.048439860343933  and  0.8561381101608276\n",
      "total error:  2.473426938056946\n",
      "training error:  0.5877593755722046  and  1.0398352146148682  and  0.8051077127456665\n",
      "total error:  2.4327023029327393\n",
      "training error:  0.6017695665359497  and  1.0963784456253052  and  0.9303228855133057\n",
      "total error:  2.6284708976745605\n",
      "training error:  0.5472219586372375  and  1.0410106182098389  and  0.8759455680847168\n",
      "total error:  2.464178144931793\n",
      "training error:  0.5810727477073669  and  1.0818030834197998  and  0.8615800738334656\n",
      "total error:  2.5244559049606323\n",
      "training error:  0.5705621242523193  and  1.110950231552124  and  0.8495640158653259\n",
      "total error:  2.5310763716697693\n",
      "training error:  0.5729851722717285  and  1.0945501327514648  and  1.0436969995498657\n",
      "total error:  2.711232304573059\n",
      "training error:  0.5688962936401367  and  1.030807375907898  and  0.8133341670036316\n",
      "total error:  2.4130378365516663\n",
      "training error:  0.581544041633606  and  1.1915547847747803  and  0.9135288000106812\n",
      "total error:  2.6866276264190674\n",
      "training error:  0.5815454721450806  and  1.08888840675354  and  0.9267383813858032\n",
      "total error:  2.597172260284424\n",
      "training error:  0.6088790893554688  and  1.1072032451629639  and  0.8842853903770447\n",
      "total error:  2.6003677248954773\n",
      "training error:  0.5842750072479248  and  1.1066346168518066  and  0.9019216299057007\n",
      "total error:  2.592831254005432\n",
      "training error:  0.5931614637374878  and  1.0802700519561768  and  0.8532049655914307\n",
      "total error:  2.526636481285095\n",
      "training error:  0.6305115222930908  and  1.0662940740585327  and  0.9099396467208862\n",
      "total error:  2.6067452430725098\n",
      "training error:  0.5794503092765808  and  1.1676793098449707  and  0.9391168355941772\n",
      "total error:  2.6862464547157288\n",
      "training error:  0.5915263295173645  and  1.0917818546295166  and  0.8425551056861877\n",
      "total error:  2.525863289833069\n",
      "training error:  0.5621887445449829  and  1.0151673555374146  and  0.8420016765594482\n",
      "total error:  2.4193577766418457\n",
      "training error:  0.5563217997550964  and  1.0120110511779785  and  0.8879839777946472\n",
      "total error:  2.456316828727722\n",
      "training error:  0.5755215883255005  and  1.0863971710205078  and  0.9049459099769592\n",
      "total error:  2.5668646693229675\n",
      "training error:  0.5623683333396912  and  1.0079561471939087  and  0.8316864371299744\n",
      "total error:  2.402010917663574\n",
      "training error:  0.5865980982780457  and  1.0863906145095825  and  0.8992654085159302\n",
      "total error:  2.5722541213035583\n",
      "training error:  0.5726505517959595  and  1.0270370244979858  and  0.8488008975982666\n",
      "total error:  2.448488473892212\n",
      "training error:  0.5686492919921875  and  1.085379958152771  and  0.9070900082588196\n",
      "total error:  2.561119258403778\n",
      "training error:  0.601982057094574  and  1.0512930154800415  and  0.9141579866409302\n",
      "total error:  2.5674330592155457\n",
      "training error:  0.5932964086532593  and  1.0388771295547485  and  0.8390529155731201\n",
      "total error:  2.471226453781128\n",
      "training error:  0.5687171220779419  and  1.0222647190093994  and  0.9244710206985474\n",
      "total error:  2.5154528617858887\n",
      "training error:  0.5627012848854065  and  1.070525884628296  and  0.8633100390434265\n",
      "total error:  2.496537208557129\n",
      "training error:  0.5897880792617798  and  1.162748098373413  and  0.9305312633514404\n",
      "total error:  2.6830674409866333\n",
      "training error:  0.5708780288696289  and  1.051945686340332  and  0.8857701420783997\n",
      "total error:  2.5085938572883606\n",
      "training error:  0.575840950012207  and  1.0791033506393433  and  0.8471810817718506\n",
      "total error:  2.502125382423401\n",
      "training error:  0.5654821395874023  and  1.0672630071640015  and  0.9087762832641602\n",
      "total error:  2.541521430015564\n",
      "training error:  0.5761142373085022  and  1.069149136543274  and  0.8555620312690735\n",
      "total error:  2.5008254051208496\n",
      "training error:  0.5798958539962769  and  1.2392981052398682  and  0.9366645216941833\n",
      "total error:  2.7558584809303284\n",
      "training error:  0.5966267585754395  and  1.0795108079910278  and  0.9178280830383301\n",
      "total error:  2.5939656496047974\n",
      "training error:  0.5699976086616516  and  1.0597070455551147  and  0.8967994451522827\n",
      "total error:  2.526504099369049\n",
      "training error:  0.5792888402938843  and  1.0398640632629395  and  0.83549565076828\n",
      "total error:  2.4546485543251038\n",
      "training error:  0.5806101560592651  and  1.0826129913330078  and  0.9779013991355896\n",
      "total error:  2.6411245465278625\n",
      "training error:  0.5632278323173523  and  1.0906270742416382  and  0.8378270864486694\n",
      "total error:  2.49168199300766\n",
      "training error:  0.5858855843544006  and  1.0750641822814941  and  0.8230024576187134\n",
      "total error:  2.483952224254608\n",
      "training error:  0.5579952001571655  and  1.04145085811615  and  0.9145892858505249\n",
      "total error:  2.5140353441238403\n",
      "training error:  0.5780922174453735  and  1.0334991216659546  and  0.8588600158691406\n",
      "total error:  2.4704513549804688\n",
      "training error:  0.5602494478225708  and  1.1134815216064453  and  0.8879847526550293\n",
      "total error:  2.5617157220840454\n",
      "training error:  0.5604575276374817  and  1.0624315738677979  and  0.9002759456634521\n",
      "total error:  2.5231650471687317\n",
      "training error:  0.6033384203910828  and  1.0968971252441406  and  0.8759763240814209\n",
      "total error:  2.5762118697166443\n",
      "training error:  0.5754464268684387  and  1.0718061923980713  and  0.9265046119689941\n",
      "total error:  2.573757231235504\n",
      "training error:  0.5659976601600647  and  1.073872685432434  and  0.8995125889778137\n",
      "total error:  2.5393829345703125\n",
      "training error:  0.563014805316925  and  1.1376298666000366  and  0.9830857515335083\n",
      "total error:  2.68373042345047\n",
      "training error:  0.5779405832290649  and  1.2105134725570679  and  0.928189218044281\n",
      "total error:  2.716643273830414\n",
      "training error:  0.6203337907791138  and  1.1117419004440308  and  0.9059035181999207\n",
      "total error:  2.637979209423065\n",
      "training error:  0.5827318429946899  and  1.092247724533081  and  1.0353326797485352\n",
      "total error:  2.710312247276306\n",
      "training error:  0.570469856262207  and  1.1206799745559692  and  0.9301806688308716\n",
      "total error:  2.621330499649048\n",
      "training error:  0.5813628435134888  and  1.0868192911148071  and  0.9001441597938538\n",
      "total error:  2.5683262944221497\n",
      "training error:  0.5618433356285095  and  1.055679202079773  and  0.8446688652038574\n",
      "total error:  2.46219140291214\n",
      "training error:  0.5645026564598083  and  1.0685913562774658  and  0.8598344326019287\n",
      "total error:  2.492928445339203\n",
      "training error:  0.592094361782074  and  1.0476269721984863  and  0.8670335412025452\n",
      "total error:  2.5067548751831055\n",
      "training error:  0.5615779161453247  and  1.049072265625  and  0.9106399416923523\n",
      "total error:  2.521290123462677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.5690383911132812  and  1.0531363487243652  and  0.8423528075218201\n",
      "total error:  2.4645275473594666\n",
      "training error:  0.6423468589782715  and  1.4036061763763428  and  1.004421353340149\n",
      "total error:  3.050374388694763\n",
      "training error:  0.5784029960632324  and  1.3391778469085693  and  0.8245582580566406\n",
      "total error:  2.7421391010284424\n",
      "training error:  0.5877491235733032  and  1.068476915359497  and  0.9486024975776672\n",
      "total error:  2.6048285365104675\n",
      "training error:  0.5786504149436951  and  1.0563125610351562  and  0.900822639465332\n",
      "total error:  2.5357856154441833\n",
      "training error:  0.5847354531288147  and  1.2581926584243774  and  0.8896453976631165\n",
      "total error:  2.7325735092163086\n",
      "training error:  0.6644133925437927  and  1.1910604238510132  and  0.9751296043395996\n",
      "total error:  2.8306034207344055\n",
      "training error:  0.5974355936050415  and  1.0602850914001465  and  0.9427566528320312\n",
      "total error:  2.6004773378372192\n",
      "training error:  0.5824794173240662  and  1.0600624084472656  and  0.8090463876724243\n",
      "total error:  2.451588213443756\n",
      "training error:  0.603011429309845  and  1.0443617105484009  and  0.824268102645874\n",
      "total error:  2.47164124250412\n",
      "training error:  0.5490690469741821  and  1.005845308303833  and  0.8802435398101807\n",
      "total error:  2.435157895088196\n",
      "training error:  0.5673680901527405  and  1.0080604553222656  and  0.8623948097229004\n",
      "total error:  2.4378233551979065\n",
      "training error:  0.5857042670249939  and  1.108599066734314  and  0.8994007110595703\n",
      "total error:  2.593704044818878\n",
      "training error:  0.5532534122467041  and  1.0145493745803833  and  0.8476943373680115\n",
      "total error:  2.415497124195099\n",
      "training error:  0.5873673558235168  and  1.0683165788650513  and  0.9104794859886169\n",
      "total error:  2.566163420677185\n",
      "training error:  0.5552695989608765  and  1.0459964275360107  and  0.8767939209938049\n",
      "total error:  2.478059947490692\n",
      "training error:  0.5607024431228638  and  1.0659661293029785  and  0.835454523563385\n",
      "total error:  2.4621230959892273\n",
      "training error:  0.5606155395507812  and  1.0333261489868164  and  0.8301675915718079\n",
      "total error:  2.4241092801094055\n",
      "training error:  0.5747641324996948  and  1.055825114250183  and  0.917091965675354\n",
      "total error:  2.547681212425232\n",
      "training error:  0.5711537599563599  and  1.0310306549072266  and  0.844124436378479\n",
      "total error:  2.4463088512420654\n",
      "training error:  0.5669926404953003  and  1.1523635387420654  and  0.8715575337409973\n",
      "total error:  2.590913712978363\n",
      "training error:  0.5617961883544922  and  0.9956328868865967  and  0.8304156064987183\n",
      "total error:  2.387844681739807\n",
      "training error:  0.5574327707290649  and  1.1356971263885498  and  0.8611180186271667\n",
      "total error:  2.5542479157447815\n",
      "training error:  0.5800573825836182  and  1.046716570854187  and  0.8731792569160461\n",
      "total error:  2.4999532103538513\n",
      "training error:  0.582646369934082  and  1.0333155393600464  and  0.8771251440048218\n",
      "total error:  2.49308705329895\n",
      "training error:  0.5760241150856018  and  1.0513663291931152  and  0.8745378851890564\n",
      "total error:  2.5019283294677734\n",
      "training error:  0.5730722546577454  and  1.054492712020874  and  0.9203665852546692\n",
      "total error:  2.5479315519332886\n",
      "training error:  0.5806460380554199  and  1.0316731929779053  and  0.8414840698242188\n",
      "total error:  2.453803300857544\n",
      "training error:  0.5718696117401123  and  1.0421252250671387  and  0.8265256881713867\n",
      "total error:  2.4405205249786377\n",
      "training error:  0.5641869902610779  and  1.020655870437622  and  0.8710976839065552\n",
      "total error:  2.455940544605255\n",
      "training error:  0.5826271772384644  and  1.0892971754074097  and  0.9665226340293884\n",
      "total error:  2.6384469866752625\n",
      "training error:  0.5615907311439514  and  1.0434478521347046  and  0.8479517698287964\n",
      "total error:  2.4529903531074524\n",
      "training error:  0.5646204352378845  and  1.0537528991699219  and  0.8465185761451721\n",
      "total error:  2.4648919105529785\n",
      "training error:  0.5633165836334229  and  1.0053431987762451  and  0.8191165924072266\n",
      "total error:  2.3877763748168945\n",
      "training error:  0.5614525675773621  and  1.082351803779602  and  0.9078894853591919\n",
      "total error:  2.551693856716156\n",
      "training error:  0.5372447371482849  and  1.0141311883926392  and  0.7932469248771667\n",
      "total error:  2.344622850418091\n",
      "training error:  0.5390540361404419  and  1.0225205421447754  and  0.8239136338233948\n",
      "total error:  2.385488212108612\n",
      "training error:  0.5685667991638184  and  1.0025403499603271  and  0.8874151706695557\n",
      "total error:  2.458522319793701\n",
      "training error:  0.5432936549186707  and  1.051729679107666  and  0.8448845148086548\n",
      "total error:  2.4399078488349915\n",
      "training error:  0.5845025777816772  and  1.0744848251342773  and  0.9286108016967773\n",
      "total error:  2.587598204612732\n",
      "training error:  0.5884099006652832  and  1.097517728805542  and  0.9020826816558838\n",
      "total error:  2.588010311126709\n",
      "training error:  0.5599919557571411  and  1.0629453659057617  and  0.9084583520889282\n",
      "total error:  2.531395673751831\n",
      "training error:  0.5489906072616577  and  1.1066105365753174  and  0.8531773090362549\n",
      "total error:  2.50877845287323\n",
      "training error:  0.565420389175415  and  1.1243401765823364  and  0.8576462268829346\n",
      "total error:  2.547406792640686\n",
      "training error:  0.5553501844406128  and  1.0251094102859497  and  0.9037992358207703\n",
      "total error:  2.4842588305473328\n",
      "training error:  0.6019090414047241  and  1.101532220840454  and  0.9534830451011658\n",
      "total error:  2.656924307346344\n",
      "training error:  0.5365892052650452  and  1.0177483558654785  and  0.8369095921516418\n",
      "total error:  2.3912471532821655\n",
      "training error:  0.5648482441902161  and  1.0615577697753906  and  0.8424015641212463\n",
      "total error:  2.468807578086853\n",
      "training error:  0.5522346496582031  and  1.0214791297912598  and  0.8137357831001282\n",
      "total error:  2.387449562549591\n",
      "training error:  0.5549381971359253  and  1.1084482669830322  and  0.8892788290977478\n",
      "total error:  2.5526652932167053\n",
      "training error:  0.5443811416625977  and  1.0515263080596924  and  0.8463197946548462\n",
      "total error:  2.4422272443771362\n",
      "training error:  0.5522099733352661  and  1.0176169872283936  and  0.8277933597564697\n",
      "total error:  2.3976203203201294\n",
      "training error:  0.5739201307296753  and  1.0968072414398193  and  0.8773817420005798\n",
      "total error:  2.5481091141700745\n",
      "training error:  0.5568265914916992  and  1.0119643211364746  and  0.8667853474617004\n",
      "total error:  2.4355762600898743\n",
      "training error:  0.5576409101486206  and  1.094536304473877  and  0.9078401923179626\n",
      "total error:  2.56001740694046\n",
      "training error:  0.5691036581993103  and  1.0995221138000488  and  0.8496893644332886\n",
      "total error:  2.5183151364326477\n",
      "training error:  0.5665167570114136  and  1.1060426235198975  and  0.8689552545547485\n",
      "total error:  2.5415146350860596\n",
      "training error:  0.5768157243728638  and  1.0942952632904053  and  0.9422695636749268\n",
      "total error:  2.613380551338196\n",
      "training error:  0.5935884118080139  and  1.1161677837371826  and  1.019000768661499\n",
      "total error:  2.7287569642066956\n",
      "training error:  0.542725682258606  and  1.055168628692627  and  0.8489570617675781\n",
      "total error:  2.446851372718811\n",
      "training error:  0.5652097463607788  and  1.1030042171478271  and  0.8573074340820312\n",
      "total error:  2.525521397590637\n",
      "training error:  0.5685642957687378  and  1.0511939525604248  and  0.8349173069000244\n",
      "total error:  2.454675555229187\n",
      "training error:  0.5665721893310547  and  1.1688907146453857  and  0.8319157958030701\n",
      "total error:  2.5673786997795105\n",
      "training error:  0.5470677614212036  and  1.0338053703308105  and  0.7973303198814392\n",
      "total error:  2.3782034516334534\n",
      "training error:  0.5585339069366455  and  1.0398885011672974  and  0.8403515815734863\n",
      "total error:  2.438773989677429\n",
      "training error:  0.5549923777580261  and  1.0355517864227295  and  0.8601284027099609\n",
      "total error:  2.4506725668907166\n",
      "training error:  0.5837564468383789  and  1.072770357131958  and  0.872348427772522\n",
      "total error:  2.528875231742859\n",
      "training error:  0.5830905437469482  and  1.0989477634429932  and  0.8779347538948059\n",
      "total error:  2.5599730610847473\n",
      "training error:  0.5686720609664917  and  1.0752224922180176  and  0.9957148432731628\n",
      "total error:  2.639609396457672\n",
      "training error:  0.5777179002761841  and  1.0578150749206543  and  0.9284864664077759\n",
      "total error:  2.5640194416046143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.5712482929229736  and  1.1011531352996826  and  0.9648962616920471\n",
      "total error:  2.6372976899147034\n",
      "training error:  0.5531517267227173  and  1.0358562469482422  and  0.8858247995376587\n",
      "total error:  2.474832773208618\n",
      "training error:  0.5504785776138306  and  1.0594062805175781  and  0.8176162242889404\n",
      "total error:  2.427501082420349\n",
      "training error:  0.6481895446777344  and  1.3124055862426758  and  1.0967170000076294\n",
      "total error:  3.0573121309280396\n",
      "training error:  0.5789716243743896  and  1.0552022457122803  and  0.8863322138786316\n",
      "total error:  2.5205060839653015\n",
      "training error:  0.5574877262115479  and  1.2035374641418457  and  0.914854884147644\n",
      "total error:  2.6758800745010376\n",
      "training error:  0.5669169425964355  and  1.2011942863464355  and  0.8040738105773926\n",
      "total error:  2.5721850395202637\n",
      "training error:  0.547856330871582  and  1.0747255086898804  and  0.8497315049171448\n",
      "total error:  2.472313344478607\n",
      "training error:  0.5463268756866455  and  1.0333623886108398  and  0.8274086713790894\n",
      "total error:  2.4070979356765747\n",
      "training error:  0.5971848368644714  and  1.1785509586334229  and  1.0015950202941895\n",
      "total error:  2.7773308157920837\n",
      "training error:  0.5482957363128662  and  1.0924932956695557  and  0.850043535232544\n",
      "total error:  2.490832567214966\n",
      "training error:  0.5764044523239136  and  1.02870774269104  and  0.8397397994995117\n",
      "total error:  2.4448519945144653\n",
      "training error:  0.5464102029800415  and  1.075108528137207  and  0.8863468766212463\n",
      "total error:  2.507865607738495\n",
      "training error:  0.5611375570297241  and  1.0788109302520752  and  0.8698503971099854\n",
      "total error:  2.5097988843917847\n",
      "training error:  0.5967190861701965  and  1.13010573387146  and  0.8854814767837524\n",
      "total error:  2.612306296825409\n",
      "training error:  0.582575798034668  and  1.1005100011825562  and  0.874413251876831\n",
      "total error:  2.557499051094055\n",
      "training error:  0.5555241107940674  and  1.0378401279449463  and  0.8328320980072021\n",
      "total error:  2.426196336746216\n",
      "training error:  0.5578330159187317  and  1.0795228481292725  and  0.8662713766098022\n",
      "total error:  2.5036272406578064\n",
      "training error:  0.5531644821166992  and  0.9949986934661865  and  0.8212149143218994\n",
      "total error:  2.369378089904785\n",
      "training error:  0.5945447683334351  and  1.1292985677719116  and  0.9816070795059204\n",
      "total error:  2.705450415611267\n",
      "training error:  0.5460846424102783  and  1.0229889154434204  and  0.8452481031417847\n",
      "total error:  2.4143216609954834\n",
      "training error:  0.5305880308151245  and  1.0171012878417969  and  0.8863096237182617\n",
      "total error:  2.433998942375183\n",
      "training error:  0.549695611000061  and  1.0322273969650269  and  0.8610824942588806\n",
      "total error:  2.4430055022239685\n",
      "training error:  0.594974160194397  and  1.0380127429962158  and  0.8401272296905518\n",
      "total error:  2.4731141328811646\n",
      "training error:  0.5463243722915649  and  1.0854817628860474  and  0.8731124401092529\n",
      "total error:  2.5049185752868652\n",
      "training error:  0.5721746683120728  and  1.0128521919250488  and  0.8434749245643616\n",
      "total error:  2.428501784801483\n",
      "training error:  0.5555943846702576  and  1.0536855459213257  and  0.871637761592865\n",
      "total error:  2.4809176921844482\n",
      "training error:  0.5934397578239441  and  1.3007428646087646  and  0.8461244106292725\n",
      "total error:  2.740307033061981\n",
      "training error:  0.6902564764022827  and  1.2660753726959229  and  0.9231714606285095\n",
      "total error:  2.879503309726715\n",
      "training error:  0.5751641988754272  and  1.138566255569458  and  0.8983250856399536\n",
      "total error:  2.612055540084839\n",
      "training error:  0.5456004738807678  and  1.0360456705093384  and  0.8477874398231506\n",
      "total error:  2.429433584213257\n",
      "training error:  0.5722066164016724  and  1.1372541189193726  and  1.0160911083221436\n",
      "total error:  2.7255518436431885\n",
      "training error:  0.5374351143836975  and  1.0267329216003418  and  0.8295906186103821\n",
      "total error:  2.3937586545944214\n",
      "training error:  0.5441179275512695  and  1.0092923641204834  and  0.8762878179550171\n",
      "total error:  2.42969810962677\n",
      "training error:  0.5624946355819702  and  1.0907211303710938  and  0.8338032960891724\n",
      "total error:  2.4870190620422363\n",
      "training error:  0.5218473076820374  and  1.0345187187194824  and  0.882659912109375\n",
      "total error:  2.4390259385108948\n",
      "training error:  0.5797322988510132  and  1.0852073431015015  and  0.9558733105659485\n",
      "total error:  2.620812952518463\n",
      "training error:  0.553107738494873  and  1.072364330291748  and  0.9287254810333252\n",
      "total error:  2.5541975498199463\n",
      "training error:  0.5727088451385498  and  1.1118069887161255  and  0.9842360019683838\n",
      "total error:  2.668751835823059\n",
      "training error:  0.5866002440452576  and  1.100710153579712  and  0.8517491817474365\n",
      "total error:  2.539059579372406\n",
      "training error:  0.5626683235168457  and  1.1113746166229248  and  0.9377210140228271\n",
      "total error:  2.6117639541625977\n",
      "training error:  0.5910981297492981  and  1.1619776487350464  and  0.9138559103012085\n",
      "total error:  2.666931688785553\n",
      "training error:  0.5488330721855164  and  1.0790038108825684  and  0.8499994277954102\n",
      "total error:  2.477836310863495\n",
      "training error:  0.6324647665023804  and  1.184716820716858  and  0.850953221321106\n",
      "total error:  2.6681348085403442\n",
      "training error:  0.5558225512504578  and  1.050138235092163  and  0.8508697748184204\n",
      "total error:  2.4568305611610413\n",
      "training error:  0.5565675497055054  and  1.1344232559204102  and  0.8759819269180298\n",
      "total error:  2.5669727325439453\n",
      "training error:  0.5441219806671143  and  1.1843684911727905  and  0.9270721673965454\n",
      "total error:  2.65556263923645\n",
      "training error:  0.5561530590057373  and  1.0598567724227905  and  0.8403517007827759\n",
      "total error:  2.4563615322113037\n",
      "training error:  0.5464044213294983  and  1.014060616493225  and  0.8411704301834106\n",
      "total error:  2.401635468006134\n",
      "training error:  0.5779542922973633  and  1.112493634223938  and  0.9348556995391846\n",
      "total error:  2.625303626060486\n",
      "training error:  0.5717071890830994  and  1.0518003702163696  and  0.8690038919448853\n",
      "total error:  2.4925114512443542\n",
      "training error:  0.5430834293365479  and  1.0441701412200928  and  0.9034624695777893\n",
      "total error:  2.49071604013443\n",
      "training error:  0.5447232723236084  and  1.0543452501296997  and  0.8188130259513855\n",
      "total error:  2.4178815484046936\n",
      "training error:  0.5227645039558411  and  1.034453272819519  and  0.9073255658149719\n",
      "total error:  2.464543342590332\n",
      "training error:  0.5747447609901428  and  1.158573865890503  and  0.8825716972351074\n",
      "total error:  2.615890324115753\n",
      "training error:  0.5646241903305054  and  1.024841547012329  and  0.8600047826766968\n",
      "total error:  2.4494705200195312\n",
      "training error:  0.5575284957885742  and  1.1338155269622803  and  0.9319921731948853\n",
      "total error:  2.6233361959457397\n",
      "training error:  0.6261492371559143  and  1.1521382331848145  and  0.8911271691322327\n",
      "total error:  2.6694146394729614\n",
      "training error:  0.5424946546554565  and  1.056444525718689  and  0.8127260208129883\n",
      "total error:  2.411665201187134\n",
      "training error:  0.5572203993797302  and  1.044648289680481  and  0.857110321521759\n",
      "total error:  2.45897901058197\n",
      "training error:  0.5551351308822632  and  1.0728790760040283  and  0.9098409414291382\n",
      "total error:  2.5378551483154297\n",
      "training error:  0.5460705161094666  and  1.0903793573379517  and  0.9479644894599915\n",
      "total error:  2.5844143629074097\n",
      "training error:  0.5828680992126465  and  1.0725632905960083  and  0.9692315459251404\n",
      "total error:  2.624662935733795\n",
      "training error:  0.5614203214645386  and  1.1072922945022583  and  0.9658963680267334\n",
      "total error:  2.6346089839935303\n",
      "training error:  0.5891033411026001  and  1.050950527191162  and  1.0731656551361084\n",
      "total error:  2.7132195234298706\n",
      "training error:  0.5580217838287354  and  1.0286288261413574  and  0.8652517795562744\n",
      "total error:  2.451902389526367\n",
      "training error:  0.5563666820526123  and  1.1555676460266113  and  0.924903154373169\n",
      "total error:  2.6368374824523926\n",
      "training error:  0.5369934439659119  and  1.0304367542266846  and  0.8082358837127686\n",
      "total error:  2.375666081905365\n",
      "training error:  0.5679292678833008  and  1.034443974494934  and  0.8780659437179565\n",
      "total error:  2.4804391860961914\n",
      "training error:  0.5711320638656616  and  1.0792725086212158  and  0.9023825526237488\n",
      "total error:  2.552787125110626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.608500599861145  and  1.1010791063308716  and  0.9055929183959961\n",
      "total error:  2.6151726245880127\n",
      "training error:  0.5400739908218384  and  1.0361213684082031  and  0.8413223624229431\n",
      "total error:  2.4175177216529846\n",
      "training error:  0.5593010783195496  and  1.0068706274032593  and  0.8395212888717651\n",
      "total error:  2.405692994594574\n",
      "training error:  0.5534769296646118  and  1.0620336532592773  and  0.8314172029495239\n",
      "total error:  2.446927785873413\n",
      "training error:  0.5737706422805786  and  1.0399667024612427  and  0.8638936877250671\n",
      "total error:  2.4776310324668884\n",
      "training error:  0.5536398887634277  and  1.068602204322815  and  0.8353724479675293\n",
      "total error:  2.457614541053772\n",
      "training error:  0.5647276043891907  and  1.0584698915481567  and  1.0580612421035767\n",
      "total error:  2.681258738040924\n",
      "training error:  0.5578919649124146  and  1.0926432609558105  and  0.934403657913208\n",
      "total error:  2.584938883781433\n",
      "training error:  0.5829120874404907  and  1.1862423419952393  and  1.0487819910049438\n",
      "total error:  2.817936420440674\n",
      "training error:  0.5314147472381592  and  1.011462688446045  and  0.8154019713401794\n",
      "total error:  2.3582794070243835\n",
      "training error:  0.5472608804702759  and  1.0207538604736328  and  0.8094789981842041\n",
      "total error:  2.377493739128113\n",
      "training error:  0.5513604879379272  and  1.0253442525863647  and  0.8803746700286865\n",
      "total error:  2.4570794105529785\n",
      "training error:  0.53816157579422  and  1.0596396923065186  and  0.85274338722229\n",
      "total error:  2.4505446553230286\n",
      "training error:  0.568875789642334  and  1.08858323097229  and  0.9038541316986084\n",
      "total error:  2.5613131523132324\n",
      "training error:  0.5753799080848694  and  1.1003237962722778  and  0.8070663213729858\n",
      "total error:  2.482770025730133\n",
      "training error:  0.5395855903625488  and  1.0062615871429443  and  0.8097749948501587\n",
      "total error:  2.355622172355652\n",
      "training error:  0.5290005207061768  and  1.0503721237182617  and  0.857984185218811\n",
      "total error:  2.4373568296432495\n",
      "training error:  0.5648545026779175  and  1.0285109281539917  and  0.8613598346710205\n",
      "total error:  2.4547252655029297\n",
      "training error:  0.5753775835037231  and  1.0519800186157227  and  0.9206404685974121\n",
      "total error:  2.547998070716858\n",
      "training error:  0.5658957958221436  and  1.0231566429138184  and  0.8641989827156067\n",
      "total error:  2.4532514214515686\n",
      "training error:  0.5542780160903931  and  1.0651286840438843  and  0.8122943639755249\n",
      "total error:  2.4317010641098022\n",
      "training error:  0.5396166443824768  and  1.0803295373916626  and  0.9003063440322876\n",
      "total error:  2.520252525806427\n",
      "training error:  0.5323696732521057  and  1.012801170349121  and  0.8532245755195618\n",
      "total error:  2.3983954191207886\n",
      "training error:  0.5743460655212402  and  1.0744364261627197  and  0.8277949094772339\n",
      "total error:  2.476577401161194\n",
      "training error:  0.5256737470626831  and  1.0326766967773438  and  0.8593226671218872\n",
      "total error:  2.417673110961914\n",
      "training error:  0.5295040607452393  and  0.9923765659332275  and  0.9051666855812073\n",
      "total error:  2.427047312259674\n",
      "training error:  0.5380751490592957  and  1.1483983993530273  and  0.8864704966545105\n",
      "total error:  2.5729440450668335\n",
      "training error:  0.5619251728057861  and  1.022240400314331  and  0.8081106543540955\n",
      "total error:  2.3922762274742126\n",
      "training error:  0.5225799083709717  and  0.9895207285881042  and  0.8426712155342102\n",
      "total error:  2.354771852493286\n",
      "training error:  0.5384542942047119  and  1.0639420747756958  and  0.8582949042320251\n",
      "total error:  2.460691273212433\n",
      "training error:  0.5333967804908752  and  1.0092979669570923  and  0.8346132636070251\n",
      "total error:  2.3773080110549927\n",
      "training error:  0.5731515884399414  and  1.1126079559326172  and  0.8261648416519165\n",
      "total error:  2.511924386024475\n",
      "training error:  0.6035357117652893  and  1.253659963607788  and  0.9936783909797668\n",
      "total error:  2.8508740663528442\n",
      "training error:  0.553545355796814  and  0.999230146408081  and  0.8357707858085632\n",
      "total error:  2.3885462880134583\n",
      "training error:  0.5460233092308044  and  1.0456782579421997  and  0.9169615507125854\n",
      "total error:  2.5086631178855896\n",
      "training error:  0.5467085838317871  and  1.020900845527649  and  0.8189364671707153\n",
      "total error:  2.3865458965301514\n",
      "training error:  0.5640062093734741  and  1.0222902297973633  and  0.8710191249847412\n",
      "total error:  2.4573155641555786\n",
      "training error:  0.5562114119529724  and  1.0578279495239258  and  0.8327910304069519\n",
      "total error:  2.44683039188385\n",
      "training error:  0.5454745292663574  and  1.0327354669570923  and  0.8013334274291992\n",
      "total error:  2.379543423652649\n",
      "training error:  0.5518901348114014  and  1.0083229541778564  and  0.8424005508422852\n",
      "total error:  2.402613639831543\n",
      "training error:  0.5410215258598328  and  1.1151258945465088  and  0.8646811842918396\n",
      "total error:  2.520828604698181\n",
      "training error:  0.5397059917449951  and  1.0690219402313232  and  0.8126487731933594\n",
      "total error:  2.4213767051696777\n",
      "training error:  0.5478823781013489  and  1.0384457111358643  and  0.8416123390197754\n",
      "total error:  2.4279404282569885\n",
      "training error:  0.532289981842041  and  1.0065248012542725  and  0.8145644664764404\n",
      "total error:  2.353379249572754\n",
      "training error:  0.5310381650924683  and  0.9850465059280396  and  0.7999330759048462\n",
      "total error:  2.316017746925354\n",
      "training error:  0.5639446973800659  and  1.09009850025177  and  0.8831602334976196\n",
      "total error:  2.5372034311294556\n",
      "training error:  0.5888432264328003  and  1.0688917636871338  and  0.8575401306152344\n",
      "total error:  2.5152751207351685\n",
      "training error:  0.5449516773223877  and  1.1575639247894287  and  0.9390044212341309\n",
      "total error:  2.6415200233459473\n",
      "training error:  0.5670071244239807  and  1.0690776109695435  and  0.8297616243362427\n",
      "total error:  2.465846359729767\n",
      "training error:  0.5764271020889282  and  1.0776479244232178  and  0.8305659890174866\n",
      "total error:  2.4846410155296326\n",
      "training error:  0.5569067001342773  and  1.0442219972610474  and  0.8757643699645996\n",
      "total error:  2.4768930673599243\n",
      "training error:  0.5373214483261108  and  0.9675226211547852  and  0.8318564295768738\n",
      "total error:  2.3367004990577698\n",
      "training error:  0.5468671321868896  and  1.0148820877075195  and  0.8093744516372681\n",
      "total error:  2.3711236715316772\n",
      "training error:  0.5362012386322021  and  0.9883259534835815  and  0.7877837419509888\n",
      "total error:  2.3123109340667725\n",
      "training error:  0.5385596752166748  and  0.9962277412414551  and  0.8332227468490601\n",
      "total error:  2.36801016330719\n",
      "training error:  0.5510456562042236  and  1.1479243040084839  and  0.8801729083061218\n",
      "total error:  2.5791428685188293\n",
      "training error:  0.5156617164611816  and  0.9782036542892456  and  0.8447856903076172\n",
      "total error:  2.3386510610580444\n",
      "training error:  0.5583176612854004  and  1.0057752132415771  and  0.7876859307289124\n",
      "total error:  2.35177880525589\n",
      "training error:  0.5373794436454773  and  1.0342434644699097  and  0.8428429365158081\n",
      "total error:  2.414465844631195\n",
      "training error:  0.5386986136436462  and  1.1044647693634033  and  0.9157822132110596\n",
      "total error:  2.558945596218109\n",
      "training error:  0.6197851896286011  and  1.1666359901428223  and  0.8984227180480957\n",
      "total error:  2.684843897819519\n",
      "training error:  0.543323814868927  and  1.0407087802886963  and  0.7841907739639282\n",
      "total error:  2.3682233691215515\n",
      "training error:  0.5494285821914673  and  1.0241234302520752  and  0.837277889251709\n",
      "total error:  2.4108299016952515\n",
      "training error:  0.527058482170105  and  1.044970989227295  and  0.8912734985351562\n",
      "total error:  2.463302969932556\n",
      "training error:  0.5407584309577942  and  1.035654902458191  and  0.9021384716033936\n",
      "total error:  2.4785518050193787\n",
      "training error:  0.543779730796814  and  1.1542327404022217  and  0.8495372533798218\n",
      "total error:  2.5475497245788574\n",
      "training error:  0.5555185675621033  and  0.9931854009628296  and  0.8228704929351807\n",
      "total error:  2.3715744614601135\n",
      "training error:  0.5452975034713745  and  1.0275702476501465  and  0.7892940044403076\n",
      "total error:  2.3621617555618286\n",
      "training error:  0.5559291839599609  and  0.987545907497406  and  0.8721315860748291\n",
      "total error:  2.415606677532196\n",
      "training error:  0.5442410111427307  and  1.0258922576904297  and  0.8045554161071777\n",
      "total error:  2.374688684940338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  0.5471341609954834  and  1.0136408805847168  and  0.7887009382247925\n",
      "total error:  2.3494759798049927\n",
      "training error:  0.5365278720855713  and  1.100887417793274  and  1.1029349565505981\n",
      "total error:  2.7403502464294434\n",
      "training error:  0.558200478553772  and  1.0334367752075195  and  0.8564598560333252\n",
      "total error:  2.4480971097946167\n",
      "training error:  0.5337881445884705  and  1.0536854267120361  and  0.8831799626350403\n",
      "total error:  2.470653533935547\n",
      "training error:  0.5430501699447632  and  1.0098017454147339  and  0.7806083559989929\n",
      "total error:  2.33346027135849\n",
      "training error:  0.5154911875724792  and  0.9818286895751953  and  0.8191725015640259\n",
      "total error:  2.3164923787117004\n",
      "training error:  0.5346687436103821  and  1.0221436023712158  and  0.8173761963844299\n",
      "total error:  2.374188542366028\n",
      "training error:  0.5401254296302795  and  1.025716781616211  and  0.9112526774406433\n",
      "total error:  2.477094888687134\n",
      "training error:  0.5300000905990601  and  0.9706761837005615  and  0.8082332611083984\n",
      "total error:  2.30890953540802\n",
      "training error:  0.5248894095420837  and  0.9871566891670227  and  0.8483771085739136\n",
      "total error:  2.36042320728302\n",
      "training error:  0.5595681667327881  and  1.1034955978393555  and  0.964005708694458\n",
      "total error:  2.6270694732666016\n",
      "training error:  0.5782757997512817  and  1.0938843488693237  and  0.8339192867279053\n",
      "total error:  2.5060794353485107\n",
      "training error:  0.5418440103530884  and  1.057814121246338  and  0.8312321901321411\n",
      "total error:  2.4308903217315674\n",
      "training error:  0.5080238580703735  and  0.9932491779327393  and  0.8713696002960205\n",
      "total error:  2.3726426362991333\n",
      "training error:  0.5134079456329346  and  1.0141340494155884  and  0.8312033414840698\n",
      "total error:  2.3587453365325928\n",
      "training error:  0.5274433493614197  and  0.9393673539161682  and  0.8063743114471436\n",
      "total error:  2.2731850147247314\n",
      "training error:  0.5337491035461426  and  1.0354013442993164  and  0.8366410732269287\n",
      "total error:  2.4057915210723877\n",
      "training error:  0.5220317840576172  and  1.0326783657073975  and  0.8340229988098145\n",
      "total error:  2.388733148574829\n",
      "training error:  0.5291059017181396  and  0.9987020492553711  and  0.8556269407272339\n",
      "total error:  2.3834348917007446\n",
      "training error:  0.5409088134765625  and  1.1109362840652466  and  1.0687172412872314\n",
      "total error:  2.7205623388290405\n",
      "training error:  0.5285835862159729  and  1.0070290565490723  and  0.8115566968917847\n",
      "total error:  2.34716933965683\n",
      "training error:  0.5336468815803528  and  1.040506362915039  and  0.8556966185569763\n",
      "total error:  2.429849863052368\n",
      "training error:  0.5668355822563171  and  1.0443956851959229  and  0.8395386934280396\n",
      "total error:  2.4507699608802795\n",
      "training error:  0.5257435441017151  and  1.0026543140411377  and  0.7982180118560791\n",
      "total error:  2.326615869998932\n",
      "training error:  0.528770923614502  and  1.008859634399414  and  0.8554871082305908\n",
      "total error:  2.393117666244507\n",
      "training error:  0.5817917585372925  and  1.0797827243804932  and  0.8515031337738037\n",
      "total error:  2.5130776166915894\n",
      "training error:  0.5300538539886475  and  0.9953901767730713  and  0.8302098512649536\n",
      "total error:  2.3556538820266724\n",
      "training error:  0.5463855862617493  and  1.0460469722747803  and  0.8988276124000549\n",
      "total error:  2.4912601709365845\n",
      "training error:  0.552310585975647  and  1.0676045417785645  and  0.8566222786903381\n",
      "total error:  2.4765374064445496\n",
      "training error:  0.6308799982070923  and  1.0755109786987305  and  0.9404627084732056\n",
      "total error:  2.6468536853790283\n",
      "training error:  0.5536264181137085  and  1.1073427200317383  and  0.8524518013000488\n",
      "total error:  2.5134209394454956\n",
      "training error:  0.5470569133758545  and  1.0048799514770508  and  0.8460065126419067\n",
      "total error:  2.397943377494812\n",
      "training error:  0.5220162868499756  and  0.9871353507041931  and  0.7702623605728149\n",
      "total error:  2.2794139981269836\n",
      "training error:  0.5387055277824402  and  1.0320274829864502  and  0.8888808488845825\n",
      "total error:  2.459613859653473\n",
      "training error:  0.550197422504425  and  1.0510196685791016  and  0.8581089973449707\n",
      "total error:  2.4593260884284973\n",
      "training error:  0.5588817596435547  and  1.0395398139953613  and  0.821754515171051\n",
      "total error:  2.420176088809967\n",
      "training error:  0.5512265563011169  and  1.0094373226165771  and  0.8081802129745483\n",
      "total error:  2.3688440918922424\n",
      "training error:  0.5399754047393799  and  1.0166666507720947  and  0.8068180084228516\n",
      "total error:  2.363460063934326\n",
      "training error:  0.5913251638412476  and  1.1620897054672241  and  0.8341459631919861\n",
      "total error:  2.5875608325004578\n",
      "training error:  0.5276671648025513  and  1.0064880847930908  and  0.8433936834335327\n",
      "total error:  2.377548933029175\n",
      "training error:  0.5345655083656311  and  1.078869342803955  and  0.9044539928436279\n",
      "total error:  2.517888844013214\n",
      "training error:  0.521308422088623  and  0.9935534000396729  and  0.8109474182128906\n",
      "total error:  2.3258092403411865\n",
      "training error:  0.5215930938720703  and  0.9966180920600891  and  0.8859701156616211\n",
      "total error:  2.4041813015937805\n",
      "training error:  0.5071395039558411  and  0.973883867263794  and  0.8803029656410217\n",
      "total error:  2.3613263368606567\n",
      "training error:  0.5147079229354858  and  0.9915602207183838  and  0.7919210195541382\n",
      "total error:  2.298189163208008\n",
      "training error:  0.5212663412094116  and  1.046815037727356  and  0.7744377255439758\n",
      "total error:  2.3425191044807434\n",
      "training error:  0.5558608770370483  and  0.9819165468215942  and  0.7628592252731323\n",
      "total error:  2.300636649131775\n",
      "training error:  0.5242716073989868  and  0.9917581081390381  and  0.8482614755630493\n",
      "total error:  2.364291191101074\n",
      "training error:  0.5182880163192749  and  1.0280776023864746  and  0.8630644679069519\n",
      "total error:  2.4094300866127014\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys,os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy import linalg\n",
    "from numpy import dot\n",
    "import geomloss as gs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import grad\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.modules import Linear\n",
    "from torch.autograd.functional import jacobian,hessian,vjp,vhp,hvp\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "FilePath = '../../'\n",
    "\n",
    "file_list = ['GSM1599494_ES_d0_main.csv', 'GSM1599497_ES_d2_LIFminus.csv', 'GSM1599498_ES_d4_LIFminus.csv', 'GSM1599499_ES_d7_LIFminus.csv']\n",
    "\n",
    "table_list = []\n",
    "for filein in file_list:\n",
    "    table_list.append(pd.read_csv(FilePath+filein, header=None))\n",
    "\n",
    "matrix_list = []\n",
    "gene_names = table_list[0].values[:,0]\n",
    "for table in table_list:\n",
    "    matrix_list.append(table.values[:,1:].astype('float32'))\n",
    "\n",
    "cell_counts = [matrix.shape[1] for matrix in matrix_list]\n",
    "\n",
    "def normalize_run(mat):\n",
    "    rpm = np.sum(mat,0)/1e6\n",
    "    detect_pr = np.sum(mat==0,0)/float(mat.shape[0])\n",
    "    return np.log(mat*(np.median(detect_pr)/detect_pr)*1.0/rpm + 1.0)\n",
    "\n",
    "norm_mat = [normalize_run(matrix) for matrix in matrix_list]\n",
    "\n",
    "qt_mat = [np.percentile(norm_in,q=np.linspace(0,100,50),axis=1) for norm_in in norm_mat] \n",
    "wdiv=np.sum((qt_mat[0]-qt_mat[3])**2,0)\n",
    "w_order = np.argsort(-wdiv)\n",
    "\n",
    "wsub = w_order[0:100]\n",
    "\n",
    "\n",
    "def nmf(X, latent_features, max_iter=100, error_limit=1e-6, fit_error_limit=1e-6, print_iter=200):\n",
    "    \"\"\"\n",
    "    Decompose X to A*Y\n",
    "    \"\"\"\n",
    "    eps = 1e-5\n",
    "    print('Starting NMF decomposition with {} latent features and {} iterations.'.format(latent_features, max_iter))\n",
    "    #X = X.toarray()   I am passing in a scipy sparse matrix\n",
    "\n",
    "    # mask\n",
    "    mask = np.sign(X)\n",
    "\n",
    "    # initial matrices. A is random [0,1] and Y is A\\X.\n",
    "    rows, columns = X.shape\n",
    "    A = np.random.rand(rows, latent_features) \n",
    "    A = np.maximum(A, eps)\n",
    "\n",
    "    Y = linalg.lstsq(A, X)[0]\n",
    "    Y = np.maximum(Y, eps)\n",
    "\n",
    "    masked_X = mask * X\n",
    "    X_est_prev = dot(A, Y)\n",
    "    for i in range(1, max_iter + 1):\n",
    "        # ===== updates =====\n",
    "        # Matlab: A=A.*(((W.*X)*Y')./((W.*(A*Y))*Y'));\n",
    "        top = dot(masked_X, Y.T)\n",
    "        bottom = (dot((mask * dot(A, Y)), Y.T)) + eps\n",
    "        A *= top / bottom\n",
    "\n",
    "        A = np.maximum(A, eps)\n",
    "        # print 'A',  np.round(A, 2)\n",
    "\n",
    "        # Matlab: Y=Y.*((A'*(W.*X))./(A'*(W.*(A*Y))));\n",
    "        top = dot(A.T, masked_X)\n",
    "        bottom = dot(A.T, mask * dot(A, Y)) + eps\n",
    "        Y *= top / bottom\n",
    "        Y = np.maximum(Y, eps)\n",
    "        # print 'Y', np.round(Y, 2)\n",
    "\n",
    "\n",
    "        # ==== evaluation ====\n",
    "        if i % print_iter == 0 or i == 1 or i == max_iter:\n",
    "            print('Iteration {}:'.format(i),)\n",
    "            X_est = dot(A, Y)\n",
    "            err = mask * (X_est_prev - X_est)\n",
    "            fit_residual = np.sqrt(np.sum(err ** 2))\n",
    "            X_est_prev = X_est\n",
    "\n",
    "            curRes = linalg.norm(mask * (X - X_est), ord='fro')\n",
    "            print('fit residual', np.round(fit_residual, 4),)\n",
    "            print('total residual', np.round(curRes, 4))\n",
    "            if curRes < error_limit or fit_residual < fit_error_limit:\n",
    "                break\n",
    "    return A, Y, dot(A,Y)\n",
    "\n",
    "np.random.seed(0)\n",
    "norm_imputed = [nmf(normin[wsub,:], latent_features = len(wsub)*4, max_iter=500)[2] for normin in norm_mat]\n",
    "\n",
    "norm_adj = np.mean(norm_imputed[3],1)[:,np.newaxis]\n",
    "subvec = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "gnvec = gene_names[w_order[subvec]]\n",
    "\n",
    "cov_mat = np.cov(norm_imputed[3][subvec,:])\n",
    "whiten = np.diag(np.diag(cov_mat)**(-0.5))\n",
    "unwhiten = np.diag(np.diag(cov_mat)**(0.5))\n",
    "\n",
    "norm_imputed2 = [np.dot(whiten,(normin - norm_adj)[subvec,:]) for normin in norm_imputed]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, dim_hidden=64, num_hidden=0, activation=nn.LeakyReLU()):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        if num_hidden == 0:\n",
    "            self.linears = nn.ModuleList([nn.Linear(dim_in, dim_out)])\n",
    "        elif num_hidden >= 1:\n",
    "            self.linears = nn.ModuleList()\n",
    "            self.linears.append(nn.Linear(dim_in, dim_hidden))\n",
    "            self.linears.extend([nn.Linear(dim_hidden, dim_hidden) for _ in range(num_hidden-1)])\n",
    "            self.linears.append(nn.Linear(dim_hidden, dim_out))\n",
    "        else:\n",
    "            raise Exception('number of hidden layers must be positive')\n",
    "\n",
    "        for m in self.linears:\n",
    "            #nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.uniform_(m.bias,a=-0.1,b=0.1)\n",
    "            #nn.init.constant_(m.bias,0) \n",
    " \n",
    "        self.activation = activation \n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.linears[:-1]:\n",
    "            x = self.activation(m(x))\n",
    "            #x = F.dropout(x,p=0.5)\n",
    "\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "\n",
    "def compute_gradient_penalty(D, real_sample, fake_sample,k,p):\n",
    "    real_samples = real_sample.requires_grad_(True)\n",
    "    fake_samples = fake_sample.requires_grad_(True)\n",
    "\n",
    "    real_validity = D(real_samples)\n",
    "    fake_validity = D(fake_samples)\n",
    "\n",
    "    real_grad_out = torch.ones((real_samples.shape[0],1),dtype=torch.float32,requires_grad=False,device=\"cuda\")\n",
    "    real_grad = grad(\n",
    "        real_validity, real_samples, real_grad_out, create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    real_grad_norm = real_grad.view(real_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "\n",
    "    fake_grad_out = torch.ones((fake_samples.shape[0],1),dtype=torch.float32,requires_grad=False,device=\"cuda\")\n",
    "    fake_grad = grad(\n",
    "        fake_validity, fake_samples, fake_grad_out, create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    fake_grad_norm = fake_grad.view(fake_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "\n",
    "    return (torch.sum(real_grad_norm) + torch.sum(fake_grad_norm)) * k / (real_sample.shape[0]+fake_sample.shape[0])\n",
    "\n",
    "class JumpEulerForwardCuda(nn.Module):\n",
    "    def __init__(self,in_features,num_hidden,dim_hidden,step_size):\n",
    "        super(JumpEulerForwardCuda,self).__init__()\n",
    "\n",
    "        self.drift = MLP(in_features,in_features,dim_hidden,num_hidden)\n",
    "        self.intensity = torch.tensor(intensity,device=\"cuda\")\n",
    "        self.mean = nn.Parameter(0.01*torch.ones(in_features))\n",
    "        self.covHalf = nn.Parameter(0.08*torch.eye(in_features))\n",
    "        self.diffusion = nn.Parameter(torch.ones(bd,10))\n",
    "        self.in_features = in_features\n",
    "        self.jump = MLP(in_features,in_features,dim_hidden,num_hidden)\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def forward(self,z0,Nsim,steps):\n",
    "\n",
    "        PopulationPath = torch.empty(size = (Nsim,steps+1,self.in_features),device=\"cuda\")\n",
    "        PopulationPath[:,0,:] = z0\n",
    "        state = z0\n",
    "\n",
    "        for i in range(1,steps+1):\n",
    "            DP = D.poisson.Poisson(self.intensity*self.step_size) ## step_size\n",
    "            pois = DP.sample((Nsim,1)).cuda()\n",
    "            state = state + self.drift(state)*self.step_size + math.sqrt(self.step_size)*torch.normal(0,1,size=(Nsim,bd),device=\"cuda\")@self.diffusion+\\\n",
    "                (pois*self.mean + pois**(0.5)*torch.normal(0,1,size=(Nsim,self.in_features),device=\"cuda\")@self.covHalf)*self.jump(state)\n",
    "            PopulationPath[:,i,:] = state\n",
    "        return PopulationPath\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "sed = 200\n",
    "setup_seed(sed)\n",
    "\n",
    "a=gs.SamplesLoss(loss='sinkhorn',p=2,blur=0.01)\n",
    "\n",
    "\n",
    "train_data = norm_imputed2\n",
    "\n",
    "train0 = torch.tensor(train_data[0],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "train2 = torch.tensor(train_data[1],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "train4 = torch.tensor(train_data[2],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "train7 = torch.tensor(train_data[3],dtype=torch.float32,requires_grad = True,device=\"cuda\").t()\n",
    "\n",
    "train0 = train0+0.001*torch.normal(0,1,size=train0.shape,device=\"cuda\")\n",
    "train2 = train2+0.001*torch.normal(0,1,size=train2.shape,device=\"cuda\")\n",
    "train4 = train4+0.001*torch.normal(0,1,size=train4.shape,device=\"cuda\")\n",
    "train7 = train7+0.001*torch.normal(0,1,size=train7.shape,device=\"cuda\")\n",
    "\n",
    "\n",
    "intensity = 10\n",
    "lr = 0.0003\n",
    "step_size = 0.03\n",
    "kuan = 256\n",
    "ceng = 4\n",
    "bd = 2\n",
    "n_critic = 3\n",
    "k = 2\n",
    "p = 6\n",
    "\n",
    "n_sims = train0.shape[0]\n",
    "in_features = train0.shape[1]\n",
    "n_steps = [10,20,35]\n",
    "\n",
    "\n",
    "netG = JumpEulerForwardCuda(10,ceng,kuan,step_size).cuda()\n",
    "netD1 = MLP(10,1,dim_hidden=kuan,num_hidden=ceng).cuda()\n",
    "netD2 = MLP(10,1,dim_hidden=kuan,num_hidden=ceng).cuda()\n",
    "netD3 = MLP(10,1,dim_hidden=kuan,num_hidden=ceng).cuda()\n",
    "\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerSD1 = optim.Adam(netD1.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerSD2 = optim.Adam(netD2.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerSD3 = optim.Adam(netD3.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "n_epochs =  20000\n",
    "\n",
    "wd = []\n",
    "for epoch in range(n_epochs):\n",
    "  \n",
    "    \n",
    "\n",
    "    for _ in range(n_critic):\n",
    "        fake_data = netG(train0,n_sims,n_steps[2])\n",
    "        fake1 = fake_data[:,n_steps[0],:]\n",
    "        fake2 = fake_data[:,n_steps[1],:]\n",
    "        fake3 = fake_data[:,n_steps[2],:]\n",
    "\n",
    "        optimizerSD1.zero_grad()\n",
    "\n",
    "        div_gp1 = compute_gradient_penalty(netD1,train2,fake1,k,p)\n",
    "        d1_loss = -torch.mean(netD1(train2))+torch.mean(netD1(fake1))+div_gp1\n",
    "        d1_loss.backward(retain_graph=True) # retain_graph=True\n",
    "\n",
    "        optimizerSD1.step()\n",
    "\n",
    "\n",
    "        optimizerSD2.zero_grad()\n",
    "        \n",
    "        div_gp2 = compute_gradient_penalty(netD2,train4,fake2,k,p)\n",
    "        d2_loss = -torch.mean(netD2(train4))+torch.mean(netD2(fake2))+div_gp2\n",
    "        d2_loss.backward(retain_graph=True)\n",
    "\n",
    "        optimizerSD2.step()\n",
    "        \n",
    "        \n",
    "        optimizerSD3.zero_grad()\n",
    "        \n",
    "        div_gp3 = compute_gradient_penalty(netD3,train7,fake3,k,p)\n",
    "        d3_loss = -torch.mean(netD3(train7))+torch.mean(netD3(fake3))+div_gp3\n",
    "        d3_loss.backward(retain_graph=True)\n",
    "\n",
    "        optimizerSD3.step()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for _ in range(1):\n",
    "        optimizerG.zero_grad()\n",
    "        fake_data = netG(train0,n_sims,n_steps[2])\n",
    "        fake1 = fake_data[:,n_steps[0],:]\n",
    "        fake2 = fake_data[:,n_steps[1],:]\n",
    "        fake3 = fake_data[:,n_steps[2],:]\n",
    "        g_loss = -torch.mean(netD1(fake1))-torch.mean(netD2(fake2))-torch.mean(netD3(fake3))\n",
    "        g_loss.backward() \n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "    if epoch %10==0:\n",
    "        x1 = a(fake_data[:,n_steps[0],:],train2).item()\n",
    "        x2 = a(fake_data[:,n_steps[1],:],train4).item()\n",
    "        x3 = a(fake_data[:,n_steps[2],:],train7).item()\n",
    "        \n",
    "        wd.append(x1+x2+x3)\n",
    "        \n",
    "        print(\"training error: \",x1,\" and \",x2,\" and \",x3)\n",
    "        print(\"total error: \",wd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(),\"./epsilon0.001.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
